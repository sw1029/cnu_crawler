# cnu_crawler/spiders/colleges.py
import asyncio
import json
import re
from typing import List, Dict
from loguru import logger

from ..core.browser import get_driver
from ..core.fetcher import fetch_json
from ..core.parser import html_select
from ..storage.models import College, get_session

MENU_PATTERN = re.compile(r'collegeList\((\d+)\)')

async def discover_colleges(root_url: str) -> List[Dict]:
    """ë©”ì¸/ëŒ€í•™ ë©”ë‰´ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì„ ê°€ë¡œì±„ ì‹¤ì œ API íŒ¨í„´ì„ ì¶”ì¶œ."""
    logger.info("ğŸ” ëŒ€í•™ ëª©ë¡ íƒìƒ‰ ì¤‘ â€¦")
    with get_driver() as driver:
        driver.get(root_url)
        # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª©ë¡ (Chrome DevTools Protocol ì´ìš©)
        logs = driver.get_log("performance")
    # ë„¤íŠ¸ì›Œí¬ ë¡œê·¸ì—ì„œ collegeList API ì¶”ì¶œ
    api_urls = {json.loads(l["message"])["message"]["params"]["request"]["url"]
                for l in logs
                if "collegeList" in l["message"]}
    # ì˜ˆìƒ: ë‹¨ì¼ URL
    colleges: List[Dict] = []
    for api in api_urls:
        data = await fetch_json(api)
        # [{collegeCd, collegeNm, url}, â€¦] í˜•íƒœ ì˜ˆìƒ
        for c in data:
            colleges.append({"code": c["collegeCd"], "name": c["collegeNm"], "url": c["url"]})
    # DB upsert
    with get_session() as sess:
        for c in colleges:
            obj = sess.query(College).filter_by(code=c["code"]).one_or_none()
            if obj:
                obj.name, obj.url = c["name"], c["url"]
            else:
                obj = College(**c)
                sess.add(obj)
        sess.commit()
    return colleges
