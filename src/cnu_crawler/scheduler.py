# cnu_crawler/scheduler.py
import argparse
import asyncio
from loguru import logger
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from cnu_crawler.config import ROOT_URL, SCHEDULE_MINUTES
from cnu_crawler.storage.models import init_db, get_session, College, Department
from cnu_crawler.spiders.colleges import discover_colleges
from cnu_crawler.spiders.departments import crawl_departments
from cnu_crawler.spiders.notices import crawl_department_notices
from cnu_crawler.storage.csv_sink import dump_daily_csv
from cnu_crawler.core.fetcher import Fetcher

async def run_pipeline():
    # 1) ëŒ€í•™ ëª©ë¡ (ë³€ë™ ì ìŒ) â€“ í•˜ë£¨ 1íšŒë§Œ íƒìƒ‰
    colleges = await discover_colleges(ROOT_URL)
    # 2) í•™ê³¼ ëª©ë¡ + 3) ê³µì§€ì‚¬í•­ ì¦ë¶„
    with get_session() as sess:
        colleges_db = sess.query(College).all()
    for college in colleges_db:
        await crawl_departments(college)
    with get_session() as sess:
        depts = sess.query(Department).all()
    # ë³‘ë ¬ í¬ë¡¤ë§
    await asyncio.gather(*(crawl_department_notices(d) for d in depts))
    dump_daily_csv()

async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--init", action="store_true",
                        help="DB ì´ˆê¸°í™”ë§Œ ìˆ˜í–‰ í›„ ì¢…ë£Œ")
    parser.add_argument("--run", action="store_true",
                        help="ë‹¨ì¼ ì‹¤í–‰(ìŠ¤ì¼€ì¤„ëŸ¬ ì—†ì´)")
    args = parser.parse_args()

    if args.init:
        init_db()
        print("DB initialized.")
        return

    if args.run:
        await run_pipeline()
        return

    # ìŠ¤ì¼€ì¤„ëŸ¬
    init_db()
    sched = AsyncIOScheduler(timezone="Asia/Seoul")
    sched.add_job(run_pipeline, "interval", minutes=SCHEDULE_MINUTES, next_run_time=None)
    sched.start()
    logger.info(f"ğŸš€ Scheduler started. every {SCHEDULE_MINUTES} min")
    try:
        await asyncio.Event().wait()  # keep running
    finally:
        await Fetcher.instance().close()

if __name__ == "__main__":
    asyncio.run(main())
