# cnu_crawler/spiders/colleges.py
import re
from typing import List, Dict
from urllib.parse import urljoin  # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜í•˜ê¸° ìœ„í•¨

from loguru import logger
from selenium.webdriver.common.by import By  # XPath ë“±ìœ¼ë¡œ ìš”ì†Œë¥¼ ì°¾ê¸° ìœ„í•¨
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from cnu_crawler.core.browser import get_driver
from cnu_crawler.storage import College, get_session
from cnu_crawler.utils import clean_text  # í…ìŠ¤íŠ¸ ì •ì œìš©


async def discover_colleges(root_url: str) -> List[Dict]:
    """
    ë©”ì¸ í˜ì´ì§€ì—ì„œ ì§ì ‘ HTMLì„ íŒŒì‹±í•˜ì—¬ ëŒ€í•™ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ ì œê³µí•œ XPath ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” ëŒ€í•™ ëª©ë¡ íƒìƒ‰ ì¤‘ (HTML ì§ì ‘ íŒŒì‹± ë°©ì‹): {root_url}")
    colleges_data: List[Dict] = []

    COLLEGES_CONTAINER_XPATH = "/html/body/div[3]/div/div[3]"
    INDIVIDUAL_COLLEGE_LINK_XPATH = ".//ul//li/a"

    try:
        with get_driver() as driver:
            driver.get(root_url)

            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.XPATH, COLLEGES_CONTAINER_XPATH))
                )
                logger.info(f"ëŒ€í•™ ëª©ë¡ ì»¨í…Œì´ë„ˆ XPath '{COLLEGES_CONTAINER_XPATH}' ë°œê²¬ë¨.")
            except Exception as e_wait:
                logger.error(f"ëŒ€í•™ ëª©ë¡ ì»¨í…Œì´ë„ˆ XPath '{COLLEGES_CONTAINER_XPATH}'ë¥¼ ì°¾ëŠ” ì¤‘ íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜: {e_wait}")
                logger.debug(f"í˜„ì¬ í˜ì´ì§€ ì†ŒìŠ¤ (ì¼ë¶€): {driver.page_source[:1000]}")
                return []

            container_element = driver.find_element(By.XPATH, COLLEGES_CONTAINER_XPATH)
            college_link_elements = container_element.find_elements(By.XPATH, INDIVIDUAL_COLLEGE_LINK_XPATH)

            if not college_link_elements:
                logger.warning(
                    f"ì»¨í…Œì´ë„ˆ('{COLLEGES_CONTAINER_XPATH}') ë‚´ì—ì„œ ëŒ€í•™ ë§í¬ ('{INDIVIDUAL_COLLEGE_LINK_XPATH}')ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. XPath ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° í™•ì¸ í•„ìš”.")
                return []

            logger.info(f"{len(college_link_elements)}ê°œì˜ ëŒ€í•™ ë§í¬ ë°œê²¬.")

            for idx, link_element in enumerate(college_link_elements):
                try:
                    # .text ëŒ€ì‹  get_attribute("textContent") ì‚¬ìš©
                    college_name_raw = link_element.get_attribute("textContent")
                    if college_name_raw is None:  # textContentê°€ nullì¼ ìˆ˜ë„ ìˆìŒ
                        college_name_raw = ""
                    college_name = clean_text(college_name_raw)

                    college_url = link_element.get_attribute("href")

                    if not college_name:
                        logger.warning(
                            f"ë§í¬ ìš”ì†Œì—ì„œ ëŒ€í•™ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì¸ë±ìŠ¤: {idx}, ìš”ì†Œ HTML: {link_element.get_attribute('outerHTML')[:150]}). ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    if not college_url:
                        logger.warning(f"ë§í¬ ìš”ì†Œì—ì„œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì´ë¦„: {college_name}). ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    college_url = urljoin(root_url, college_url)

                    url_path_segments = [part for part in college_url.split('/') if
                                         part and part not in ('http:', 'https:', '')]
                    if url_path_segments:
                        college_code_candidate = url_path_segments[-1].split('?')[0].split('#')[0]
                    else:
                        college_code_candidate = college_name

                    college_code = re.sub(r'\s+', '-', college_code_candidate.lower())
                    college_code = re.sub(r'[^a-z0-9-_.]', '', college_code)[:50]
                    if not college_code:
                        college_code = f"college-{idx + 1}"

                    colleges_data.append({
                        "code": college_code,
                        "name": college_name,
                        "url": college_url
                    })
                    logger.debug(f"ì¶”ì¶œëœ ëŒ€í•™: code='{college_code}', name='{college_name}', url='{college_url}'")

                except Exception as e_parse_element:
                    logger.error(f"ê°œë³„ ëŒ€í•™ ë§í¬ ìš”ì†Œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ì¸ë±ìŠ¤: {idx}): {e_parse_element}")
                    logger.debug(f"ì˜¤ë¥˜ ë°œìƒ ìš”ì†Œ HTML (ì¼ë¶€): {link_element.get_attribute('outerHTML')[:200]}")

    except Exception as e_main:
        logger.opt(exception=True).error(f"discover_colleges (HTML íŒŒì‹±) ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e_main}")
        return []

    if not colleges_data:
        logger.warning("ì¶”ì¶œëœ ëŒ€í•™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    try:
        with get_session() as sess:
            updated_count = 0
            added_count = 0
            for c_data in colleges_data:
                obj = sess.query(College).filter_by(code=c_data["code"]).one_or_none()
                if obj:
                    if obj.name != c_data["name"] or obj.url != c_data["url"]:
                        obj.name = c_data["name"]
                        obj.url = c_data["url"]
                        updated_count += 1
                else:
                    obj = College(**c_data)
                    sess.add(obj)
                    added_count += 1
            if updated_count > 0 or added_count > 0:
                sess.commit()
                logger.success(f"ëŒ€í•™ ì •ë³´ DB ì—…ë°ì´íŠ¸: {added_count}ê°œ ì¶”ê°€, {updated_count}ê°œ ìˆ˜ì • (ì´ {len(colleges_data)}ê°œ ì²˜ë¦¬).")
            else:
                logger.info("DBì— ë³€ê²½ëœ ëŒ€í•™ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e_db:
        logger.opt(exception=True).error(f"ëŒ€í•™ ì •ë³´ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

    return colleges_data