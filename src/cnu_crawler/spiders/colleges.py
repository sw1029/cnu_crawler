# src/cnu_crawler/spiders/colleges.py
import asyncio
import re
from typing import List, Dict, Optional
from urllib.parse import urljoin

from loguru import logger
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from cnu_crawler.core.browser import get_driver
from cnu_crawler.core.fetcher import fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import College, get_session
from cnu_crawler.utils import clean_text
from cnu_crawler.config import ROOT_URL


# --- Helper Functions ---
def _generate_college_code(name: str, prefix: str = "coll") -> str:
    """ëŒ€í•™ ì´ë¦„ê³¼ ì ‘ë‘ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    cleaned_name = re.sub(r'\s+', '', name.lower())
    alnum_name = re.sub(r'[^a-z0-9]', '', cleaned_name)

    # hash(name)ì˜ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ìŠ¬ë¼ì´ì‹±í•©ë‹ˆë‹¤.
    # hash() ê²°ê³¼ê°€ ìŒìˆ˜ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, str() ë³€í™˜ í›„ '-' ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜,
    # hex()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ í˜•ì‹ì˜ ë¬¸ìì—´ì„ ì–»ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ str()ì„ ì‚¬ìš©í•˜ê³ , ìŒìˆ˜ ë¶€í˜¸ê°€ í¬í•¨ë  ìˆ˜ ìˆìŒì„ ì¸ì§€í•©ë‹ˆë‹¤.
    # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ hex(hash(name))ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, str(abs(hash(name)))ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    hash_str_part = str(hash(name)).replace('-', '')[:6]  # ìŒìˆ˜ ë¶€í˜¸ ì œê±° í›„ 6ìë¦¬

    return f"{prefix}_{alnum_name[:20]}_{hash_str_part}"  # ìˆ˜ì •ëœ ë¶€ë¶„


def _save_colleges_to_db(colleges_data: List[Dict], log_prefix: str):
    """ìˆ˜ì§‘ëœ College ì •ë³´ë¥¼ DBì— ì €ì¥í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    if not colleges_data:
        logger.info(f"[{log_prefix}] DBì— ì €ì¥í•  College ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    with get_session() as sess:
        added_count = 0
        updated_count = 0
        for c_data in colleges_data:
            existing_college = sess.query(College).filter_by(code=c_data["code"]).one_or_none()
            if existing_college:
                changed = False
                if existing_college.name != c_data["name"]:
                    existing_college.name = c_data["name"]
                    changed = True
                if existing_college.url != c_data["url"]:
                    existing_college.url = c_data["url"]
                    changed = True
                if existing_college.college_type != c_data.get("college_type", existing_college.college_type):
                    existing_college.college_type = c_data.get("college_type", existing_college.college_type)
                    changed = True

                if changed:
                    updated_count += 1
                    logger.trace(f"[{log_prefix}] ê¸°ì¡´ College ì—…ë°ì´íŠ¸: code='{c_data['code']}', name='{c_data['name']}'")
            else:
                new_college = College(**c_data)
                sess.add(new_college)
                added_count += 1
                logger.trace(f"[{log_prefix}] ìƒˆ College ì¶”ê°€: code='{c_data['code']}', name='{c_data['name']}'")

        if added_count > 0 or updated_count > 0:
            try:
                sess.commit()
                logger.success(f"[{log_prefix}] College ì •ë³´ DB ì—…ë°ì´íŠ¸: {added_count}ê°œ ì¶”ê°€, {updated_count}ê°œ ìˆ˜ì •.")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{log_prefix}] College ì •ë³´ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")
                sess.rollback()
        else:
            logger.info(f"[{log_prefix}] DBì— ë³€ê²½ëœ College ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


# --- Main Discover Functions ---

async def discover_plus_normal_colleges(root_url: str = ROOT_URL) -> List[Dict]:
    logger.info(f"ğŸ” ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ ëª©ë¡ íƒìƒ‰ ì‹œì‘ (ì¶œì²˜: {root_url})")  #
    colleges_data: List[Dict] = []
    COLLEGES_CONTAINER_XPATH = "/html/body/div[3]/div/div[3]"
    INDIVIDUAL_COLLEGE_LINK_XPATH = ".//ul//li/a"

    try:
        with get_driver() as driver:
            driver.get(root_url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.XPATH, COLLEGES_CONTAINER_XPATH))
            )
            container_element = driver.find_element(By.XPATH, COLLEGES_CONTAINER_XPATH)
            college_link_elements = container_element.find_elements(By.XPATH, INDIVIDUAL_COLLEGE_LINK_XPATH)

            if not college_link_elements:
                logger.warning(
                    f"[{root_url}] ì»¨í…Œì´ë„ˆ('{COLLEGES_CONTAINER_XPATH}') ë‚´ì—ì„œ ëŒ€í•™ ë§í¬ ('{INDIVIDUAL_COLLEGE_LINK_XPATH}')ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            logger.info(f"[{root_url}] {len(college_link_elements)}ê°œì˜ ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ ë§í¬ í›„ë³´ ë°œê²¬.")  #
            for idx, link_element in enumerate(college_link_elements):
                college_name_raw = link_element.get_attribute("textContent")
                college_name = clean_text(college_name_raw if college_name_raw else "")
                college_url_raw = link_element.get_attribute("href")

                if not college_name or not college_url_raw:
                    logger.warning(f"ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ ë§í¬ì—ì„œ ì´ë¦„ ë˜ëŠ” URL ëˆ„ë½ (ì¸ë±ìŠ¤: {idx}). ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                college_url = urljoin(root_url, college_url_raw)
                college_code = _generate_college_code(college_name, prefix="plus_normal")  #

                colleges_data.append({
                    "code": college_code,
                    "name": college_name,
                    "url": college_url,
                    "college_type": "normal_college"
                })

        _save_colleges_to_db(colleges_data, "Plus ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™")
        return colleges_data
    except Exception as e:
        logger.opt(exception=True).error(f"Plus ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ ëª©ë¡ íƒìƒ‰ ì¤‘ ì˜ˆì™¸: {e}")  #
        return []


async def discover_grad_page_colleges_and_depts(grad_info_url: str = "https://grad.cnu.ac.kr/grad/grad/normal-grad.do"):
    logger.info(f"ğŸ“ ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€({grad_info_url})ì—ì„œ 'ëŒ€í•™' ë‹¨ìœ„(ì†Œì†) íƒìƒ‰ ì‹œì‘...")  #
    colleges_data: List[Dict] = []

    try:
        html_content = await fetch_text(grad_info_url)
        h4_elements = html_select(html_content, "h4")

        processed_college_names = set()

        if not h4_elements:
            logger.warning(f"'{grad_info_url}' ì—ì„œ <h4> íƒœê·¸ (ëŒ€í•™ëª… í›„ë³´)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info(f"'{grad_info_url}' ì—ì„œ {len(h4_elements)}ê°œì˜ <h4> íƒœê·¸ ë°œê²¬. 'ëŒ€í•™'ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ í•„í„°ë§ ì‹œë„.")  #
            for name_raw in h4_elements:
                name = clean_text(name_raw)
                if name.endswith("ëŒ€í•™") and len(name) > 3 and name not in processed_college_names \
                        and not any(ex in name for ex in ["ê³µì§€ì‚¬í•­", "ìë£Œì‹¤"]):
                    college_code = _generate_college_code(name, prefix="gradpage")  #
                    colleges_data.append({
                        "code": code,
                        "name": f"{name}(ì¼ë°˜ëŒ€í•™ì›ì†Œì†)",
                        "url": grad_info_url,
                        "college_type": "grad_page_college"
                    })
                    processed_college_names.add(name)
            logger.info(f"ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ì—ì„œ {len(colleges_data)}ê°œì˜ 'ëŒ€í•™' ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ.")

        if not colleges_data:
            logger.info(f"ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ì—ì„œ ê°œë³„ 'ëŒ€í•™' ë‹¨ìœ„ë¥¼ ì°¾ì§€ ëª»í•´ 'ì¼ë°˜ëŒ€í•™ì›' ì „ì²´ë¥¼ í•˜ë‚˜ì˜ Collegeë¡œ ë“±ë¡í•©ë‹ˆë‹¤.")
            colleges_data.append({
                "code": "grad_school_main_unit",
                "name": "ì¼ë°˜ëŒ€í•™ì›(ì „ì²´)",
                "url": grad_info_url,
                "college_type": "grad_page_college"
            })

        _save_colleges_to_db(colleges_data, "ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ ê¸°ë°˜ 'ëŒ€í•™' ë‹¨ìœ„")
        return colleges_data
    except Exception as e:
        logger.opt(exception=True).error(f"ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€({grad_info_url}) 'ëŒ€í•™' ë‹¨ìœ„ íƒìƒ‰ ì¤‘ ì˜ˆì™¸: {e}")  #
        return []


async def discover_plus_all_graduate_schools(plus_url: str = ROOT_URL) -> List[Dict]:
    logger.info(f"ğŸ” Plus ({plus_url}) ì „ì²´ ëŒ€í•™ì› ëª©ë¡ íƒìƒ‰ ì‹œì‘...")
    colleges_data: List[Dict] = []
    GRADUATE_SCHOOL_LINK_XPATH = "/html/body/div[3]/div/div[2]/ul/li/a"

    try:
        with get_driver() as driver:
            driver.get(plus_url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_all_elements_located((By.XPATH, GRADUATE_SCHOOL_LINK_XPATH))
            )
            grad_school_link_elements = driver.find_elements(By.XPATH, GRADUATE_SCHOOL_LINK_XPATH)

            if not grad_school_link_elements:
                logger.warning(f"Plus({plus_url})ì—ì„œ ì „ì²´ ëŒ€í•™ì› ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (XPath: {GRADUATE_SCHOOL_LINK_XPATH}).")
                return []

            logger.info(f"Plus({plus_url})ì—ì„œ {len(grad_school_link_elements)}ê°œì˜ ì „ì²´ ëŒ€í•™ì› ë§í¬ í›„ë³´ ë°œê²¬.")
            for idx, link_element in enumerate(grad_school_link_elements):
                name_raw = link_element.text
                name = clean_text(name_raw if name_raw else "")
                url_raw = link_element.get_attribute("href")

                if not name or not url_raw:
                    logger.warning(f"Plus ì „ì²´ ëŒ€í•™ì› ë§í¬ì—ì„œ ì´ë¦„ ë˜ëŠ” URL ëˆ„ë½ (ì¸ë±ìŠ¤: {idx}). ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                url = urljoin(plus_url, url_raw)
                college_type = "plus_general_grad" if (idx == 0) else "plus_special_grad"
                code_prefix = "plus_gen_grad" if college_type == "plus_general_grad" else "plus_spec_grad"
                college_code = _generate_college_code(name, prefix=code_prefix)

                colleges_data.append({
                    "code": college_code,
                    "name": name,
                    "url": url,
                    "college_type": college_type
                })

        _save_colleges_to_db(colleges_data, "Plus ì „ì²´ ëŒ€í•™ì›")
        return colleges_data
    except Exception as e:
        logger.opt(exception=True).error(f"Plus ì „ì²´ ëŒ€í•™ì› ëª©ë¡ íƒìƒ‰ ì¤‘ ì˜ˆì™¸: {e}")
        return []


async def discover_all_colleges_entrypoint():
    logger.info("ëª¨ë“  College ì •ë³´ ìˆ˜ì§‘ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    await discover_plus_normal_colleges(ROOT_URL)
    await discover_grad_page_colleges_and_depts("https://grad.cnu.ac.kr/grad/grad/normal-grad.do")
    await discover_plus_all_graduate_schools(ROOT_URL)
    logger.info("ëª¨ë“  College ì •ë³´ ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ.")