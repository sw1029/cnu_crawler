# src/cnu_crawler/spiders/notices.py
import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Coroutine, Any, Tuple
from urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode

from loguru import logger
from aiohttp import ClientError

from cnu_crawler.core.fetcher import fetch_text, fetch_json
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session  #
from cnu_crawler.utils import clean_text, parse_date_flexible  #
from cnu_crawler.config import (  #
    REQUEST_DELAY_NOTICE_PAGE_SECONDS,
    REQUEST_DELAY_DEPARTMENT_SECONDS
)

# ê²Œì‹œíŒ ìœ í˜• ìƒìˆ˜ ì •ì˜
BOARD_TYPE_ACADEMIC = "academic"
BOARD_TYPE_UNDERGRAD = "undergrad"
BOARD_TYPE_GRAD = "grad"
BOARD_TYPE_GRAD_KEYWORD = "grad_keyword_found"


# BOARD_CODESëŠ” ì´ì œ ì‚¬ìš©ë˜ì§€ ì•Šê±°ë‚˜, get_notice_list_urlì—ì„œ ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.
# ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ "ì—…ë°ì´íŠ¸í•œ urlë§Œ ì‚¬ìš©"ì— ë”°ë¼, BOARD_CODES ê¸°ë°˜ì˜ URL ìƒì„±ì€ ì œê±°í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# BOARD_CODES = {
#     BOARD_TYPE_UNDERGRAD: "board?code=undergrad_notice",
#     BOARD_TYPE_GRAD: "board?code=grad_notice"
# }

def college_code_from_url(college_url: str) -> Optional[str]:  # ì´ì „ ë‹µë³€ì—ì„œ ì¶”ê°€ë¨
    try:
        hostname = college_url.split('/')[2]
        return hostname.split('.')[0]
    except IndexError:
        logger.warning(f"URLì—ì„œ ëŒ€í•™ ì½”ë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {college_url}")
        return None


def get_notice_list_url(dept: Department, board_type: str, page: int) -> Optional[str]:
    """
    Department ê°ì²´ì— ì €ì¥ëœ URL í…œí”Œë¦¿ê³¼ board_typeì„ ì‚¬ìš©í•˜ì—¬ ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ Noneì„ ë°˜í™˜í•˜ì—¬ í•´ë‹¹ ê²Œì‹œíŒ ìˆ˜ì§‘ì„ ê±´ë„ˆë›°ë„ë¡ í•©ë‹ˆë‹¤.
    """
    url_template: Optional[str] = None

    if board_type == BOARD_TYPE_ACADEMIC:
        url_template = dept.academic_notice_url_template
    elif board_type == BOARD_TYPE_UNDERGRAD:
        url_template = dept.undergrad_notice_url_template
    elif board_type == BOARD_TYPE_GRAD:
        url_template = dept.grad_notice_url_template
    elif board_type == BOARD_TYPE_GRAD_KEYWORD:
        url_template = dept.specific_grad_keyword_notice_url

    if not url_template:
        logger.trace(f"[{dept.name}] ê²Œì‹œíŒ ìœ í˜• '{board_type}'ì— ëŒ€í•œ URL í…œí”Œë¦¿ì´ DBì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None  # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ URL ìƒì„± ë¶ˆê°€ -> ì´ ê²Œì‹œíŒ ìœ í˜•ì€ ìˆ˜ì§‘ ì•ˆ í•¨

    try:
        # URL í…œí”Œë¦¿ì— í˜ì´ì§€ ë²ˆí˜¸ í”Œë ˆì´ìŠ¤í™€ë” ì²˜ë¦¬ (ì˜ˆ: {page} ë˜ëŠ” {})
        if "{page}" in url_template:
            return url_template.replace("{page}", str(page))
        elif "{}" in url_template:  # ë‹¨ìˆœ format í”Œë ˆì´ìŠ¤í™€ë”
            return url_template.format(page)
        else:
            # URL í…œí”Œë¦¿ì— í˜ì´ì§€ í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì—†ëŠ” ê²½ìš°, í˜ì´ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹.
            # ì´ ë°©ì‹ì€ URL í…œí”Œë¦¿ì´ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì—†ì´ë„ ìœ íš¨í•œ ëª©ë¡ URLì¼ ë•Œë¥¼ ê°€ì •í•©ë‹ˆë‹¤.
            # ë˜ëŠ” í…œí”Œë¦¿ ìì²´ê°€ ì´ë¯¸ page=1ì„ í¬í•¨í•˜ê³  ìˆì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” ì¼ë°˜ì ì¸ page íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            parsed_template = urlparse(url_template)
            query_params = parse_qs(parsed_template.query)

            page_param_name = "page"  # ê¸°ë³¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì´ë¦„
            # ì‹¤ì œë¡œëŠ” ë‹¤ì–‘í•œ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì´ë¦„(pageNo, p, pg ë“±)ì„ ê³ ë ¤í•´ì•¼ í•  ìˆ˜ ìˆìŒ
            # ë˜ëŠ” URL í…œí”Œë¦¿ ìì²´ì— ì´ ì •ë³´ê°€ í¬í•¨ë˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŒ.

            query_params[page_param_name] = [str(page)]
            new_query = urlencode(query_params, doseq=True)
            # fragmentëŠ” ëª©ë¡ URLì— ì¼ë°˜ì ìœ¼ë¡œ ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œê±°
            return urlunparse((parsed_template.scheme, parsed_template.netloc, parsed_template.path,
                               parsed_template.params, new_query, ''))

    except Exception as e:
        logger.error(f"[{dept.name}] URL í…œí”Œë¦¿ ('{url_template}') ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (page={page}, board_type='{board_type}'): {e}")
        return None


async def _parse_notice_page_content(dept: Department, board_type: str, list_url: str, last_post_id_db: str) -> Tuple[
    List[Dict], bool]:
    # ì´ í•¨ìˆ˜ì˜ ë‚´ìš©ì€ ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ (JSON ìš°ì„  íŒŒì‹±, HTML Fallback, ì¦ë¶„ ë¹„êµ ë“±)
    # ë‹¨, Notice ì €ì¥ ì‹œ source_display_name ì„¤ì • ë¡œì§ì€ board_typeì— ë”°ë¼ ìœ ì§€
    posts_data: List[Dict] = []
    stop_crawling = False
    fetch_successful = False

    try:
        logger.trace(f"[{dept.name} ({board_type})] JSON API ì‹œë„: {list_url}")
        data = await fetch_json(list_url)  #
        current_page_posts = data.get("posts") if isinstance(data, dict) else data

        if not isinstance(current_page_posts, list):
            logger.warning(
                f"[{dept.name} ({board_type})] JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ ({list_url}). HTML Fallback ì‹œë„ ì˜ˆì •. ë°ì´í„°: {str(data)[:200]}")
            raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

        logger.trace(f"[{dept.name} ({board_type})] JSON API ì„±ê³µ. {len(current_page_posts)}ê°œ í•­ëª© ìˆ˜ì‹ .")
        for p_item in current_page_posts:
            post_id_str = str(p_item.get("id", "")).strip()
            title = clean_text(str(p_item.get("title", "")))
            raw_url = p_item.get("url", "")
            date_str = p_item.get("date", "")

            if not all([post_id_str, title, raw_url, date_str]):
                logger.warning(f"[{dept.name} ({board_type})] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½: {p_item}")
                continue

            if post_id_str.isdigit() and last_post_id_db.isdigit():
                if int(post_id_str) <= int(last_post_id_db): stop_crawling = True; break
            elif post_id_str <= last_post_id_db and post_id_str != "":
                stop_crawling = True; break

            parsed_date = parse_date_flexible(date_str)  #
            if not parsed_date: logger.warning(
                f"[{dept.name} ({board_type})] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

            full_url = urljoin(list_url, raw_url)
            notice_item = {"dept_id": dept.id, "board": board_type, "post_id": post_id_str,
                           "title": title, "url": full_url, "posted_at": parsed_date}
            if board_type == BOARD_TYPE_GRAD_KEYWORD:
                notice_item["source_display_name"] = f"{dept.name} ëŒ€í•™ì›"  #
            posts_data.append(notice_item)

        if posts_data: fetch_successful = True
        if stop_crawling: return posts_data, stop_crawling

    except (ClientError, json.JSONDecodeError, ValueError, Exception) as e_json:
        log_msg_prefix = f"[{dept.name} ({board_type})] JSON API"
        if isinstance(e_json, ClientError) and hasattr(e_json, 'status') and e_json.status == 404:  #
            logger.warning(f"{log_msg_prefix} í˜¸ì¶œ ì‹¤íŒ¨ - 404 Not Found ({list_url}). HTML Fallback ì‹œë„.")
        elif isinstance(e_json, json.JSONDecodeError):
            logger.warning(f"{log_msg_prefix} íŒŒì‹± ì‹¤íŒ¨ ({list_url}): {e_json}. HTML Fallback ì‹œë„.")
        elif isinstance(e_json, asyncio.TimeoutError):  #
            logger.warning(f"{log_msg_prefix} í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({list_url}). HTML Fallback ì‹œë„.")
        elif isinstance(e_json, ClientError):  #
            logger.warning(
                f"{log_msg_prefix} í˜¸ì¶œ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")
        else:
            logger.warning(
                f"{log_msg_prefix} ì²˜ë¦¬ ì¤‘ ê¸°íƒ€ ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")

        try:
            logger.trace(f"[{dept.name} ({board_type})] HTML Fallback ì‹œë„: {list_url}")
            html_content = await fetch_text(list_url)

            ids_html = html_select(html_content, "td.no")  #
            titles_html = html_select(html_content, "td.title a")  #
            links_html = html_select(html_content, "td.title a", "href")  #
            dates_html = html_select(html_content, "td.date")  #

            min_len = min(len(ids_html), len(titles_html), len(links_html), len(dates_html))
            if min_len == 0 and sum(map(len, [ids_html, titles_html, links_html, dates_html])) > 0:
                logger.warning(f"[{dept.name} ({board_type})] HTMLì—ì„œ ì¼ë¶€ ì •ë³´ë§Œ ì¶”ì¶œë¨. íŒŒì‹± ê±´ë„ˆëœ€. URL: {list_url}")
            elif min_len > 0:
                logger.trace(f"[{dept.name} ({board_type})] HTML Fallbackìœ¼ë¡œ {min_len}ê°œ í•­ëª© í›„ë³´ ë°œê²¬. URL: {list_url}")

            for i in range(min_len):
                post_id_str = clean_text(ids_html[i])
                if not post_id_str.isdigit():
                    id_match_from_url = re.search(r'(?:idx|id|no|seq|docSn)=(\d+)', links_html[i], re.I)
                    if id_match_from_url:
                        post_id_str = id_match_from_url.group(1)
                    else:
                        logger.warning(
                            f"[{dept.name} ({board_type})] HTML í•­ëª© IDê°€ ìˆ«ìê°€ ì•„ë‹ˆê³  URLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ ('{ids_html[i]}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

                if post_id_str.isdigit() and last_post_id_db.isdigit():
                    if int(post_id_str) <= int(last_post_id_db): stop_crawling = True; break
                elif post_id_str <= last_post_id_db and post_id_str != "":
                    stop_crawling = True; break

                title = clean_text(titles_html[i])
                raw_url = links_html[i]
                date_str = dates_html[i]
                parsed_date = parse_date_flexible(date_str)
                if not parsed_date: logger.warning(
                    f"[{dept.name} ({board_type})] HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

                full_url = urljoin(list_url, raw_url)
                notice_item = {"dept_id": dept.id, "board": board_type, "post_id": post_id_str,
                               "title": title, "url": full_url, "posted_at": parsed_date}
                if board_type == BOARD_TYPE_GRAD_KEYWORD:
                    notice_item["source_display_name"] = f"{dept.name} ëŒ€í•™ì›"
                posts_data.append(notice_item)

            if posts_data: fetch_successful = True
            if stop_crawling: return posts_data, stop_crawling

        except ClientError as e_html_fetch:  #
            log_msg_prefix_html = f"[{dept.name} ({board_type})] HTML Fallback"
            if hasattr(e_html_fetch, 'status') and e_html_fetch.status == 404:  #
                logger.error(f"{log_msg_prefix_html} URL ì ‘ê·¼ ì‹¤íŒ¨ - 404 Not Found ({list_url}): {e_html_fetch.message}")
            elif isinstance(e_html_fetch, ClientError):  #
                logger.error(
                    f"{log_msg_prefix_html} URL ì ‘ê·¼ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
            else:
                logger.error(
                    f"{log_msg_prefix_html} URL ì ‘ê·¼ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
        except Exception as e_html_parse:
            logger.error(f"[{dept.name} ({board_type})] HTML Fallback íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {e_html_parse}")

    if not fetch_successful:
        logger.warning(f"[{dept.name} ({board_type})] ìµœì¢…ì ìœ¼ë¡œ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URL: {list_url}")

    return posts_data, stop_crawling


async def crawl_board(dept: Department, board_type: str):
    page = 1
    inserted_count = 0

    logger.info(f"ğŸ“„ [{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œì‘")

    with get_session() as sess:
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_type)
                       .order_by(Notice.post_id.desc())
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"
    logger.debug(f"[{dept.name} ({board_type})] DBì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ID: {last_post_id_db}")

    list_url = get_notice_list_url(dept, board_type, page)
    if not list_url:
        # get_notice_list_url ë‚´ë¶€ì—ì„œ ì´ë¯¸ ë¡œê·¸ë¥¼ ë‚¨ê¸°ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ë¡œê·¸ ì—†ì´ ì¢…ë£Œ
        return

    logger.debug(f"í˜ì´ì§€ {page} ({board_type}) ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")

    posts_to_save, stop_increment_crawl = await _parse_notice_page_content(dept, board_type, list_url, last_post_id_db)

    if stop_increment_crawl and not posts_to_save:
        logger.info(f"[{dept.name} ({board_type})] ì¦ë¶„ ì¡°ê±´ì— ë”°ë¼ ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    elif not posts_to_save:
        logger.info(f"[{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (URL: {list_url}).")

    if posts_to_save:
        try:
            with get_session() as sess:
                sess.bulk_insert_mappings(Notice, posts_to_save)
                sess.commit()
            inserted_count = len(posts_to_save)
            logger.debug(f"[{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ {inserted_count}ê±´ DB ì €ì¥ ì™„ë£Œ.")
        except Exception as e_db:
            logger.opt(exception=True).error(f"[{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_type})] ì²« í˜ì´ì§€ ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:
        logger.info(f"ğŸ“„ [{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def find_and_attempt_parse_board_by_keyword(dept: Department, keywords: List[str], board_type_for_db: str,
                                                  search_url: str) -> bool:
    """
    ì£¼ì–´ì§„ search_urlì—ì„œ keywordsë¥¼ í¬í•¨í•˜ëŠ” ë§í¬ë¥¼ ì°¾ì•„ í•´ë‹¹ ë§í¬ì˜ ì²« í˜ì´ì§€ë§Œ íŒŒì‹± ì‹œë„.
    ì„±ê³µì ìœ¼ë¡œ ë§í¬ë¥¼ ì°¾ê³  í•´ë‹¹ URL í…œí”Œë¦¿ì„ Department ê°ì²´ì— ì„¤ì •í•˜ë©´ True ë°˜í™˜.
    """
    logger.debug(f"[{dept.name}] '{search_url}' ì—ì„œ '{keywords}' í‚¤ì›Œë“œë¡œ '{board_type_for_db}' ê²Œì‹œíŒ ë§í¬ íƒìƒ‰ ì‹œë„...")
    try:
        html_content = await fetch_text(search_url)
        found_board_url_template = None

        all_links_href = html_select(html_content, "a", attr="href")
        all_links_text = html_select(html_content, "a")

        for text, href in zip(all_links_text, all_links_href):
            cleaned_text = clean_text(text)
            if any(kw.lower() in cleaned_text.lower() for kw in keywords):
                potential_url = urljoin(search_url, href)
                parsed_link = urlparse(potential_url)
                query_params = parse_qs(parsed_link.query)
                common_page_params = ['page', 'pageNo', 'pageNum', 'pg', 'p', 'start']
                for p_key in common_page_params: query_params.pop(p_key, None)
                new_query = urlencode(query_params, doseq=True)
                base_link_for_template = urlunparse(
                    (parsed_link.scheme, parsed_link.netloc, parsed_link.path, parsed_link.params, new_query, ''))

                if "?" in base_link_for_template and not base_link_for_template.endswith("?"):
                    found_board_url_template = base_link_for_template + "&page={page}"
                elif "?" not in base_link_for_template:
                    found_board_url_template = base_link_for_template + "?page={page}"
                else:
                    found_board_url_template = base_link_for_template + "page={page}"

                logger.info(
                    f"[{dept.name}] í‚¤ì›Œë“œ '{keywords}' ì¼ì¹˜ ë§í¬ ë°œê²¬: '{cleaned_text}' -> {potential_url}. ìƒì„±ëœ í…œí”Œë¦¿: {found_board_url_template}")
                break

        if found_board_url_template:
            # ì°¾ì€ URL í…œí”Œë¦¿ì„ Department ê°ì²´ì— ì €ì¥ (DBì— ë°˜ì˜)
            with get_session() as sess:
                db_dept = sess.query(Department).filter_by(id=dept.id).first()
                if db_dept:
                    if board_type_for_db == BOARD_TYPE_ACADEMIC:
                        if db_dept.academic_notice_url_template != found_board_url_template:
                            db_dept.academic_notice_url_template = found_board_url_template
                            logger.info(f"[{dept.name}] í•™ì‚¬ê³µì§€ URL í…œí”Œë¦¿ ì—…ë°ì´íŠ¸: {found_board_url_template}")
                    elif board_type_for_db == BOARD_TYPE_UNDERGRAD:  # ì˜ˆì‹œ: ë‹¤ë¥¸ íƒ€ì…ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                        if db_dept.undergrad_notice_url_template != found_board_url_template:
                            db_dept.undergrad_notice_url_template = found_board_url_template
                            logger.info(f"[{dept.name}] í•™ë¶€ê³µì§€ URL í…œí”Œë¦¿ ì—…ë°ì´íŠ¸: {found_board_url_template}")
                    # ... ë‹¤ë¥¸ board_typeì— ëŒ€í•œ ì—…ë°ì´íŠ¸ ë¡œì§ ...
                    elif board_type_for_db == BOARD_TYPE_GRAD_KEYWORD:
                        if db_dept.specific_grad_keyword_notice_url != found_board_url_template:
                            db_dept.specific_grad_keyword_notice_url = found_board_url_template
                            logger.info(f"[{dept.name}] ëŒ€í•™ì›í‚¤ì›Œë“œ ê³µì§€ URL í…œí”Œë¦¿ ì—…ë°ì´íŠ¸: {found_board_url_template}")
                    sess.commit()
                    # í˜„ì¬ dept ê°ì²´ì—ë„ ë°˜ì˜ (ì´ë¯¸ DBì™€ ë™ê¸°í™”ëœ ê°ì²´ë¼ë©´ í•„ìš” ì—†ì„ ìˆ˜ ìˆìœ¼ë‚˜, ëª…ì‹œì  ë°˜ì˜)
                    if hasattr(dept, board_type_for_db.lower() + "_notice_url_template"):
                        setattr(dept, board_type_for_db.lower() + "_notice_url_template", found_board_url_template)
                    elif board_type_for_db == BOARD_TYPE_GRAD_KEYWORD:
                        dept.specific_grad_keyword_notice_url = found_board_url_template

            await crawl_board(dept, board_type_for_db)  # ì—…ë°ì´íŠ¸ëœ í…œí”Œë¦¿ìœ¼ë¡œ ë°”ë¡œ íŒŒì‹± ì‹œë„
            return True
        else:
            logger.info(f"[{dept.name}] '{search_url}'ì—ì„œ '{keywords}' ê´€ë ¨ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        logger.error(f"[{dept.name}] '{search_url}'ì—ì„œ '{keywords}' ê²Œì‹œíŒ íƒìƒ‰/íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return False


async def crawl_grad_keyword_notices_simplified(dept: Department):
    """ "ëŒ€í•™ì›" í‚¤ì›Œë“œ ê´€ë ¨ ê³µì§€ë¥¼ ë‹¨ìˆœí™”ëœ ë°©ì‹ìœ¼ë¡œ íƒìƒ‰ ë° íŒŒì‹± ì‹œë„ """
    if dept.specific_grad_keyword_notice_url:  # ì´ë¯¸ DBì— URL í…œí”Œë¦¿ì´ ìˆë‹¤ë©´ ì‚¬ìš©
        logger.info(f"[{dept.name}] ì´ë¯¸ ì„¤ì •ëœ 'ëŒ€í•™ì›' ê´€ë ¨ ê³µì§€ URL({dept.specific_grad_keyword_notice_url}) ì‚¬ìš© ì‹œë„.")
        await crawl_board(dept, BOARD_TYPE_GRAD_KEYWORD)
    else:
        # í•™ê³¼ ë©”ì¸ í˜ì´ì§€(dept.url)ì—ì„œ "ëŒ€í•™ì› ê³µì§€" ë“±ì˜ í‚¤ì›Œë“œë¡œ ë§í¬ íƒìƒ‰ ì‹œë„
        grad_notice_keywords = ["ëŒ€í•™ì›ê³µì§€", "ëŒ€í•™ì› ê²Œì‹œíŒ", "ëŒ€í•™ì› ìë£Œì‹¤", "ëŒ€í•™ì› ì¼ë°˜ì†Œì‹", "ì„ì‚¬ê³µì§€", "ë°•ì‚¬ê³µì§€"]
        parsed_grad_keyword_board = await find_and_attempt_parse_board_by_keyword(
            dept, grad_notice_keywords, BOARD_TYPE_GRAD_KEYWORD, dept.url
        )
        if not parsed_grad_keyword_board:
            logger.info(f"[{dept.name}] í•™ê³¼ ë©”ì¸ í˜ì´ì§€ì—ì„œ 'ëŒ€í•™ì›' ê´€ë ¨ ëª…ì‹œì  ê³µì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def crawl_department_notices(dept: Department):
    delay_seconds = REQUEST_DELAY_DEPARTMENT_SECONDS
    if delay_seconds > 0:
        logger.trace(f"'{dept.name}' í•™ê³¼ ê³µì§€ì‚¬í•­ ì „ì²´ ìˆ˜ì§‘ ì‹œì‘ ì „ {delay_seconds:.1f}ì´ˆ ëŒ€ê¸°...")
        await asyncio.sleep(delay_seconds)

    parsed_academic = False
    # 1. í•™ì‚¬ê³µì§€: DBì— ì €ì¥ëœ academic_notice_url_template ì‚¬ìš©
    if dept.academic_notice_url_template:
        logger.info(f"[{dept.name}] ì„¤ì •ëœ í•™ì‚¬ê³µì§€ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
        await crawl_board(dept, BOARD_TYPE_ACADEMIC)
        parsed_academic = True
    else:
        # í…œí”Œë¦¿ì´ ì—†ë‹¤ë©´, í•™ê³¼ ë©”ì¸ í˜ì´ì§€ì—ì„œ "í•™ì‚¬ê³µì§€" í‚¤ì›Œë“œë¡œ ë§í¬ë¥¼ ì°¾ì•„ë³´ê³ ,
        # ì°¾ìœ¼ë©´ í•´ë‹¹ ë§í¬ë¥¼ academic_notice_url_templateìœ¼ë¡œ ì—…ë°ì´íŠ¸ í›„ íŒŒì‹± ì‹œë„
        logger.info(f"[{dept.name}] í•™ì‚¬ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. '{dept.url}'ì—ì„œ 'í•™ì‚¬ê³µì§€' í‚¤ì›Œë“œ íƒìƒ‰ ì‹œë„.")
        academic_keywords = ["í•™ì‚¬ê³µì§€", "í•™ì‚¬ì•ˆë‚´", "í•™ë¶€í•™ì‚¬", "í•™ì‚¬ì¼ì •"]
        # find_and_attempt_parse_board_by_keywordê°€ ì„±ê³µí•˜ë©´ ë‚´ë¶€ì ìœ¼ë¡œ crawl_board í˜¸ì¶œ ë° dept ê°ì²´ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸
        parsed_academic = await find_and_attempt_parse_board_by_keyword(
            dept, academic_keywords, BOARD_TYPE_ACADEMIC, dept.url
        )

    # 2. ì¼ë°˜ ê³µì§€ì‚¬í•­ (í•™ë¶€/ëŒ€í•™ì›)
    # í•™ì‚¬ê³µì§€ íŒŒì‹±ì„ ì‹œë„í–ˆëŠ”ì§€ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ (ë˜ëŠ” parsed_academic ì—¬ë¶€ì— ë”°ë¼ ì¡°ê±´ë¶€ë¡œ) ì‹¤í–‰ ê°€ëŠ¥
    # ì—¬ê¸°ì„œëŠ” í•™ì‚¬ê³µì§€ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜, ë˜ëŠ” í•­ìƒ ì¼ë°˜ ê³µì§€ë„ í™•ì¸í•˜ëŠ” ë¡œì§
    if not parsed_academic:  # í•™ì‚¬ê³µì§€ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ì‹œë„í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¼ë°˜ê³µì§€ ì§„í–‰ (ì„ íƒì  ë¡œì§)
        logger.info(f"[{dept.name}] í•™ì‚¬ê³µì§€ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ URL í…œí”Œë¦¿ì´ ì—†ì–´ ì¼ë°˜ ê³µì§€ì‚¬í•­ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

    # í•™ë¶€ ê³µì§€
    if dept.undergrad_notice_url_template:
        logger.info(f"[{dept.name}] ì„¤ì •ëœ í•™ë¶€ ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
        await crawl_board(dept, BOARD_TYPE_UNDERGRAD)
    # elif dept.dept_type not in ["grad_school_dept", ...]: # í•™ë¶€ ê³µì§€ í…œí”Œë¦¿ ì—†ê³ , ëŒ€í•™ì› ì „ìš© ì•„ë‹ˆë©´ ê¸°ë³¸ ì‹œë„ (ì œê±° - ëª…ì‹œì  í…œí”Œë¦¿ë§Œ ì‚¬ìš©)
    #     logger.debug(f"[{dept.name}] í•™ë¶€ ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. ê¸°ë³¸ 'undergrad' íƒ€ì… ì‹œë„ ì•ˆí•¨.")

    # ëŒ€í•™ì› ê³µì§€
    if dept.grad_notice_url_template:
        logger.info(f"[{dept.name}] ì„¤ì •ëœ ëŒ€í•™ì› ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
        await crawl_board(dept, BOARD_TYPE_GRAD)
    # elif dept.dept_type in ["grad_school_dept", ...] or "ëŒ€í•™ì›" in dept.name: # ëŒ€í•™ì› ê³µì§€ í…œí”Œë¦¿ ì—†ê³ , ëŒ€í•™ì› ê´€ë ¨ì´ë©´ ê¸°ë³¸ ì‹œë„ (ì œê±°)
    #    logger.debug(f"[{dept.name}] ëŒ€í•™ì› ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. ê¸°ë³¸ 'grad' íƒ€ì… ì‹œë„ ì•ˆí•¨.")

    # 3. "ëŒ€í•™ì›" í‚¤ì›Œë“œ ê´€ë ¨ ê³µì§€ (ì¡°ê±´ë¶€ ì‹¤í–‰)
    #    (ì´ ë¡œì§ì€ í•™ê³¼ ë©”ì¸ í˜ì´ì§€ë¥¼ ë‹¤ì‹œ ìŠ¤ìº”í•˜ë¯€ë¡œ, ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰)
    if dept.dept_type in ["grad_school_dept", "plus_special_grad_dept", "plus_general_grad_dept"] or \
            "ëŒ€í•™ì›" in dept.name or \
            dept.specific_grad_keyword_notice_url:  # íŠ¹ì • URLì´ ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° í¬í•¨
        await crawl_grad_keyword_notices_simplified(dept)