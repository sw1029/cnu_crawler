# src/cnu_crawler/spiders/notices.py
import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Coroutine, Any  # Coroutine, Any ì¶”ê°€
from urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode

from loguru import logger
from aiohttp import ClientError

from cnu_crawler.core.fetcher import fetch_text, fetch_json
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session
from cnu_crawler.utils import clean_text, parse_date_flexible
from cnu_crawler.config import (
    REQUEST_DELAY_NOTICE_PAGE_SECONDS,
    REQUEST_DELAY_DEPARTMENT_SECONDS
)

# BOARD_TYPESëŠ” Department ëª¨ë¸ì˜ URL í…œí”Œë¦¿ í•„ë“œì™€ ì—°ê´€ì§€ì–´ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
# ë˜ëŠ”, ê° crawl_board í˜¸ì¶œ ì‹œ ëª…ì‹œì ìœ¼ë¡œ board_typeì„ ì§€ì •
BOARD_TYPE_ACADEMIC = "academic"
BOARD_TYPE_UNDERGRAD = "undergrad"
BOARD_TYPE_GRAD = "grad"
BOARD_TYPE_GRAD_KEYWORD = "grad_keyword_found"  # "ëŒ€í•™ì›" í‚¤ì›Œë“œë¡œ ì°¾ì€ ê³µì§€


def get_notice_list_url(dept: Department, board_type: str, page: int) -> Optional[str]:
    """
    Department ê°ì²´ì— ì €ì¥ëœ URL í…œí”Œë¦¿ê³¼ board_typeì„ ì‚¬ìš©í•˜ì—¬ ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    url_template: Optional[str] = None

    if board_type == BOARD_TYPE_ACADEMIC:
        url_template = dept.academic_notice_url_template
    elif board_type == BOARD_TYPE_UNDERGRAD:
        url_template = dept.undergrad_notice_url_template
    elif board_type == BOARD_TYPE_GRAD:
        url_template = dept.grad_notice_url_template
    elif board_type == BOARD_TYPE_GRAD_KEYWORD:
        # specific_grad_keyword_notice_urlì€ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ë‹¨ì¼ URLì¼ ìˆ˜ ìˆìŒ
        # ë˜ëŠ” í˜ì´ì§€ë„¤ì´ì…˜ì´ ìˆëŠ” ëª©ë¡ URLì¼ ìˆ˜ë„ ìˆìŒ. ì—¬ê¸°ì„œëŠ” ëª©ë¡ URL í…œí”Œë¦¿ìœ¼ë¡œ ê°€ì •.
        url_template = dept.specific_grad_keyword_notice_url
    else:
        logger.warning(f"[{dept.name}] ì•Œ ìˆ˜ ì—†ëŠ” board_type: '{board_type}'")
        return None

    if not url_template:
        logger.trace(f"[{dept.name}] ê²Œì‹œíŒ ìœ í˜• '{board_type}'ì— ëŒ€í•œ URL í…œí”Œë¦¿ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    try:
        # URL í…œí”Œë¦¿ì— í˜ì´ì§€ ë²ˆí˜¸ í”Œë ˆì´ìŠ¤í™€ë” ì²˜ë¦¬
        if "{page}" in url_template:
            return url_template.replace("{page}", str(page))
        elif "{}" in url_template:  # ë‹¨ìˆœ format í”Œë ˆì´ìŠ¤í™€ë”
            return url_template.format(page)
        else:
            # í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” URLì´ê±°ë‚˜, ì§ì ‘ ì¶”ê°€í•´ì•¼ í•˜ëŠ” ê²½ìš°
            # í…œí”Œë¦¿ ìì²´ê°€ ì´ë¯¸ ì™„ì „í•œ 1í˜ì´ì§€ URLì¼ ìˆ˜ ìˆìŒ (í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²½ìš°)
            # ì—¬ê¸°ì„œëŠ” í˜ì´ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ëŠ” í˜•íƒœë¡œ ê°€ì •
            parsed_template = urlparse(url_template)
            query_params = parse_qs(parsed_template.query)

            # í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì´ë¦„ ì¶”ë¡  (ë§¤ìš° ê¸°ë³¸ì ì¸ ë°©ì‹)
            page_param_name = "page"  # ê¸°ë³¸ê°’
            # ì‹¤ì œë¡œëŠ” ë” ë§ì€ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì´ë¦„ (pageNo, p, pageNum ë“±)ì„ í™•ì¸í•´ì•¼ í•¨

            query_params[page_param_name] = [str(page)]
            new_query = urlencode(query_params, doseq=True)
            # fragmentëŠ” ìœ ì§€í•˜ì§€ ì•ŠìŒ (ì¼ë°˜ì ìœ¼ë¡œ ëª©ë¡ APIì—ëŠ” fragment ë¶ˆí•„ìš”)
            return urlunparse((parsed_template.scheme, parsed_template.netloc, parsed_template.path,
                               parsed_template.params, new_query, ''))

    except Exception as e:
        logger.error(f"[{dept.name}] URL í…œí”Œë¦¿ ('{url_template}') ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (page={page}, board_type='{board_type}'): {e}")
        return None


async def _parse_notice_page_content(dept: Department, board_type: str, list_url: str, last_post_id_db: str) -> Tuple[
    List[Dict], bool]:
    """
    ì£¼ì–´ì§„ list_urlì—ì„œ ê³µì§€ì‚¬í•­ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤. (JSON ìš°ì„ , ì‹¤íŒ¨ ì‹œ HTML)
    ë°˜í™˜: (ì¶”ì¶œëœ ê³µì§€ì‚¬í•­ dict ë¦¬ìŠ¤íŠ¸, ì¦ë¶„ ìˆ˜ì§‘ ì¤‘ë‹¨ ì—¬ë¶€)
    """
    posts_data: List[Dict] = []
    stop_crawling = False
    fetch_successful = False

    try:  # JSON API ì‹œë„
        logger.trace(f"[{dept.name} ({board_type})] JSON API ì‹œë„: {list_url}")
        data = await fetch_json(list_url)  #
        # FIXME: ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ 'posts' í‚¤ ë° ë‚´ë¶€ í•„ë“œëª…('id', 'title' ë“±) ìˆ˜ì • í•„ìš”
        current_page_posts = data.get("posts") if isinstance(data, dict) else data

        if not isinstance(current_page_posts, list):
            logger.warning(
                f"[{dept.name} ({board_type})] JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ ({list_url}). HTML Fallback ì‹œë„ ì˜ˆì •. ë°ì´í„°: {str(data)[:200]}")
            raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")  # HTML Fallback ìœ ë„

        logger.trace(f"[{dept.name} ({board_type})] JSON API ì„±ê³µ. {len(current_page_posts)}ê°œ í•­ëª© ìˆ˜ì‹ .")
        for p_item in current_page_posts:
            post_id_str = str(p_item.get("id", "")).strip()
            title = clean_text(str(p_item.get("title", "")))
            raw_url = p_item.get("url", "")
            date_str = p_item.get("date", "")

            if not all([post_id_str, title, raw_url, date_str]):
                logger.warning(f"[{dept.name} ({board_type})] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½: {p_item}")
                continue

            if post_id_str.isdigit() and last_post_id_db.isdigit():
                if int(post_id_str) <= int(last_post_id_db): stop_crawling = True; break
            elif post_id_str <= last_post_id_db and post_id_str != "":
                stop_crawling = True; break

            parsed_date = parse_date_flexible(date_str)  #
            if not parsed_date: logger.warning(
                f"[{dept.name} ({board_type})] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

            full_url = urljoin(list_url, raw_url)
            notice_item = {"dept_id": dept.id, "board": board_type, "post_id": post_id_str,
                           "title": title, "url": full_url, "posted_at": parsed_date}
            if board_type == BOARD_TYPE_GRAD_KEYWORD:  # "ëŒ€í•™ì›" í‚¤ì›Œë“œë¡œ ì°¾ì€ ê³µì§€
                notice_item["source_display_name"] = f"{dept.name} ëŒ€í•™ì›"  #
            posts_data.append(notice_item)

        if posts_data: fetch_successful = True
        if stop_crawling: return posts_data, stop_crawling  # ì¦ë¶„ ì¤‘ë‹¨ ì‹œ ë°”ë¡œ ë°˜í™˜

    except (ClientError, json.JSONDecodeError, ValueError, Exception) as e_json:
        # ì—ëŸ¬ ë¡œê¹… (ì´ì „ ë‹µë³€ì˜ ìƒì„¸ ë¡œê¹… ì°¸ê³ í•˜ì—¬ ì ìš©)
        log_msg_prefix = f"[{dept.name} ({board_type})] JSON API"
        if isinstance(e_json, ClientError) and hasattr(e_json, 'status') and e_json.status == 404:
            logger.warning(f"{log_msg_prefix} í˜¸ì¶œ ì‹¤íŒ¨ - 404 Not Found ({list_url}). HTML Fallback ì‹œë„.")
        elif isinstance(e_json, json.JSONDecodeError):
            logger.warning(f"{log_msg_prefix} íŒŒì‹± ì‹¤íŒ¨ ({list_url}): {e_json}. HTML Fallback ì‹œë„.")
        # ... ê¸°íƒ€ ClientError, TimeoutError ë“± ìƒì„¸ ë¡œê¹… ...
        else:
            logger.warning(
                f"{log_msg_prefix} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")

        # HTML Fallback ì‹œë„
        try:
            logger.trace(f"[{dept.name} ({board_type})] HTML Fallback ì‹œë„: {list_url}")
            html_content = await fetch_text(list_url)

            # FIXME: ê° ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ì— ë§ëŠ” ì •í™•í•œ CSS ì„ íƒìë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            # ì•„ë˜ëŠ” ì¼ë°˜ì ì¸ ê²Œì‹œíŒ ëª©ë¡ í…Œì´ë¸” êµ¬ì¡°ì— ëŒ€í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.
            ids_html = html_select(html_content, "td.no")  #
            titles_html = html_select(html_content, "td.title a")  #
            links_html = html_select(html_content, "td.title a", "href")  #
            dates_html = html_select(html_content, "td.date")  #

            min_len = min(len(ids_html), len(titles_html), len(links_html), len(dates_html))
            if min_len == 0 and sum(map(len, [ids_html, titles_html, links_html, dates_html])) > 0:
                logger.warning(f"[{dept.name} ({board_type})] HTMLì—ì„œ ì¼ë¶€ ì •ë³´ë§Œ ì¶”ì¶œë¨. íŒŒì‹± ê±´ë„ˆëœ€. URL: {list_url}")
            elif min_len > 0:
                logger.trace(f"[{dept.name} ({board_type})] HTML Fallbackìœ¼ë¡œ {min_len}ê°œ í•­ëª© í›„ë³´ ë°œê²¬. URL: {list_url}")

            for i in range(min_len):
                post_id_str = clean_text(ids_html[i])
                if not post_id_str.isdigit():  # 'ê³µì§€' ë“± ìˆ«ì ì•„ë‹Œ ID ì²˜ë¦¬
                    id_match_from_url = re.search(r'(?:idx|id|no|seq|docSn)=(\d+)', links_html[i], re.I)
                    if id_match_from_url:
                        post_id_str = id_match_from_url.group(1)
                    else:
                        logger.warning(
                            f"[{dept.name} ({board_type})] HTML í•­ëª© IDê°€ ìˆ«ìê°€ ì•„ë‹ˆê³  URLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ ('{ids_html[i]}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

                if post_id_str.isdigit() and last_post_id_db.isdigit():
                    if int(post_id_str) <= int(last_post_id_db): stop_crawling = True; break
                elif post_id_str <= last_post_id_db and post_id_str != "":
                    stop_crawling = True; break

                title = clean_text(titles_html[i])
                raw_url = links_html[i]
                date_str = dates_html[i]
                parsed_date = parse_date_flexible(date_str)
                if not parsed_date: logger.warning(
                    f"[{dept.name} ({board_type})] HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

                full_url = urljoin(list_url, raw_url)
                notice_item = {"dept_id": dept.id, "board": board_type, "post_id": post_id_str,
                               "title": title, "url": full_url, "posted_at": parsed_date}
                if board_type == BOARD_TYPE_GRAD_KEYWORD:
                    notice_item["source_display_name"] = f"{dept.name} ëŒ€í•™ì›"
                posts_data.append(notice_item)

            if posts_data: fetch_successful = True
            if stop_crawling: return posts_data, stop_crawling

        except ClientError as e_html_fetch:
            log_msg_prefix_html = f"[{dept.name} ({board_type})] HTML Fallback"
            if hasattr(e_html_fetch, 'status') and e_html_fetch.status == 404:
                logger.error(f"{log_msg_prefix_html} URL ì ‘ê·¼ ì‹¤íŒ¨ - 404 Not Found ({list_url}): {e_html_fetch.message}")  #
            # ... ê¸°íƒ€ ClientError ìƒì„¸ ë¡œê¹… ...
            else:
                logger.error(
                    f"{log_msg_prefix_html} URL ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
        except Exception as e_html_parse:
            logger.error(f"[{dept.name} ({board_type})] HTML Fallback íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {e_html_parse}")

    if not fetch_successful:  # JSON, HTML ëª¨ë‘ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ
        logger.warning(f"[{dept.name} ({board_type})] ìµœì¢…ì ìœ¼ë¡œ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URL: {list_url}")

    return posts_data, stop_crawling


async def crawl_board(dept: Department, board_type: str):
    """íŠ¹ì • í•™ê³¼ì˜ íŠ¹ì • ê²Œì‹œíŒ ìœ í˜•ì— ëŒ€í•´ ì²« í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    page = 1  # ì²« í˜ì´ì§€ë§Œ ëŒ€ìƒ
    inserted_count = 0

    logger.info(f"ğŸ“„ [{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œì‘")

    with get_session() as sess:  # DB ì—°ê²° ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_type)
                       .order_by(Notice.post_id.desc())
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"
    logger.debug(f"[{dept.name} ({board_type})] DBì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ID: {last_post_id_db}")

    list_url = get_notice_list_url(dept, board_type, page)
    if not list_url or "invalid_board_key" in list_url:
        logger.error(f"[{dept.name} ({board_type})] ìœ íš¨í•œ ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ìƒì„±í•  ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        # ì´ì „ ë¡œê·¸ì—ì„œ ì´ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë¯€ë¡œ, get_notice_list_urlì´ í•­ìƒ ìœ íš¨í•œ ë¬¸ìì—´ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
        # ë˜ëŠ” URL ìƒì„± ì‹¤íŒ¨ì‹œ Noneì„ ë°˜í™˜í•˜ê³  ì—¬ê¸°ì„œ ì²´í¬
        if not list_url: return

    logger.debug(f"í˜ì´ì§€ {page} ({board_type}) ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")

    # _parse_notice_page_content í•¨ìˆ˜ í˜¸ì¶œ (ì²« í˜ì´ì§€ë§Œ)
    # ì´ í•¨ìˆ˜ëŠ” (íŒŒì‹±ëœ ê³µì§€ ë¦¬ìŠ¤íŠ¸, ì¦ë¶„ ì¤‘ë‹¨ ì—¬ë¶€)ë¥¼ ë°˜í™˜
    posts_to_save, stop_increment_crawl = await _parse_notice_page_content(dept, board_type, list_url, last_post_id_db)

    if stop_increment_crawl and not posts_to_save:  # ì¦ë¶„ìœ¼ë¡œ ì¸í•´ ê°€ì ¸ì˜¬ ìƒˆ ê¸€ì´ ì—†ëŠ” ê²½ìš°
        logger.info(f"[{dept.name} ({board_type})] ì¦ë¶„ ì¡°ê±´ì— ë”°ë¼ ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    elif not posts_to_save:  # ì¦ë¶„ ì¤‘ë‹¨ì€ ì•„ë‹ˆì§€ë§Œ, íŒŒì‹± ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (404, ë¹ˆ í˜ì´ì§€ ë“±)
        logger.info(f"[{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (URL: {list_url}).")  #

    if posts_to_save:
        try:
            with get_session() as sess:
                # Notice ëª¨ë¸ì— source_display_name í•„ë“œê°€ ìˆì–´ì•¼ í•¨
                sess.bulk_insert_mappings(Notice, posts_to_save)
                sess.commit()
            inserted_count = len(posts_to_save)
            logger.debug(f"[{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ {inserted_count}ê±´ DB ì €ì¥ ì™„ë£Œ.")
        except Exception as e_db:
            logger.opt(exception=True).error(f"[{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_type})] ì²« í˜ì´ì§€ ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:  # inserted_countê°€ 0ì¸ ëª¨ë“  ê²½ìš°
        logger.info(f"ğŸ“„ [{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")  #


async def find_and_attempt_parse_board_by_keyword(dept: Department, keywords: List[str], board_type_for_db: str,
                                                  search_url: str) -> bool:
    """
    ì£¼ì–´ì§„ search_urlì—ì„œ keywordsë¥¼ í¬í•¨í•˜ëŠ” ë§í¬ë¥¼ ì°¾ì•„ í•´ë‹¹ ë§í¬ì˜ ì²« í˜ì´ì§€ë§Œ íŒŒì‹± ì‹œë„.
    ì„±ê³µ ì—¬ë¶€ (ë§í¬ë¥¼ ì°¾ê³  íŒŒì‹± ì‹œë„ë¥¼ í–ˆëŠ”ì§€)ë¥¼ ë°˜í™˜.
    """
    logger.debug(f"[{dept.name}] '{search_url}' ì—ì„œ '{keywords}' í‚¤ì›Œë“œë¡œ '{board_type_for_db}' ê²Œì‹œíŒ ë§í¬ íƒìƒ‰ ì‹œë„...")
    try:
        html_content = await fetch_text(search_url)
        found_board_url = None

        all_links_href = html_select(html_content, "a", attr="href")
        all_links_text = html_select(html_content, "a")

        for text, href in zip(all_links_text, all_links_href):
            cleaned_text = clean_text(text)
            if any(kw.lower() in cleaned_text.lower() for kw in keywords):
                # ë§í¬ê°€ ìœ íš¨í•œ ê²Œì‹œíŒ ëª©ë¡ URLì¸ì§€ ì¶”ê°€ ê²€ì¦ í•„ìš” (ì˜ˆ: íŠ¹ì • íŒ¨í„´ í¬í•¨ ì—¬ë¶€)
                # ì—¬ê¸°ì„œëŠ” ì²« ë²ˆì§¸ ë°œê²¬ ë§í¬ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •
                potential_url = urljoin(search_url, href)
                # ì´ë¯¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ ì œê±°í•˜ê³  í…œí”Œë¦¿í™” ì‹œë„
                parsed_link = urlparse(potential_url)
                # queryì—ì„œ page ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œê±° (ë§¤ìš° ë‹¨ìˆœí•œ ë°©ì‹)
                # query_params = parse_qs(parsed_link.query)
                # for page_key in ['page', 'pageNo', 'pageNum', 'pg']: query_params.pop(page_key, None)
                # new_query = urlencode(query_params, doseq=True)
                # base_link_for_template = urlunparse((parsed_link.scheme, parsed_link.netloc, parsed_link.path, parsed_link.params, new_query, ''))

                # ì—¬ê¸°ì„œëŠ” ë°œê²¬ëœ URLì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , get_notice_list_urlì—ì„œ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì²˜ë¦¬
                found_board_url = potential_url
                logger.info(
                    f"[{dept.name}] í‚¤ì›Œë“œ '{keywords}' ì¼ì¹˜ ë§í¬ ë°œê²¬: '{cleaned_text}' -> {found_board_url} (ê²Œì‹œíŒ íƒ€ì…: {board_type_for_db})")
                break

        if found_board_url:
            # Department ê°ì²´ì˜ í•´ë‹¹ board_type URL í…œí”Œë¦¿ì„ ì„ì‹œë¡œ ì„¤ì •í•˜ê±°ë‚˜,
            # get_notice_list_urlì´ ì´ URLì„ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì • í•„ìš”.
            # ì—¬ê¸°ì„œëŠ” Department ëª¨ë¸ì— ì €ì¥ëœ í…œí”Œë¦¿ì´ ìš°ì„ ì´ë¼ê³  ê°€ì •í•˜ê³ ,
            # ë§Œì•½ ì´ í•¨ìˆ˜ê°€ ì°¾ì€ URLì´ ë” ì •í™•í•˜ë‹¤ë©´, í•´ë‹¹ Department ê°ì²´ì˜ URL í…œí”Œë¦¿ì„ ì—…ë°ì´íŠ¸í•´ì•¼ í•¨.
            # ì§€ê¸ˆì€ ì°¾ì€ URLì„ ê¸°ë°˜ìœ¼ë¡œ ì„ì‹œ URL í…œí”Œë¦¿ì„ ë§Œë“¤ì–´ crawl_board í˜¸ì¶œ ì‹œë„.

            # dept ê°ì²´ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ê²ƒì€ side effectë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜.
            # ì—¬ê¸°ì„œëŠ” get_notice_list_urlì´ ì˜ ë™ì‘í•˜ë„ë¡ í•´ë‹¹ í…œí”Œë¦¿ í•„ë“œë¥¼ ì„ì‹œ ì„¤ì •.
            # ë” ì¢‹ì€ ë°©ë²•ì€ crawl_boardê°€ URLì„ ì§ì ‘ ë°›ë„ë¡ í•˜ëŠ” ê²ƒ.
            temp_original_templates = {
                BOARD_TYPE_ACADEMIC: dept.academic_notice_url_template,
                BOARD_TYPE_UNDERGRAD: dept.undergrad_notice_url_template,
                BOARD_TYPE_GRAD: dept.grad_notice_url_template,
                BOARD_TYPE_GRAD_KEYWORD: dept.specific_grad_keyword_notice_url
            }

            # í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  í…œí”Œë¦¿ ìƒì„±
            parsed_found_url = urlparse(found_board_url)
            query_found = parse_qs(parsed_found_url.query)
            if any(p_key in query_found for p_key in ['page', 'pageNo', 'pageNum', 'pg']):  # ì´ë¯¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´
                # í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ {}ë¡œ êµì²´í•˜ëŠ” ì •êµí•œ ë¡œì§ í•„ìš”. ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”.
                # ë˜ëŠ” found_board_urlì„ page=1ë¡œ ê°„ì£¼í•˜ê³  íŒŒì‹±
                # ì—¬ê¸°ì„œëŠ” get_notice_list_urlì´ ì²˜ë¦¬í•˜ë„ë¡ ì›ë³¸ URLì„ í…œí”Œë¦¿ì²˜ëŸ¼ ì‚¬ìš©
                url_template_for_crawl = found_board_url
            else:  # í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ ì—†ë‹¤ë©´ ì¶”ê°€
                url_template_for_crawl = found_board_url + ("&page={}" if "?" in found_board_url else "?page={}")

            if board_type_for_db == BOARD_TYPE_ACADEMIC:
                dept.academic_notice_url_template = url_template_for_crawl
            elif board_type_for_db == BOARD_TYPE_UNDERGRAD:
                dept.undergrad_notice_url_template = url_template_for_crawl
            elif board_type_for_db == BOARD_TYPE_GRAD:
                dept.grad_notice_url_template = url_template_for_crawl
            elif board_type_for_db == BOARD_TYPE_GRAD_KEYWORD:
                dept.specific_grad_keyword_notice_url = url_template_for_crawl

            await crawl_board(dept, board_type_for_db)

            # ì›ë˜ í…œí”Œë¦¿ìœ¼ë¡œ ë³µì› (ì£¼ì˜: ì´ ë°©ì‹ì€ ë™ì‹œì„± ë¬¸ì œ ë°œìƒ ê°€ëŠ¥. ê°ì²´ ìƒíƒœ ë³€ê²½ì€ ì‹ ì¤‘í•´ì•¼ í•¨)
            if board_type_for_db == BOARD_TYPE_ACADEMIC:
                dept.academic_notice_url_template = temp_original_templates[BOARD_TYPE_ACADEMIC]
            elif board_type_for_db == BOARD_TYPE_UNDERGRAD:
                dept.undergrad_notice_url_template = temp_original_templates[BOARD_TYPE_UNDERGRAD]
            elif board_type_for_db == BOARD_TYPE_GRAD:
                dept.grad_notice_url_template = temp_original_templates[BOARD_TYPE_GRAD]
            elif board_type_for_db == BOARD_TYPE_GRAD_KEYWORD:
                dept.specific_grad_keyword_notice_url = temp_original_templates[BOARD_TYPE_GRAD_KEYWORD]

            return True  # ë§í¬ ì°¾ê³  íŒŒì‹± ì‹œë„í•¨
        else:
            logger.info(f"[{dept.name}] '{search_url}'ì—ì„œ '{keywords}' ê´€ë ¨ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return False

    except Exception as e:
        logger.error(f"[{dept.name}] '{search_url}'ì—ì„œ '{keywords}' ê²Œì‹œíŒ íƒìƒ‰/íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return False


async def crawl_grad_keyword_notices_simplified(dept: Department):
    """
    "ëŒ€í•™ì›" í‚¤ì›Œë“œ ê´€ë ¨ ê³µì§€ë¥¼ ë§¤ìš° ë‹¨ìˆœí™”ëœ ë°©ì‹ìœ¼ë¡œ íƒìƒ‰ ë° íŒŒì‹± ì‹œë„.
    í•™ê³¼ ë©”ì¸ í˜ì´ì§€(dept.url)ì—ì„œ "ëŒ€í•™ì›" "ê³µì§€" ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë§í¬ë¥¼ ì°¾ìŒ.
    """
    # "ëŒ€í•™ì›" ìì²´ë¥¼ ì§€ì¹­í•˜ëŠ” ì´ë¦„ì˜ Department ê°ì²´ (ì˜ˆ: dept.name == "ì¼ë°˜ëŒ€í•™ì›")ëŠ” ì´ ë¡œì§ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ
    if "ëŒ€í•™ì›" not in dept.name and dept.dept_type not in ["grad_school_dept", "plus_special_grad_dept",
                                                         "plus_general_grad_dept"]:
        # ì¼ë°˜ í•™ê³¼ì˜ ê²½ìš°, "ëŒ€í•™ì› ê³¼ì •" ë“±ì— ëŒ€í•œ ê³µì§€ê°€ ë³„ë„ë¡œ ìˆëŠ”ì§€ í™•ì¸ ì‹œë„
        # ì´ ë¡œì§ì€ ë§¤ìš° ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ
        logger.debug(f"[{dept.name}] ì¼ë°˜ í•™ê³¼ë¡œ ê°„ì£¼, 'ëŒ€í•™ì› ê³µì§€' ë“± í‚¤ì›Œë“œ íƒìƒ‰ ì‹œë„ (ë§¤ìš° íœ´ë¦¬ìŠ¤í‹±).")

    # Department ëª¨ë¸ì— specific_grad_keyword_notice_urlì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
    if dept.specific_grad_keyword_notice_url:
        logger.info(f"[{dept.name}] ì´ë¯¸ ì„¤ì •ëœ 'ëŒ€í•™ì›' ê´€ë ¨ ê³µì§€ URL ì‚¬ìš© ì‹œë„: {dept.specific_grad_keyword_notice_url}")
        await crawl_board(dept, BOARD_TYPE_GRAD_KEYWORD)  # í˜ì´ì§€ ë²ˆí˜¸ëŠ” get_notice_list_urlì—ì„œ ì²˜ë¦¬
    else:
        # í•™ê³¼ ë©”ì¸ í˜ì´ì§€(dept.url)ì—ì„œ "ëŒ€í•™ì› ê³µì§€", "ëŒ€í•™ì› ê²Œì‹œíŒ", "ì¼ë°˜ì†Œì‹(ëŒ€í•™ì›)" ë“±ì˜ ë§í¬ íƒìƒ‰
        # ì´ ë¶€ë¶„ì€ find_and_attempt_parse_board_by_keyword í•¨ìˆ˜ì™€ ìœ ì‚¬í•œ ë¡œì§ ì‚¬ìš© ê°€ëŠ¥
        grad_notice_keywords = ["ëŒ€í•™ì›ê³µì§€", "ëŒ€í•™ì› ê²Œì‹œíŒ", "ëŒ€í•™ì› ìë£Œì‹¤", "ëŒ€í•™ì› ì¼ë°˜ì†Œì‹", "ì„ì‚¬ê³µì§€", "ë°•ì‚¬ê³µì§€"]
        # "í•™ì‚¬ê³µì§€"ë‚˜ "ì¼ë°˜ì†Œì‹"ì€ ë„ˆë¬´ ì¼ë°˜ì ì´ë¯€ë¡œ "ëŒ€í•™ì›"ê³¼ ì¡°í•©ëœ í‚¤ì›Œë“œ ìš°ì„ 

        parsed_grad_keyword_board = await find_and_attempt_parse_board_by_keyword(
            dept, grad_notice_keywords, BOARD_TYPE_GRAD_KEYWORD, dept.url
        )
        if not parsed_grad_keyword_board:
            logger.info(f"[{dept.name}] í•™ê³¼ ë©”ì¸ í˜ì´ì§€ì—ì„œ 'ëŒ€í•™ì›' ê´€ë ¨ ëª…ì‹œì  ê³µì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            # ì¶”ê°€ì ìœ¼ë¡œ, "ëŒ€í•™ì›"ì´ë¼ëŠ” í…ìŠ¤íŠ¸ ì£¼ë³€ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ëŠ” ê²ƒì€ í˜„ì¬ í”„ë ˆì„ì›Œí¬ì—ì„œ ì–´ë ¤ì›€
            # "10ê¸€ì ì´í•˜ì˜ 'ëŒ€í•™ì›' í¬í•¨ í•­ëª© -> í•˜ìœ„ í•™ì‚¬ê³µì§€/ì¼ë°˜ì†Œì‹ -> ë§í¬ ì—†ìœ¼ë©´ í•´ë‹¹ ì˜ì—­ íŒŒì‹±" ì€
            # í˜„ì¬ aiohttp + beautifulsoup ê¸°ë°˜ìœ¼ë¡œëŠ” ë§¤ìš° ë³µì¡í•˜ê³ , Selenium ë° ì •êµí•œ DOM ë¶„ì„ í•„ìš”


async def crawl_department_notices(dept: Department):
    """
    ì£¼ì–´ì§„ Department ê°ì²´ì— ëŒ€í•´ ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ ê³µì§€ì‚¬í•­ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    1. í•™ì‚¬ê³µì§€ (academic)
    2. ì¼ë°˜ê³µì§€ (undergrad/grad) - í•™ì‚¬ê³µì§€ ì—†ì—ˆì„ ê²½ìš°
    3. ëŒ€í•™ì› í‚¤ì›Œë“œ ê´€ë ¨ ê³µì§€ (grad_keyword_found)
    """
    # ê° ì‘ì—… ì „ ë”œë ˆì´
    delay_seconds = REQUEST_DELAY_DEPARTMENT_SECONDS
    if delay_seconds > 0:
        logger.trace(f"'{dept.name}' í•™ê³¼ ê³µì§€ì‚¬í•­ ì „ì²´ ìˆ˜ì§‘ ì‹œì‘ ì „ {delay_seconds:.1f}ì´ˆ ëŒ€ê¸°...")
        await asyncio.sleep(delay_seconds)

    parsed_any_notice = False

    # 1. "í•™ì‚¬ê³µì§€" ìš°ì„  íƒìƒ‰ ë° íŒŒì‹± ì‹œë„
    # Department ëª¨ë¸ì— academic_notice_url_templateì´ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ì§ì ‘ ì‚¬ìš©
    if dept.academic_notice_url_template:
        logger.info(f"[{dept.name}] ì„¤ì •ëœ í•™ì‚¬ê³µì§€ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
        await crawl_board(dept, BOARD_TYPE_ACADEMIC)
        parsed_any_notice = True  # ì‹œë„ ìì²´ë¥¼ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ë‚´ë¶€ì—ì„œ ê²°ê³¼ ë¡œê¹…)
    else:
        # í•™ê³¼ ë©”ì¸ í˜ì´ì§€(dept.url)ì—ì„œ "í•™ì‚¬ê³µì§€", "í•™ì‚¬ì•ˆë‚´" ë“±ì˜ ë§í¬ë¥¼ ì°¾ì•„ íŒŒì‹± ì‹œë„
        # ì´ ë¡œì§ì€ find_and_attempt_parse_board_by_keyword í•¨ìˆ˜ë¡œ ëŒ€ì²´ ê°€ëŠ¥
        academic_keywords = ["í•™ì‚¬ê³µì§€", "í•™ì‚¬ì•ˆë‚´", "í•™ë¶€í•™ì‚¬"]  # ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ê°€ ê°€ëŠ¥
        parsed_academic = await find_and_attempt_parse_board_by_keyword(
            dept, academic_keywords, BOARD_TYPE_ACADEMIC, dept.url
        )
        if parsed_academic: parsed_any_notice = True

    # 2. "í•™ì‚¬ê³µì§€"ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ íŒŒì‹± ì‹œë„ í›„ ê²°ê³¼ê°€ ì—†ë‹¤ë©´, ì¼ë°˜ ê³µì§€ì‚¬í•­(í•™ë¶€/ëŒ€í•™ì›) íŒŒì‹±
    #    (í˜„ì¬ëŠ” parsed_any_noticeë¡œ ì´ì „ ë‹¨ê³„ì˜ ì„±ê³µ ì—¬ë¶€ë§Œ íŒë‹¨, ì‹¤ì œ ë°ì´í„° ìœ ë¬´ëŠ” crawl_boardì—ì„œ ë¡œê¹…)
    #    ë˜ëŠ”, í•™ì‚¬ê³µì§€ì™€ ë³„ê°œë¡œ í•­ìƒ ì¼ë°˜ê³µì§€ë„ ê°€ì ¸ì˜¤ë ¤ë©´ ì´ if ì¡°ê±´ ì œê±°
    if not parsed_any_notice or dept.undergrad_notice_url_template or dept.grad_notice_url_template:  # í•™ì‚¬ê³µì§€ ì‹œë„ ì•ˆí–ˆê±°ë‚˜, ì¼ë°˜ ê³µì§€ í…œí”Œë¦¿ì´ ìˆë‹¤ë©´
        if not parsed_any_notice:
            logger.info(f"[{dept.name}] í•™ì‚¬ê³µì§€ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ URL í…œí”Œë¦¿ì´ ì—†ì–´ ì¼ë°˜ ê³µì§€ì‚¬í•­ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

        if dept.undergrad_notice_url_template:
            logger.info(f"[{dept.name}] ì„¤ì •ëœ í•™ë¶€ ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_UNDERGRAD)
            if not parsed_any_notice: parsed_any_notice = True  # ì¼ë°˜ ê³µì§€ ì‹œë„ ìì²´ë¥¼ ê¸°ë¡
        elif dept.dept_type not in ["grad_school_dept", "plus_special_grad_dept",
                                    "plus_general_grad_dept"]:  # ëŒ€í•™ì› ì „ìš©ì´ ì•„ë‹Œ ê²½ìš°, ê¸°ë³¸ undergrad ì‹œë„
            logger.debug(f"[{dept.name}] í•™ë¶€ ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. ê¸°ë³¸ 'undergrad' íƒ€ì…ìœ¼ë¡œ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_UNDERGRAD)  # get_notice_list_url ì—ì„œ BOARD_CODES ê¸°ë³¸ê°’ ì‚¬ìš© ì‹œë„
            if not parsed_any_notice: parsed_any_notice = True

        if dept.grad_notice_url_template:  # ëŒ€í•™ì› ê³µì§€ê°€ ì„¤ì •ëœ ê²½ìš°
            logger.info(f"[{dept.name}] ì„¤ì •ëœ ëŒ€í•™ì› ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_GRAD)
            if not parsed_any_notice: parsed_any_notice = True
        elif dept.dept_type in ["grad_school_dept", "plus_special_grad_dept",
                                "plus_general_grad_dept"] or "ëŒ€í•™ì›" in dept.name:
            # ëŒ€í•™ì› ê´€ë ¨ í•™ê³¼ì¸ë° grad_notice_url_templateì´ ì—†ëŠ” ê²½ìš°, ê¸°ë³¸ 'grad' íƒ€ì… ì‹œë„
            logger.debug(f"[{dept.name}] ëŒ€í•™ì› ê´€ë ¨ í•™ê³¼ì´ë‚˜ ëŒ€í•™ì› ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. ê¸°ë³¸ 'grad' íƒ€ì…ìœ¼ë¡œ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_GRAD)
            if not parsed_any_notice: parsed_any_notice = True

    # 3. "ëŒ€í•™ì›" í‚¤ì›Œë“œ ê´€ë ¨ ê³µì§€ íƒìƒ‰ ë° íŒŒì‹± (ì¡°ê±´ë¶€ ì‹¤í–‰)
    #    - dept_typeì´ ëŒ€í•™ì› ê´€ë ¨ì´ê±°ë‚˜, ì´ë¦„ì— 'ëŒ€í•™ì›'ì´ í¬í•¨ëœ ê²½ìš°
    #    - ë˜ëŠ” ëª¨ë“  í•™ê³¼ì— ëŒ€í•´ ì‹œë„í•´ ë³¼ ìˆ˜ë„ ìˆìœ¼ë‚˜, ë¶€í•˜ì™€ ì •í™•ë„ ë¬¸ì œ
    if dept.dept_type in ["grad_school_dept", "plus_special_grad_dept", "plus_general_grad_dept"] or \
            "ëŒ€í•™ì›" in dept.name or \
            dept.specific_grad_keyword_notice_url:  # íŠ¹ì • URLì´ ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° í¬í•¨
        await crawl_grad_keyword_notices_simplified(dept)
        # ì´ ê²°ê³¼ë„ parsed_any_noticeì— ë°˜ì˜í•  ìˆ˜ ìˆìœ¼ë‚˜, ì´ë¯¸ ë‹¤ë¥¸ ê³µì§€ë¥¼ ê°€ì ¸ì™”ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ

    if not parsed_any_notice:
        logger.warning(f"[{dept.name}] ì–´ë–¤ ìœ í˜•ì˜ ê³µì§€ì‚¬í•­ë„ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (URL í…œí”Œë¦¿ ë¶€ì¬ ë˜ëŠ” íƒìƒ‰ ì‹¤íŒ¨).")