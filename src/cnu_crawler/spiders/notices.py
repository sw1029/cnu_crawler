# src/cnu_crawler/spiders/notices.py
import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse, urlunparse  # urlparse, urlunparse ì¶”ê°€

from loguru import logger
from aiohttp import ClientError

from cnu_crawler.core.fetcher import fetch_json, fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session
from cnu_crawler.utils import clean_text, parse_date_flexible
from cnu_crawler.config import (
    REQUEST_DELAY_NOTICE_PAGE_SECONDS,
    REQUEST_DELAY_DEPARTMENT_SECONDS
)

BOARD_CODES = {
    "undergrad": "board?code=undergrad_notice",
    "grad": "board?code=grad_notice"
}


def college_code_from_url(college_url: str) -> Optional[str]:
    try:
        hostname = college_url.split('/')[2]
        return hostname.split('.')[0]
    except IndexError:
        logger.warning(f"URLì—ì„œ ëŒ€í•™ ì½”ë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {college_url}")
        return None


def get_notice_list_url(dept: Department, board_key: str, page: int) -> str:
    # dept.urlì—ì„œ # ì´í›„ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ì‹¤ì œ base URLì„ ë§Œë“­ë‹ˆë‹¤.
    parsed_dept_url = urlparse(dept.url.rstrip("/"))
    # scheme, netloc, pathë§Œ ì‚¬ìš©í•˜ê³  query, fragmentëŠ” ì œê±°
    # pathê°€ ë¹„ì–´ìˆìœ¼ë©´ '/'ë¡œ ì„¤ì • (ì˜ˆ: https://example.com#frag -> https://example.com/)
    path_for_dept_base = parsed_dept_url.path if parsed_dept_url.path else '/'
    department_base_url = urlunparse((parsed_dept_url.scheme, parsed_dept_url.netloc, path_for_dept_base, '', '', ''))
    department_base_url = department_base_url.rstrip('/')  # ë‹¤ì‹œ í•œë²ˆ ìš°ì¸¡ / ì œê±°

    logger.trace(f"[{dept.name}] ì›ë³¸ dept.url: {dept.url}, # ì œê±° í›„ base URL: {department_base_url}")

    # --- !! ì¤‘ìš” !! ---
    # ê° í•™ê³¼ë³„ ì‹¤ì œ ê³µì§€ì‚¬í•­ URL êµ¬ì¡°ì— ë§ê²Œ ì´ ë¶€ë¶„ì„ ìƒì„¸íˆ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # current_college_code = college_code_from_url(dept.url) # dept.url ëŒ€ì‹  department_base_url ì‚¬ìš© ê³ ë ¤
    # if current_college_code == "nursing" and "menu" in dept.url: # ê°„í˜¸ëŒ€í•™ URL íŠ¹ì„± ë°˜ì˜ ì˜ˆì‹œ
    #     # FIXME: ê°„í˜¸ëŒ€í•™ì˜ ì‹¤ì œ ê³µì§€ì‚¬í•­ ëª©ë¡ URLë¡œ ìˆ˜ì • (ì˜ˆ: `#` ì´ì „ URL + ì‹¤ì œ ê²½ë¡œ)
    #     # ì˜ˆ: "https://nursing.cnu.ac.kr/nursing/board/undergrad_notice.do"
    #     # department_base_url = "https://nursing.cnu.ac.kr" # ì‹¤ì œ ë„ë©”ì¸ìœ¼ë¡œ
    #     # board_path_segment = "ì‹¤ì œ_ê²Œì‹œíŒ_ê²½ë¡œ/list.do" # ë˜ëŠ” board?code=xxx
    #     pass
    # elif current_college_code == "cem" and "menu" in dept.url: # ê²½ìƒëŒ€í•™ URL íŠ¹ì„± ë°˜ì˜ ì˜ˆì‹œ
    #     # FIXME: ê²½ìƒëŒ€í•™ì˜ ì‹¤ì œ ê³µì§€ì‚¬í•­ ëª©ë¡ URLë¡œ ìˆ˜ì •
    #     pass

    # ê¸°ë³¸ URL ìƒì„± ê·œì¹™
    board_path_segment = BOARD_CODES.get(board_key)
    if not board_path_segment:
        logger.error(f"[{dept.name}] ìœ íš¨í•˜ì§€ ì•Šì€ board_key: {board_key}ì— ëŒ€í•œ BOARD_CODE ì—†ìŒ")
        return f"invalid_board_key_for_{dept.name}_{board_key}"

    # department_base_urlì´ íŒŒì¼ëª…(.do ë“±)ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°, ê·¸ ì•ì— board_path_segmentë¥¼ ë¶™ì´ë©´ ì•ˆë¨.
    # ì´ ë¶€ë¶„ì€ ê° ëŒ€í•™ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë§¤ìš° ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
    # ê°€ì¥ í™•ì‹¤í•œ ê²ƒì€ ê° Department ê°ì²´ì— ì •í™•í•œ ê²Œì‹œíŒ URL í…œí”Œë¦¿ì„ ê°–ë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    # ì„ì‹œë°©í¸ìœ¼ë¡œ, department_base_urlì´ íŠ¹ì • í™•ì¥ìë¡œ ëë‚˜ë©´ ê·¸ ì•ê¹Œì§€ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì‹œë„.

    temp_base = department_base_url
    # `.do`ë‚˜ `.jsp` ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°, í•´ë‹¹ íŒŒì¼ëª…ì„ í¬í•¨í•œ ê²½ë¡œê°€ ì•„ë‹Œ,
    # ìƒìœ„ ë””ë ‰í† ë¦¬ì— board_path_segmentë¥¼ ì ìš©í•´ì•¼ í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
    # ì˜ˆ: https://example.com/path/to/page.do -> /board?code=... ë¥¼ ë¶™ì´ë©´ 404
    #     https://example.com/path/to/board?code=... ê°€ ë˜ì–´ì•¼ í•  ìˆ˜ ìˆìŒ
    # ì´ëŠ” ëŒ€í•™ë³„ë¡œ ê·œì¹™ì„ ë§Œë“¤ì–´ì•¼ ì •í™•í•©ë‹ˆë‹¤.
    # ì•„ë˜ëŠ” ë§¤ìš° ì¼ë°˜ì ì¸ ê°€ì •ì´ë¯€ë¡œ, ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¡œì§ ë˜ëŠ” í•™ê³¼ë³„ URL í…œí”Œë¦¿ì´ í•„ìš”í•©ë‹ˆë‹¤.
    if any(temp_base.lower().endswith(ext) for ext in ['.do', '.jsp', '.php', '.html', '.htm']):
        # ë§ˆì§€ë§‰ '/'ë¥¼ ì°¾ì•„ ê·¸ ì´ì „ê¹Œì§€ë¥¼ baseë¡œ ì‚¼ìœ¼ë ¤ëŠ” ì‹œë„.
        # í•˜ì§€ë§Œ dept.url ìì²´ê°€ ê²Œì‹œíŒ ëª©ë¡ì´ ì•„ë‹Œ í•™ê³¼ ë©”ì¸í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ,
        # ì´ ë°©ì‹ì´ í•­ìƒ ì˜³ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
        # logger.debug(f"URLì´ íŒŒì¼ëª…ìœ¼ë¡œ ëë‚˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼: {temp_base}. ìƒìœ„ ê²½ë¡œ ì‚¬ìš© ì‹œë„.")
        # temp_base = temp_base.rsplit('/', 1)[0]
        # ìœ„ì™€ ê°™ì´ ìˆ˜ì •í•˜ë©´ department_base_urlì´ ì´ë¯¸ /ë¡œ ëë‚˜ë©´ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ
        # ê°€ì¥ ì•ˆì „í•œ ê²ƒì€ department_base_urlì— BOARD_CODES[board_key]ë¥¼ ê·¸ëŒ€ë¡œ ë¶™ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.
        # (ë‹¨, BOARD_CODESì˜ ê°’ì´ ì ˆëŒ€ê²½ë¡œ(/ë¡œ ì‹œì‘)ê°€ ì•„ë‹ˆê±°ë‚˜, ì™„ì „í•œ URLì´ ì•„ë‹ˆì–´ì•¼ í•¨)
        # í˜„ì¬ BOARD_CODESëŠ” ìƒëŒ€ê²½ë¡œ í˜•íƒœì´ë¯€ë¡œ, ë°”ë¡œ ë¶™ì—¬ë´…ë‹ˆë‹¤.
        pass  # í˜„ì¬ ë¡œì§ì—ì„œëŠ” department_base_urlì— ë°”ë¡œ board_path_segmentë¥¼ ë¶™ì…ë‹ˆë‹¤.

    final_url_base = f"{temp_base}/{board_path_segment}"
    if '?' in final_url_base:  # board_path_segmentì— ì´ë¯¸ '?'ê°€ ìˆëŠ” ê²½ìš°
        return f"{final_url_base}&page={page}"
    else:
        return f"{final_url_base}?page={page}"


# ì´í•˜ crawl_board, crawl_department_notices í•¨ìˆ˜ëŠ” ì´ì „ ë‹µë³€ì˜ ë‚´ìš©ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.
# (ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ê³ , ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ì´ ê°œì„ ëœ ë²„ì „)

async def crawl_board(dept: Department, board_key: str):
    page = 1
    inserted_count = 0
    max_pages_to_crawl = 1  # ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •

    delay_per_page = REQUEST_DELAY_NOTICE_PAGE_SECONDS

    logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œì‘")

    with get_session() as sess:
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_key)
                       .order_by(Notice.post_id.desc())
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"
    logger.debug(f"[{dept.name} ({board_key})] DBì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ID: {last_post_id_db} (ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œ ì°¸ê³ ìš©)")

    consecutive_404_errors = 0

    while page <= max_pages_to_crawl:
        list_url = get_notice_list_url(dept, board_key, page)
        if "invalid_board_key" in list_url:
            logger.error(f"[{dept.name} ({board_key})] ìœ íš¨í•œ ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì¤‘ë‹¨.")
            break

        logger.debug(f"í˜ì´ì§€ {page} ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")
        posts_data: List[Dict] = []
        stop_crawling_current_board = False
        current_page_fetch_successful = False

        try:
            data = await fetch_json(list_url)
            current_page_posts = data.get("posts") if isinstance(data, dict) else data

            if not isinstance(current_page_posts, list):
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ ({list_url}). HTML Fallback ì‹œë„.")
                raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

            logger.trace(f"[{dept.name} ({board_key})] JSON API ì„±ê³µ. {len(current_page_posts)}ê°œ í•­ëª© ìˆ˜ì‹ .")
            for p_item in current_page_posts:
                post_id_str = str(p_item.get("id", ""))
                title = clean_text(str(p_item.get("title", "")))
                raw_url = p_item.get("url", "")
                date_str = p_item.get("date", "")

                if not all([post_id_str, title, raw_url, date_str]):
                    logger.warning(f"[{dept.name} ({board_key})] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½: {p_item}")
                    continue

                if post_id_str.isdigit() and last_post_id_db.isdigit():
                    if int(post_id_str) <= int(last_post_id_db):
                        stop_crawling_current_board = True;
                        break
                elif post_id_str <= last_post_id_db and post_id_str != "":
                    stop_crawling_current_board = True;
                    break

                parsed_date = parse_date_flexible(date_str)
                if not parsed_date:
                    logger.warning(
                        f"[{dept.name} ({board_key})] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                full_url = urljoin(list_url, raw_url)
                posts_data.append({
                    "dept_id": dept.id, "board": board_key, "post_id": post_id_str,
                    "title": title, "url": full_url, "posted_at": parsed_date
                })

            if posts_data: current_page_fetch_successful = True
            consecutive_404_errors = 0
            if stop_crawling_current_board: break

        except (ClientError, json.JSONDecodeError, ValueError, Exception) as e_json:
            if isinstance(e_json, ClientError) and hasattr(e_json, 'status') and e_json.status == 404:
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì‹¤íŒ¨ - 404 Not Found ({list_url}). HTML Fallback ì‹œë„.")
                consecutive_404_errors += 1
            elif isinstance(e_json, asyncio.TimeoutError):
                logger.warning(f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({list_url}). HTML Fallback ì‹œë„.")
            elif isinstance(e_json, ClientError):
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")
            elif isinstance(e_json, json.JSONDecodeError):  # JSONDecodeErrorë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬ (ë¡œê·¸ ë©”ì‹œì§€ ê°œì„ )
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API íŒŒì‹± ì‹¤íŒ¨ ({list_url}): {e_json}. ì‘ë‹µì´ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. HTML Fallback ì‹œë„.")
            else:
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API ì²˜ë¦¬ ì¤‘ ê¸°íƒ€ ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")

            try:
                html_content = await fetch_text(list_url)
                # HTML Fallback ë¡œì§ì´ ë¹„ì–´ìˆê±°ë‚˜, í•´ë‹¹ ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ì— ë§ëŠ” íŒŒì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.
                # ì•„ë˜ëŠ” ì¼ë°˜ì ì¸ ì˜ˆì‹œì´ë©°, ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ì¶° CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
                ids_html = html_select(html_content, "td.no")  # ì˜ˆì‹œ ì„ íƒì
                titles_html = html_select(html_content, "td.title a")  # ì˜ˆì‹œ ì„ íƒì
                links_html = html_select(html_content, "td.title a", "href")  # ì˜ˆì‹œ ì„ íƒì
                dates_html = html_select(html_content, "td.date")  # ì˜ˆì‹œ ì„ íƒì

                min_len = min(len(ids_html), len(titles_html), len(links_html), len(dates_html))
                if min_len == 0 and (len(ids_html) + len(titles_html) + len(links_html) + len(dates_html) > 0):
                    logger.warning(f"[{dept.name} ({board_key})] HTMLì—ì„œ ì¼ë¶€ ì •ë³´ë§Œ ì¶”ì¶œë¨. íŒŒì‹± ê±´ë„ˆëœ€. URL: {list_url}")
                elif min_len > 0:
                    logger.trace(f"[{dept.name} ({board_key})] HTML Fallbackìœ¼ë¡œ {min_len}ê°œ í•­ëª© í›„ë³´ ë°œê²¬. URL: {list_url}")

                for i in range(min_len):
                    post_id_str = clean_text(ids_html[i])
                    if not post_id_str.isdigit():
                        id_match_from_url = re.search(r'(?:idx|id|no|seq)=(\d+)', links_html[i], re.I)
                        if id_match_from_url:
                            post_id_str = id_match_from_url.group(1)
                        else:
                            logger.warning(
                                f"[{dept.name} ({board_key})] HTML í•­ëª© IDê°€ ìˆ«ìê°€ ì•„ë‹ˆê³  URLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ ('{ids_html[i]}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue

                    if post_id_str.isdigit() and last_post_id_db.isdigit():
                        if int(post_id_str) <= int(last_post_id_db):
                            stop_crawling_current_board = True;
                            break
                    elif post_id_str <= last_post_id_db and post_id_str != "":
                        stop_crawling_current_board = True;
                        break

                    title = clean_text(titles_html[i])
                    raw_url = links_html[i]
                    date_str = dates_html[i]

                    parsed_date = parse_date_flexible(date_str)
                    if not parsed_date:
                        logger.warning(
                            f"[{dept.name} ({board_key})] HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    full_url = urljoin(list_url, raw_url)
                    posts_data.append({
                        "dept_id": dept.id, "board": board_key, "post_id": post_id_str,
                        "title": title, "url": full_url, "posted_at": parsed_date
                    })

                if posts_data: current_page_fetch_successful = True
                consecutive_404_errors = 0
                if stop_crawling_current_board: break

            except ClientError as e_html_fetch:
                if hasattr(e_html_fetch, 'status') and e_html_fetch.status == 404:
                    logger.error(
                        f"[{dept.name} ({board_key})] HTML Fallback URL ì ‘ê·¼ ì‹¤íŒ¨ - 404 Not Found ({list_url}): {e_html_fetch.message}")
                    consecutive_404_errors += 1
                elif isinstance(e_html_fetch, ClientError):
                    logger.error(
                        f"[{dept.name} ({board_key})] HTML Fallback URL ì ‘ê·¼ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
            except Exception as e_html_parse:
                logger.error(f"[{dept.name} ({board_key})] HTML Fallback íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {e_html_parse}")

        if stop_crawling_current_board:
            logger.info(f"[{dept.name} ({board_key})] ì¦ë¶„ ìˆ˜ì§‘ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ì²« í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ë‹¨.")
        elif not current_page_fetch_successful and consecutive_404_errors >= 1:  # ì²« í˜ì´ì§€ê°€ 404ì´ê±°ë‚˜ ì—°ê²° ì‹¤íŒ¨
            logger.warning(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ë¶€í„° 404 ì˜¤ë¥˜ ë˜ëŠ” ì—°ê²° ì‹¤íŒ¨. í•´ë‹¹ ê²Œì‹œíŒ ìˆ˜ì§‘ ì¤‘ë‹¨. URL: {list_url}")
        elif not current_page_fetch_successful:  # 404ëŠ” ì•„ë‹ˆì§€ë§Œ ë‹¤ë¥¸ ì´ìœ ë¡œ ë°ì´í„° ëª» ì–»ìŒ
            logger.info(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URL: {list_url}")

        if posts_data:
            try:
                with get_session() as sess:
                    sess.bulk_insert_mappings(Notice, posts_data)
                    sess.commit()
                inserted_count += len(posts_data)
                logger.debug(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ {len(posts_data)}ê±´ DB ì €ì¥ ì™„ë£Œ.")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

        break

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_key})] ì²« í˜ì´ì§€ ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:
        logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def crawl_department_notices(dept: Department):
    delay_before_dept_crawl = REQUEST_DELAY_DEPARTMENT_SECONDS
    if delay_before_dept_crawl > 0:
        logger.trace(f"'{dept.name}' í•™ê³¼ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì‹œì‘ ì „ {delay_before_dept_crawl:.1f}ì´ˆ ëŒ€ê¸°...")
        await asyncio.sleep(delay_before_dept_crawl)

    for board_key_val in BOARD_CODES:
        try:
            await crawl_board(dept, board_key_val)
        except Exception as e:
            logger.opt(exception=True).error(f"[{dept.name} ({board_key_val})] ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ìµœì¢… ì˜ˆì™¸ ë°œìƒ: {e}")