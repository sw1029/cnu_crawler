# src/cnu_crawler/spiders/notices.py
import asyncio
import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Coroutine, Any, Tuple  # <- Tuple ì„í¬íŠ¸ ì¶”ê°€
from urllib.parse import urljoin, urlparse, urlunparse, parse_qs, urlencode

from loguru import logger
from aiohttp import ClientError

from cnu_crawler.core.fetcher import fetch_text, fetch_json
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session
from cnu_crawler.utils import clean_text, parse_date_flexible
from cnu_crawler.config import (
    REQUEST_DELAY_NOTICE_PAGE_SECONDS,
    REQUEST_DELAY_DEPARTMENT_SECONDS
)

BOARD_TYPE_ACADEMIC = "academic"
BOARD_TYPE_UNDERGRAD = "undergrad"
BOARD_TYPE_GRAD = "grad"
BOARD_TYPE_GRAD_KEYWORD = "grad_keyword_found"

BOARD_CODES = {  # ì´ì „ ë‹µë³€ì—ì„œ ìœ ì§€ëœ ë¶€ë¶„, get_notice_list_url ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
    BOARD_TYPE_UNDERGRAD: "board?code=undergrad_notice",
    BOARD_TYPE_GRAD: "board?code=grad_notice"
}


def college_code_from_url(college_url: str) -> Optional[str]:
    try:
        hostname = college_url.split('/')[2]
        return hostname.split('.')[0]
    except IndexError:
        logger.warning(f"URLì—ì„œ ëŒ€í•™ ì½”ë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {college_url}")
        return None


def get_notice_list_url(dept: Department, board_type: str, page: int) -> Optional[str]:
    url_template: Optional[str] = None

    if board_type == BOARD_TYPE_ACADEMIC:
        url_template = dept.academic_notice_url_template
    elif board_type == BOARD_TYPE_UNDERGRAD:
        url_template = dept.undergrad_notice_url_template
    elif board_type == BOARD_TYPE_GRAD:
        url_template = dept.grad_notice_url_template
    elif board_type == BOARD_TYPE_GRAD_KEYWORD:
        url_template = dept.specific_grad_keyword_notice_url

    # URL í…œí”Œë¦¿ì´ ì—†ëŠ” ê²½ìš°, BOARD_CODESë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ ê²½ë¡œ ì‹œë„ (ì´ì „ ë¡œì§ í˜¸í™˜ì„±)
    if not url_template and board_type in BOARD_CODES:
        department_base_url = dept.url.rstrip("/")
        # dept.urlì—ì„œ # ì´í›„ ë¶€ë¶„ ì œê±° (ì´ì „ ë‹µë³€ì˜ ë¡œì§)
        parsed_dept_url = urlparse(department_base_url)
        path_for_dept_base = parsed_dept_url.path if parsed_dept_url.path else '/'
        clean_base_url = urlunparse((parsed_dept_url.scheme, parsed_dept_url.netloc, path_for_dept_base, '', '', ''))
        clean_base_url = clean_base_url.rstrip('/')

        board_path_segment = BOARD_CODES.get(board_type)
        if board_path_segment:
            # ì´ ì¡°í•© ë°©ì‹ì€ ì—¬ì „íˆ 404 ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ, í•™ê³¼ë³„ í…œí”Œë¦¿ ì„¤ì •ì´ ìµœì„ 
            final_url_base = f"{clean_base_url}/{board_path_segment}"
            if '?' in final_url_base:
                url_template = final_url_base + "&page={page}"  # í˜ì´ì§€ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©
            else:
                url_template = final_url_base + "?page={page}"
            logger.debug(
                f"[{dept.name} ({board_type})] URL í…œí”Œë¦¿ ë¯¸ì„¤ì •, BOARD_CODES ê¸°ë°˜ URL ìƒì„±: {url_template.format(page=page)}")

    if not url_template:
        logger.trace(f"[{dept.name}] ê²Œì‹œíŒ ìœ í˜• '{board_type}'ì— ëŒ€í•œ ìµœì¢… URL í…œí”Œë¦¿ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    try:
        if "{page}" in url_template:
            return url_template.replace("{page}", str(page))
        elif "{}" in url_template:
            return url_template.format(page)
        else:
            parsed_template = urlparse(url_template)
            query_params = parse_qs(parsed_template.query)
            page_param_name = "page"
            query_params[page_param_name] = [str(page)]
            new_query = urlencode(query_params, doseq=True)
            return urlunparse((parsed_template.scheme, parsed_template.netloc, parsed_template.path,
                               parsed_template.params, new_query, ''))

    except Exception as e:
        logger.error(f"[{dept.name}] URL í…œí”Œë¦¿ ('{url_template}') ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (page={page}, board_type='{board_type}'): {e}")
        return None


# Tupleì„ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
async def _parse_notice_page_content(dept: Department, board_type: str, list_url: str, last_post_id_db: str) -> Tuple[
    List[Dict], bool]:
    posts_data: List[Dict] = []
    stop_crawling = False
    fetch_successful = False

    try:
        logger.trace(f"[{dept.name} ({board_type})] JSON API ì‹œë„: {list_url}")
        data = await fetch_json(list_url)
        current_page_posts = data.get("posts") if isinstance(data, dict) else data

        if not isinstance(current_page_posts, list):
            logger.warning(
                f"[{dept.name} ({board_type})] JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ ({list_url}). HTML Fallback ì‹œë„ ì˜ˆì •. ë°ì´í„°: {str(data)[:200]}")
            raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

        logger.trace(f"[{dept.name} ({board_type})] JSON API ì„±ê³µ. {len(current_page_posts)}ê°œ í•­ëª© ìˆ˜ì‹ .")
        for p_item in current_page_posts:
            post_id_str = str(p_item.get("id", "")).strip()
            title = clean_text(str(p_item.get("title", "")))
            raw_url = p_item.get("url", "")
            date_str = p_item.get("date", "")

            if not all([post_id_str, title, raw_url, date_str]):
                logger.warning(f"[{dept.name} ({board_type})] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½: {p_item}")
                continue

            if post_id_str.isdigit() and last_post_id_db.isdigit():
                if int(post_id_str) <= int(last_post_id_db): stop_crawling = True; break
            elif post_id_str <= last_post_id_db and post_id_str != "":
                stop_crawling = True; break

            parsed_date = parse_date_flexible(date_str)
            if not parsed_date: logger.warning(
                f"[{dept.name} ({board_type})] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

            full_url = urljoin(list_url, raw_url)
            notice_item = {"dept_id": dept.id, "board": board_type, "post_id": post_id_str,
                           "title": title, "url": full_url, "posted_at": parsed_date}
            if board_type == BOARD_TYPE_GRAD_KEYWORD:
                notice_item["source_display_name"] = f"{dept.name} ëŒ€í•™ì›"
            posts_data.append(notice_item)

        if posts_data: fetch_successful = True
        if stop_crawling: return posts_data, stop_crawling

    except (ClientError, json.JSONDecodeError, ValueError, Exception) as e_json:
        log_msg_prefix = f"[{dept.name} ({board_type})] JSON API"
        if isinstance(e_json, ClientError) and hasattr(e_json, 'status') and e_json.status == 404:
            logger.warning(f"{log_msg_prefix} í˜¸ì¶œ ì‹¤íŒ¨ - 404 Not Found ({list_url}). HTML Fallback ì‹œë„.")
        elif isinstance(e_json, json.JSONDecodeError):
            logger.warning(f"{log_msg_prefix} íŒŒì‹± ì‹¤íŒ¨ ({list_url}): {e_json}. HTML Fallback ì‹œë„.")
        elif isinstance(e_json, asyncio.TimeoutError):
            logger.warning(f"{log_msg_prefix} í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({list_url}). HTML Fallback ì‹œë„.")
        elif isinstance(e_json, ClientError):
            logger.warning(
                f"{log_msg_prefix} í˜¸ì¶œ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")
        else:
            logger.warning(
                f"{log_msg_prefix} ì²˜ë¦¬ ì¤‘ ê¸°íƒ€ ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")

        try:
            logger.trace(f"[{dept.name} ({board_type})] HTML Fallback ì‹œë„: {list_url}")
            html_content = await fetch_text(list_url)

            ids_html = html_select(html_content, "td.no")
            titles_html = html_select(html_content, "td.title a")
            links_html = html_select(html_content, "td.title a", "href")
            dates_html = html_select(html_content, "td.date")

            min_len = min(len(ids_html), len(titles_html), len(links_html), len(dates_html))
            if min_len == 0 and sum(map(len, [ids_html, titles_html, links_html, dates_html])) > 0:
                logger.warning(f"[{dept.name} ({board_type})] HTMLì—ì„œ ì¼ë¶€ ì •ë³´ë§Œ ì¶”ì¶œë¨. íŒŒì‹± ê±´ë„ˆëœ€. URL: {list_url}")
            elif min_len > 0:
                logger.trace(f"[{dept.name} ({board_type})] HTML Fallbackìœ¼ë¡œ {min_len}ê°œ í•­ëª© í›„ë³´ ë°œê²¬. URL: {list_url}")

            for i in range(min_len):
                post_id_str = clean_text(ids_html[i])
                if not post_id_str.isdigit():
                    id_match_from_url = re.search(r'(?:idx|id|no|seq|docSn)=(\d+)', links_html[i], re.I)
                    if id_match_from_url:
                        post_id_str = id_match_from_url.group(1)
                    else:
                        logger.warning(
                            f"[{dept.name} ({board_type})] HTML í•­ëª© IDê°€ ìˆ«ìê°€ ì•„ë‹ˆê³  URLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ ('{ids_html[i]}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

                if post_id_str.isdigit() and last_post_id_db.isdigit():
                    if int(post_id_str) <= int(last_post_id_db): stop_crawling = True; break
                elif post_id_str <= last_post_id_db and post_id_str != "":
                    stop_crawling = True; break

                title = clean_text(titles_html[i])
                raw_url = links_html[i]
                date_str = dates_html[i]
                parsed_date = parse_date_flexible(date_str)
                if not parsed_date: logger.warning(
                    f"[{dept.name} ({board_type})] HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤."); continue

                full_url = urljoin(list_url, raw_url)
                notice_item = {"dept_id": dept.id, "board": board_type, "post_id": post_id_str,
                               "title": title, "url": full_url, "posted_at": parsed_date}
                if board_type == BOARD_TYPE_GRAD_KEYWORD:
                    notice_item["source_display_name"] = f"{dept.name} ëŒ€í•™ì›"
                posts_data.append(notice_item)

            if posts_data: fetch_successful = True
            if stop_crawling: return posts_data, stop_crawling

        except ClientError as e_html_fetch:
            log_msg_prefix_html = f"[{dept.name} ({board_type})] HTML Fallback"
            if hasattr(e_html_fetch, 'status') and e_html_fetch.status == 404:
                logger.error(f"{log_msg_prefix_html} URL ì ‘ê·¼ ì‹¤íŒ¨ - 404 Not Found ({list_url}): {e_html_fetch.message}")
            elif isinstance(e_html_fetch, ClientError):
                logger.error(
                    f"{log_msg_prefix_html} URL ì ‘ê·¼ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
            else:  # ì´ ê²½ìš°ëŠ” ê±°ì˜ ë°œìƒ ì•ˆ í•¨ (ClientErrorê°€ ì•„ë‹Œ ê²½ìš°)
                logger.error(
                    f"{log_msg_prefix_html} URL ì ‘ê·¼ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
        except Exception as e_html_parse:
            logger.error(f"[{dept.name} ({board_type})] HTML Fallback íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {e_html_parse}")

    if not fetch_successful:
        logger.warning(f"[{dept.name} ({board_type})] ìµœì¢…ì ìœ¼ë¡œ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URL: {list_url}")

    return posts_data, stop_crawling


async def crawl_board(dept: Department, board_type: str):
    page = 1
    inserted_count = 0

    logger.info(f"ğŸ“„ [{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œì‘")

    with get_session() as sess:
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_type)
                       .order_by(Notice.post_id.desc())
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"
    logger.debug(f"[{dept.name} ({board_type})] DBì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ID: {last_post_id_db}")

    list_url = get_notice_list_url(dept, board_type, page)
    if not list_url:  # get_notice_list_urlì´ Noneì„ ë°˜í™˜í•˜ë©´ (í…œí”Œë¦¿ ì—†ê±°ë‚˜ ì˜¤ë¥˜)
        logger.error(f"[{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ì–»ì„ ìˆ˜ ì—†ì–´ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    logger.debug(f"í˜ì´ì§€ {page} ({board_type}) ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")

    posts_to_save, stop_increment_crawl = await _parse_notice_page_content(dept, board_type, list_url, last_post_id_db)

    if stop_increment_crawl and not posts_to_save:
        logger.info(f"[{dept.name} ({board_type})] ì¦ë¶„ ì¡°ê±´ì— ë”°ë¼ ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    elif not posts_to_save:
        logger.info(f"[{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (URL: {list_url}).")

    if posts_to_save:
        try:
            with get_session() as sess:
                sess.bulk_insert_mappings(Notice, posts_to_save)
                sess.commit()
            inserted_count = len(posts_to_save)
            logger.debug(f"[{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ {inserted_count}ê±´ DB ì €ì¥ ì™„ë£Œ.")
        except Exception as e_db:
            logger.opt(exception=True).error(f"[{dept.name} ({board_type})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_type})] ì²« í˜ì´ì§€ ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:
        logger.info(f"ğŸ“„ [{dept.name} ({board_type})] ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def find_and_attempt_parse_board_by_keyword(dept: Department, keywords: List[str], board_type_for_db: str,
                                                  search_url: str) -> bool:
    logger.debug(f"[{dept.name}] '{search_url}' ì—ì„œ '{keywords}' í‚¤ì›Œë“œë¡œ '{board_type_for_db}' ê²Œì‹œíŒ ë§í¬ íƒìƒ‰ ì‹œë„...")
    try:
        html_content = await fetch_text(search_url)
        found_board_url_template = None  # í˜ì´ì§€ íŒŒë¼ë¯¸í„°ê°€ í¬í•¨ëœ í…œí”Œë¦¿ ë˜ëŠ” ê¸°ë³¸ URL

        all_links_href = html_select(html_content, "a", attr="href")
        all_links_text = html_select(html_content, "a")

        for text, href in zip(all_links_text, all_links_href):
            cleaned_text = clean_text(text)
            if any(kw.lower() in cleaned_text.lower() for kw in keywords):
                potential_url = urljoin(search_url, href)
                # ì´ URLì´ ì‹¤ì œ ëª©ë¡ í˜ì´ì§€ì¸ì§€, ì•„ë‹ˆë©´ ìƒì„¸ í˜ì´ì§€ì¸ì§€, í˜ì´ì§€ íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì—‡ì¸ì§€ ë“± ë¶„ì„ í•„ìš”
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì´ URLì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³ , page íŒŒë¼ë¯¸í„°ë¥¼ ë¶™ì´ëŠ” í…œí”Œë¦¿ìœ¼ë¡œ ê°€ì •
                parsed_link = urlparse(potential_url)
                # queryì—ì„œ page ê´€ë ¨ íŒŒë¼ë¯¸í„° ì œê±° ì‹œë„ (ë§¤ìš° ë‹¨ìˆœí•œ ë°©ì‹)
                query_params = parse_qs(parsed_link.query)
                # ì¼ë°˜ì ì¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì´ë¦„ë“¤
                common_page_params = ['page', 'pageNo', 'pageNum', 'pg', 'p', 'start']
                for p_key in common_page_params: query_params.pop(p_key, None)

                new_query = urlencode(query_params, doseq=True)
                base_link_for_template = urlunparse(
                    (parsed_link.scheme, parsed_link.netloc, parsed_link.path, parsed_link.params, new_query, ''))

                # í…œí”Œë¦¿ ìƒì„±: ?page={} ë˜ëŠ” &page={}
                if "?" in base_link_for_template:
                    found_board_url_template = base_link_for_template + "&page={page}"
                else:
                    found_board_url_template = base_link_for_template + "?page={page}"

                logger.info(
                    f"[{dept.name}] í‚¤ì›Œë“œ '{keywords}' ì¼ì¹˜ ë§í¬ ë°œê²¬: '{cleaned_text}' -> {potential_url}. ìƒì„±ëœ í…œí”Œë¦¿: {found_board_url_template}")
                break

        if found_board_url_template:
            # ì°¾ì€ URL í…œí”Œë¦¿ì„ Department ê°ì²´ì— ì„ì‹œë¡œ ì„¤ì •í•˜ì—¬ crawl_boardì—ì„œ ì‚¬ìš©
            # (ì£¼ì˜: ì´ ë°©ì‹ì€ Department ê°ì²´ì˜ ìƒíƒœë¥¼ ë³€ê²½í•˜ë¯€ë¡œ, ë™ì‹œì„± ë¬¸ì œë‚˜ ì˜ë„ì¹˜ ì•Šì€ íš¨ê³¼ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŒ.
            #  ë” ë‚˜ì€ ë°©ì‹ì€ crawl_boardê°€ URL í…œí”Œë¦¿ì„ ì§ì ‘ ì¸ìë¡œ ë°›ê±°ë‚˜,
            #  Department ê°ì²´ë¥¼ ë³µì‚¬í•˜ì—¬ ìˆ˜ì • í›„ ì „ë‹¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.)
            original_template = None
            if board_type_for_db == BOARD_TYPE_ACADEMIC:
                original_template = dept.academic_notice_url_template
                dept.academic_notice_url_template = found_board_url_template
            elif board_type_for_db == BOARD_TYPE_UNDERGRAD:
                original_template = dept.undergrad_notice_url_template
                dept.undergrad_notice_url_template = found_board_url_template
            # ... ë‹¤ë¥¸ board_typeì— ëŒ€í•œ ì²˜ë¦¬ ...
            elif board_type_for_db == BOARD_TYPE_GRAD_KEYWORD:
                original_template = dept.specific_grad_keyword_notice_url
                dept.specific_grad_keyword_notice_url = found_board_url_template

            await crawl_board(dept, board_type_for_db)

            # ì›ë˜ í…œí”Œë¦¿ìœ¼ë¡œ ë³µì›
            if original_template is not None:  # original_templateì´ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë³µì› ì‹œë„
                if board_type_for_db == BOARD_TYPE_ACADEMIC:
                    dept.academic_notice_url_template = original_template
                elif board_type_for_db == BOARD_TYPE_UNDERGRAD:
                    dept.undergrad_notice_url_template = original_template
                elif board_type_for_db == BOARD_TYPE_GRAD_KEYWORD:
                    dept.specific_grad_keyword_notice_url = original_template

            return True
        else:
            logger.info(f"[{dept.name}] '{search_url}'ì—ì„œ '{keywords}' ê´€ë ¨ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        logger.error(f"[{dept.name}] '{search_url}'ì—ì„œ '{keywords}' ê²Œì‹œíŒ íƒìƒ‰/íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return False


async def crawl_grad_keyword_notices_simplified(dept: Department):
    if dept.specific_grad_keyword_notice_url:
        logger.info(f"[{dept.name}] ì´ë¯¸ ì„¤ì •ëœ 'ëŒ€í•™ì›' ê´€ë ¨ ê³µì§€ URL ì‚¬ìš© ì‹œë„: {dept.specific_grad_keyword_notice_url}")
        await crawl_board(dept, BOARD_TYPE_GRAD_KEYWORD)
    else:
        grad_notice_keywords = ["ëŒ€í•™ì›ê³µì§€", "ëŒ€í•™ì› ê²Œì‹œíŒ", "ëŒ€í•™ì› ìë£Œì‹¤", "ëŒ€í•™ì› ì¼ë°˜ì†Œì‹", "ì„ì‚¬ê³µì§€", "ë°•ì‚¬ê³µì§€"]
        parsed_grad_keyword_board = await find_and_attempt_parse_board_by_keyword(
            dept, grad_notice_keywords, BOARD_TYPE_GRAD_KEYWORD, dept.url
        )
        if not parsed_grad_keyword_board:
            logger.info(f"[{dept.name}] í•™ê³¼ ë©”ì¸ í˜ì´ì§€ì—ì„œ 'ëŒ€í•™ì›' ê´€ë ¨ ëª…ì‹œì  ê³µì§€ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def crawl_department_notices(dept: Department):
    delay_seconds = REQUEST_DELAY_DEPARTMENT_SECONDS
    if delay_seconds > 0:
        logger.trace(f"'{dept.name}' í•™ê³¼ ê³µì§€ì‚¬í•­ ì „ì²´ ìˆ˜ì§‘ ì‹œì‘ ì „ {delay_seconds:.1f}ì´ˆ ëŒ€ê¸°...")
        await asyncio.sleep(delay_seconds)

    parsed_any_notice = False

    if dept.academic_notice_url_template:
        logger.info(f"[{dept.name}] ì„¤ì •ëœ í•™ì‚¬ê³µì§€ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
        await crawl_board(dept, BOARD_TYPE_ACADEMIC)
        parsed_any_notice = True
    else:
        academic_keywords = ["í•™ì‚¬ê³µì§€", "í•™ì‚¬ì•ˆë‚´", "í•™ë¶€í•™ì‚¬"]
        parsed_academic = await find_and_attempt_parse_board_by_keyword(
            dept, academic_keywords, BOARD_TYPE_ACADEMIC, dept.url
        )
        if parsed_academic: parsed_any_notice = True

    if not parsed_any_notice or dept.undergrad_notice_url_template or dept.grad_notice_url_template:
        if not parsed_any_notice:
            logger.info(f"[{dept.name}] í•™ì‚¬ê³µì§€ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ URL í…œí”Œë¦¿ì´ ì—†ì–´ ì¼ë°˜ ê³µì§€ì‚¬í•­ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

        if dept.undergrad_notice_url_template:
            logger.info(f"[{dept.name}] ì„¤ì •ëœ í•™ë¶€ ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_UNDERGRAD)
        elif dept.dept_type not in ["grad_school_dept", "plus_special_grad_dept", "plus_general_grad_dept"]:
            logger.debug(f"[{dept.name}] í•™ë¶€ ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. ê¸°ë³¸ 'undergrad' íƒ€ì…ìœ¼ë¡œ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_UNDERGRAD)

        if dept.grad_notice_url_template:
            logger.info(f"[{dept.name}] ì„¤ì •ëœ ëŒ€í•™ì› ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_GRAD)
        elif dept.dept_type in ["grad_school_dept", "plus_special_grad_dept",
                                "plus_general_grad_dept"] or "ëŒ€í•™ì›" in dept.name:
            logger.debug(f"[{dept.name}] ëŒ€í•™ì› ê´€ë ¨ í•™ê³¼ì´ë‚˜ ëŒ€í•™ì› ê³µì§€ URL í…œí”Œë¦¿ ë¯¸ì„¤ì •. ê¸°ë³¸ 'grad' íƒ€ì…ìœ¼ë¡œ ì‹œë„.")
            await crawl_board(dept, BOARD_TYPE_GRAD)

    if dept.dept_type in ["grad_school_dept", "plus_special_grad_dept", "plus_general_grad_dept"] or \
            "ëŒ€í•™ì›" in dept.name or \
            dept.specific_grad_keyword_notice_url:
        await crawl_grad_keyword_notices_simplified(dept)