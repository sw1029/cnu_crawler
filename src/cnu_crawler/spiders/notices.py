# src/cnu_crawler/spiders/notices.py
import asyncio
import json
import re  # HTMLì—ì„œ ID ì¶”ì¶œ ì‹œ í•„ìš”í•  ìˆ˜ ìˆìŒ
from datetime import datetime
from typing import Dict, List
from urllib.parse import urljoin

from loguru import logger
from aiohttp import ClientError  # aiohttp ê´€ë ¨ ì˜ˆì™¸ ì²˜ë¦¬

from cnu_crawler.core.fetcher import fetch_json, fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session
from cnu_crawler.utils import clean_text, parse_date_flexible
from cnu_crawler.config import (
    REQUEST_DELAY_NOTICE_PAGE_SECONDS,
    REQUEST_DELAY_DEPARTMENT_SECONDS
)

BOARD_CODES = {
    "undergrad": "board?code=undergrad_notice",
    "grad": "board?code=grad_notice"
}


def get_notice_list_url(dept: Department, board_key: str, page: int) -> str:
    department_base_url = dept.url.rstrip("/")

    # --- !! ì¤‘ìš” !! ---
    # ê° í•™ê³¼ë³„ ì‹¤ì œ ê³µì§€ì‚¬í•­ URL êµ¬ì¡°ì— ë§ê²Œ ì´ ë¶€ë¶„ì„ ìƒì„¸íˆ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # dept.name ë˜ëŠ” dept.codeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ê¸°í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.
    # ì‹¤ì œ í•™ê³¼ ì½”ë“œë‚˜ ì´ë¦„, URL íŒ¨í„´ì„ í™•ì¸í•˜ì—¬ ì ìš©í•˜ì„¸ìš”.

    # ì˜ˆì‹œ: ê³µê³¼ëŒ€í•™ ëŒ€í•™ì› ('eng'ëŠ” College ì½”ë“œ, 'archi'ëŠ” Department ì½”ë“œì¼ ìˆ˜ ìˆìŒ)
    # ë¡œê·¸ì—ì„œ 'eng.cnu.ac.kr/eng/department/aerospace.do' ì™€ ê°™ì€ URLì´ dept.urlë¡œ ì‚¬ìš©ë¨
    if college_code_from_url(dept.url) == "eng" and board_key == "grad":
        # FIXME: ê³µê³¼ëŒ€í•™ ëŒ€í•™ì›ì˜ ì‹¤ì œ ê³µì§€ì‚¬í•­ ëª©ë¡ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
        # ì˜ˆ: "https://eng.cnu.ac.kr/eng/notice/grad.do?page={}"
        # ì•„ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ì„ ë”°ë¥´ë˜, ë¬¸ì œê°€ ìˆë‹¤ë©´ ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•¨ì„ ëª…ì‹œ
        pass  # íŠ¹ë³„í•œ ê·œì¹™ì´ ì—†ë‹¤ë©´ ì•„ë˜ ê¸°ë³¸ ê·œì¹™ìœ¼ë¡œ
    elif college_code_from_url(dept.url) == "art" and board_key == "undergrad":
        # FIXME: ì˜ˆìˆ ëŒ€í•™ í•™ë¶€ì˜ ì‹¤ì œ ê³µì§€ì‚¬í•­ ëª©ë¡ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
        pass

    # ê¸°ë³¸ URL ìƒì„± ê·œì¹™
    board_path_segment = BOARD_CODES.get(board_key)
    if not board_path_segment:
        logger.error(f"[{dept.name}] ìœ íš¨í•˜ì§€ ì•Šì€ board_key: {board_key}ì— ëŒ€í•œ BOARD_CODE ì—†ìŒ")
        return f"invalid_board_key_for_{dept.name}_{board_key}"

    # department_base_url (ì˜ˆ: https://eng.cnu.ac.kr/eng/department/aerospace.do)
    # board_path_segment (ì˜ˆ: board?code=grad_notice)
    # ê²°í•© ê²°ê³¼ ì˜ˆì‹œ: https://eng.cnu.ac.kr/eng/department/aerospace.do/board?code=grad_notice&page=1
    # ì´ URLì´ 404ë¥¼ ë°˜í™˜í•œë‹¤ë©´, ì´ ê²°í•© ë°©ì‹ ë˜ëŠ” BOARD_CODES ë˜ëŠ” department_base_url ìì²´ê°€ ì˜ëª»ëœ ê²ƒì„.
    # ë§ì€ ê²½ìš° .do ì™€ ê°™ì€ íŒŒì¼ëª… ë’¤ì— /ë¥¼ ë¶™ì´ê³  ê²½ë¡œë¥¼ ì¶”ê°€í•˜ë©´ 404ê°€ ë°œìƒí•©ë‹ˆë‹¤.
    # ì‹¤ì œë¡œëŠ” department_base_urlì—ì„œ íŒŒì¼ëª…ì„ ì œê±°í•˜ê³  board_path_segmentë¥¼ ë¶™ì´ê±°ë‚˜,
    # ì™„ì „íˆ ë‹¤ë¥¸ URL êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    # ì„ì‹œ ìˆ˜ì •: dept.urlì´ .do ë“±ìœ¼ë¡œ ëë‚˜ë©´, ê·¸ ì•ë¶€ë¶„ê¹Œì§€ë§Œ ì‚¬ìš© ì‹œë„
    if department_base_url.endswith(".do") or department_base_url.endswith(".jsp"):
        # department_base_url = department_base_url.rsplit('/', 1)[0] # ì˜ˆ: .../aerospace.do -> ...
        # ìœ„ì™€ ê°™ì´ ìˆ˜ì •í•˜ë©´ ì˜ë„ì¹˜ ì•Šì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        # ê° í•™ê³¼ë³„ ì •í™•í•œ URL ê·œì¹™ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì›ë˜ ë¡œì§ì„ ìœ ì§€í•˜ê³ , get_notice_list_url í•¨ìˆ˜ ìì²´ì˜ ê°œì„ ì´ í•„ìš”í•¨ì„ ì¸ì§€í•©ë‹ˆë‹¤.
        pass

    final_url_base = f"{department_base_url}/{board_path_segment}"
    if '?' in final_url_base:
        return f"{final_url_base}&page={page}"
    else:
        return f"{final_url_base}?page={page}"


def college_code_from_url(college_url: str) -> Optional[str]:
    # URLì—ì„œ ëŒ€í•™ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œ (ì˜ˆ: https://eng.cnu.ac.kr -> eng)
    try:
        return college_url.split('/')[2].split('.')[0]
    except IndexError:
        return None


async def crawl_board(dept: Department, board_key: str):
    page = 1
    inserted_count = 0
    max_pages_to_crawl = 1  # ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë„ë¡ ì„¤ì •

    delay_per_page = REQUEST_DELAY_NOTICE_PAGE_SECONDS

    logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œì‘")

    with get_session() as sess:
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_key)
                       .order_by(Notice.post_id.desc())
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"
    logger.debug(f"[{dept.name} ({board_key})] DBì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ID: {last_post_id_db} (ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œ ì°¸ê³ ìš©)")

    consecutive_404_errors = 0  # ì—°ì† 404 ì˜¤ë¥˜ ì¹´ìš´í„° (ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë¯€ë¡œ í° ì˜ë¯¸ëŠ” ì—†ì„ ìˆ˜ ìˆìŒ)

    while page <= max_pages_to_crawl:  # ì´ ë£¨í”„ëŠ” page=1ì¼ ë•Œë§Œ ì‹¤í–‰ë¨
        list_url = get_notice_list_url(dept, board_key, page)
        if "invalid_board_key" in list_url:
            logger.error(f"[{dept.name} ({board_key})] ìœ íš¨í•œ ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì¤‘ë‹¨.")
            break

        logger.debug(f"í˜ì´ì§€ {page} ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")
        posts_data: List[Dict] = []
        stop_crawling_current_board = False
        current_page_fetch_successful = False

        try:  # JSON API ì‹œë„
            data = await fetch_json(list_url)
            current_page_posts = data.get("posts") if isinstance(data, dict) else data

            if not isinstance(current_page_posts, list):
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ ({list_url}). HTML Fallback ì‹œë„.")
                raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

            logger.trace(f"[{dept.name} ({board_key})] JSON API ì„±ê³µ. {len(current_page_posts)}ê°œ í•­ëª© ìˆ˜ì‹ .")
            for p_item in current_page_posts:
                # ... (JSON íŒŒì‹± ë° ì¦ë¶„ ë¹„êµ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
                post_id_str = str(p_item.get("id", ""))
                title = clean_text(str(p_item.get("title", "")))
                raw_url = p_item.get("url", "")
                date_str = p_item.get("date", "")

                if not all([post_id_str, title, raw_url, date_str]):
                    logger.warning(f"[{dept.name} ({board_key})] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½: {p_item}")
                    continue

                if post_id_str.isdigit() and last_post_id_db.isdigit():
                    if int(post_id_str) <= int(last_post_id_db):
                        stop_crawling_current_board = True;
                        break
                elif post_id_str <= last_post_id_db and post_id_str != "":
                    stop_crawling_current_board = True;
                    break

                parsed_date = parse_date_flexible(date_str)
                if not parsed_date:
                    logger.warning(
                        f"[{dept.name} ({board_key})] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                full_url = urljoin(list_url, raw_url)
                posts_data.append({
                    "dept_id": dept.id, "board": board_key, "post_id": post_id_str,
                    "title": title, "url": full_url, "posted_at": parsed_date
                })

            if posts_data: current_page_fetch_successful = True
            consecutive_404_errors = 0
            if stop_crawling_current_board: break

        except (ClientError, json.JSONDecodeError, ValueError, Exception) as e_json:
            # === ìˆ˜ì •ëœ ì˜¤ë¥˜ ì²˜ë¦¬ ë¶€ë¶„ ===
            if isinstance(e_json, ClientError) and hasattr(e_json, 'status') and e_json.status == 404:  # type: ignore
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì‹¤íŒ¨ - 404 Not Found ({list_url}). HTML Fallback ì‹œë„.")
                consecutive_404_errors += 1
            elif isinstance(e_json, asyncio.TimeoutError):
                logger.warning(f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({list_url}). HTML Fallback ì‹œë„.")
            elif isinstance(e_json, ClientError):  # ClientConnectorError ë“± statusê°€ ì—†ëŠ” ClientError
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")
                # ì—°ê²° ì˜¤ë¥˜ ì‹œì—ëŠ” 404ê°€ ì•„ë‹ˆë¯€ë¡œ consecutive_404_errorsë¥¼ ì¦ê°€ì‹œí‚¤ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                # ë˜ëŠ” íŠ¹ì • íšŸìˆ˜ ì´ìƒ ë°œìƒ ì‹œ í•´ë‹¹ í•™ê³¼ ê±´ë„ˆë›°ê¸° ë“±ì˜ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            else:  # JSONDecodeError, ValueError, ê¸°íƒ€ Exception
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ê¸°íƒ€ ì˜¤ë¥˜ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")
            # === ìˆ˜ì • ë ===

            # HTML Fallback ì‹œë„
            try:
                html_content = await fetch_text(list_url)
                ids_html = html_select(html_content, "td.no")
                titles_html = html_select(html_content, "td.title a")
                links_html = html_select(html_content, "td.title a", "href")
                dates_html = html_select(html_content, "td.date")

                min_len = min(len(ids_html), len(titles_html), len(links_html), len(dates_html))
                if min_len == 0 and (len(ids_html) + len(titles_html) + len(links_html) + len(dates_html) > 0):
                    logger.warning(f"[{dept.name} ({board_key})] HTMLì—ì„œ ì¼ë¶€ ì •ë³´ë§Œ ì¶”ì¶œë¨. íŒŒì‹± ê±´ë„ˆëœ€.")
                elif min_len > 0:
                    logger.trace(f"[{dept.name} ({board_key})] HTML Fallback ì„±ê³µ. {min_len}ê°œ í•­ëª© í›„ë³´ ë°œê²¬.")

                for i in range(min_len):
                    # ... (HTML íŒŒì‹± ë° ì¦ë¶„ ë¹„êµ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼) ...
                    post_id_str = clean_text(ids_html[i])
                    if not post_id_str.isdigit():
                        id_match_from_url = re.search(r'(?:idx|id|no|seq)=(\d+)', links_html[i], re.I)
                        if id_match_from_url:
                            post_id_str = id_match_from_url.group(1)
                        else:
                            logger.warning(
                                f"[{dept.name} ({board_key})] HTML í•­ëª© IDê°€ ìˆ«ìê°€ ì•„ë‹ˆê³  URLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ ('{ids_html[i]}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue

                    if post_id_str.isdigit() and last_post_id_db.isdigit():
                        if int(post_id_str) <= int(last_post_id_db):
                            stop_crawling_current_board = True;
                            break
                    elif post_id_str <= last_post_id_db and post_id_str != "":
                        stop_crawling_current_board = True;
                        break

                    title = clean_text(titles_html[i])
                    raw_url = links_html[i]
                    date_str = dates_html[i]

                    parsed_date = parse_date_flexible(date_str)
                    if not parsed_date:
                        logger.warning(
                            f"[{dept.name} ({board_key})] HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    full_url = urljoin(list_url, raw_url)
                    posts_data.append({
                        "dept_id": dept.id, "board": board_key, "post_id": post_id_str,
                        "title": title, "url": full_url, "posted_at": parsed_date
                    })

                if posts_data: current_page_fetch_successful = True
                consecutive_404_errors = 0
                if stop_crawling_current_board: break

            except ClientError as e_html_fetch:
                # === ìˆ˜ì •ëœ ì˜¤ë¥˜ ì²˜ë¦¬ ë¶€ë¶„ ===
                if hasattr(e_html_fetch, 'status') and e_html_fetch.status == 404:  # type: ignore
                    logger.error(
                        f"[{dept.name} ({board_key})] HTML Fallback ì²˜ë¦¬ ì¤‘ HTTP ì˜¤ë¥˜ - 404 Not Found ({list_url}): {e_html_fetch.message}")
                    consecutive_404_errors += 1
                elif isinstance(e_html_fetch, ClientError):  # status ì—†ëŠ” ClientError
                    logger.error(
                        f"[{dept.name} ({board_key})] HTML Fallback ì²˜ë¦¬ ì¤‘ ì—°ê²° ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
                else:  # ê¸°íƒ€ ì˜ˆì™¸
                    logger.error(
                        f"[{dept.name} ({board_key})] HTML Fallback ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {type(e_html_fetch).__name__} - {e_html_fetch}")
                # === ìˆ˜ì • ë ===
            except Exception as e_html_parse:
                logger.error(f"[{dept.name} ({board_key})] HTML Fallback íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {e_html_parse}")

        # ë£¨í”„ ì¢…ë£Œ ì¡°ê±´ (ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë¯€ë¡œ, ì—¬ê¸°ì„œ í•­ìƒ break ë©ë‹ˆë‹¤)
        if stop_crawling_current_board:
            logger.info(f"[{dept.name} ({board_key})] ì¦ë¶„ ìˆ˜ì§‘ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ì²« í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ë‹¨.")
        elif not current_page_fetch_successful and consecutive_404_errors >= 1:
            logger.warning(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ë¶€í„° 404 ì˜¤ë¥˜ ë°œìƒ ë˜ëŠ” ì—°ê²° ì‹¤íŒ¨. í•´ë‹¹ ê²Œì‹œíŒ ìˆ˜ì§‘ ì¤‘ë‹¨.")
        elif not current_page_fetch_successful:
            logger.info(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        if posts_data:
            try:
                with get_session() as sess:
                    sess.bulk_insert_mappings(Notice, posts_data)
                    sess.commit()
                inserted_count += len(posts_data)
                logger.debug(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ {len(posts_data)}ê±´ DB ì €ì¥ ì™„ë£Œ.")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

        break  # ì²« í˜ì´ì§€ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë£¨í”„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¢…ë£Œ

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_key})] ì²« í˜ì´ì§€ ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:
        logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def crawl_department_notices(dept: Department):
    delay_before_dept_crawl = REQUEST_DELAY_DEPARTMENT_SECONDS
    if delay_before_dept_crawl > 0:
        logger.trace(f"'{dept.name}' í•™ê³¼ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì‹œì‘ ì „ {delay_before_dept_crawl:.1f}ì´ˆ ëŒ€ê¸°...")
        await asyncio.sleep(delay_before_dept_crawl)

    for board_key_val in BOARD_CODES:
        try:
            await crawl_board(dept, board_key_val)
        except Exception as e:  # crawl_board ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ëŠ” ì´ë¯¸ ìƒì„¸íˆ ë¡œê¹…ë  ê²ƒì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆë§Œ
            logger.opt(exception=True).error(f"[{dept.name} ({board_key_val})] ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ìµœì¢… ì˜ˆì™¸ ë°œìƒ: {e}")