# cnu_crawler/spiders/notices.py
from datetime import datetime
from typing import Dict, List
from loguru import logger
import json  # aiohttpëŠ” ì´ë¯¸ JSONDecodeErrorë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ëª…ì‹œì  import
from aiohttp import ClientError  # fetcherì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜ˆì™¸
from urllib.parse import urljoin  # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜í•˜ê¸° ìœ„í•¨

from cnu_crawler.core.fetcher import fetch_json, fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session
from cnu_crawler.utils import clean_text  # ì œëª© ë“± í…ìŠ¤íŠ¸ ì •ì œìš© (í•„ìš”ì‹œ)

# FIXME: ì‹¤ì œ ê²Œì‹œíŒ ì½”ë“œë‚˜ URL íŒŒë¼ë¯¸í„°ê°€ ë³€ê²½ë˜ì—ˆë‹¤ë©´ ìˆ˜ì • í•„ìš”.
BOARD_CODES = {  #
    "undergrad": "board?code=undergrad_notice",  #
    "grad": "board?code=grad_notice"  #
}


# ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ ì‹œë„í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def parse_date_flexible(date_str: str) -> datetime | None:
    if not date_str:
        return None

    # ì¼ë°˜ì ì¸ í˜•ì‹ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì‹œë„
    formats_to_try = [
        "%Y-%m-%dT%H:%M:%S",  # ISO ë¶€ë¶„ (TZ ì •ë³´ ì—†ì´)
        "%Y-%m-%d %H:%M:%S",
        "%Y.%m.%d %H:%M:%S",
        "%Y-%m-%d",
        "%Y.%m.%d",
        "%y-%m-%d",  # 24-05-28
        "%y.%m.%d",
    ]
    # ì›ë³¸ ì½”ë“œì˜ ISO í˜•ì‹ ì²˜ë¦¬
    if "T" in date_str:  #
        try:
            return datetime.fromisoformat(date_str.replace("Z", "+00:00"))  # # Zë¥¼ offsetìœ¼ë¡œ ëª…ì‹œ
        except ValueError:
            pass  # ë‹¤ë¥¸ í¬ë§· ì‹œë„

    for fmt in formats_to_try:
        try:
            return datetime.strptime(date_str, fmt)  #
        except ValueError:
            continue

    logger.warning(f"ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± ì‹¤íŒ¨ (ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹): '{date_str}'")
    return None


async def crawl_board(dept: Department, board_key: str):
    """ë‹¨ì¼ ê²Œì‹œíŒ(í•™ë¶€/ëŒ€í•™ì›) ì¦ë¶„ ìˆ˜ì§‘."""
    base_url_dept = dept.url.rstrip("/")  #
    page = 1
    inserted_count = 0
    max_pages_to_crawl = 20  # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© (í•„ìš”ì‹œ ì¡°ì •)

    logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì‹œì‘")

    with get_session() as sess:
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_key)  #
                       .order_by(Notice.post_id.desc())  #
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"  #
        logger.debug(f"[{dept.name} ({board_key})] ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ID: {last_post_id_db}")

    while page <= max_pages_to_crawl:
        # FIXME: ì‹¤ì œ ê³µì§€ì‚¬í•­ ëª©ë¡ URL êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •. page íŒŒë¼ë¯¸í„° ì´ë¦„ ë“± í™•ì¸.
        # ì˜ˆ: /list.do?page=1&boardId=xxx
        board_path = BOARD_CODES.get(board_key)
        if not board_path:
            logger.error(f"[{dept.name}] ìœ íš¨í•˜ì§€ ì•Šì€ board_key: {board_key}")
            return

        # list_url = f"{base_url_dept}/{board_path}&page={page}" #
        # URL ì¡°í•© ì‹œ '?'ê°€ ì´ë¯¸ board_pathì— ìˆëŠ”ì§€, base_url_deptì— ì´ë¯¸ query stringì´ ìˆëŠ”ì§€ ë“±ì„ ê³ ë ¤í•´ì•¼ í•¨
        # ì¢€ ë” ì•ˆì „í•œ ë°©ë²•:
        if '?' in board_path:
            list_url = f"{base_url_dept}/{board_path}&page={page}"
        else:
            list_url = f"{base_url_dept}/{board_path}?page={page}"

        logger.debug(f"í˜ì´ì§€ {page} ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")
        posts_data = []
        stop_crawling_current_board = False

        try:  # JSON API ì‹œë„
            data = await fetch_json(list_url)  #

            # FIXME: ì‹¤ì œ API ì‘ë‹µì—ì„œ ê²Œì‹œê¸€ ëª©ë¡ì„ ë‹´ê³  ìˆëŠ” í‚¤ë¡œ ìˆ˜ì •.
            # ì˜ˆ: data.get('result', {}).get('list', [])
            current_page_posts = data.get("posts") if isinstance(data, dict) else data  #
            if not isinstance(current_page_posts, list):
                logger.warning(f"JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤ ({list_url}). Fallback ì‹œë„. ë°ì´í„°: {str(data)[:200]}")
                raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

            for p_item in current_page_posts:
                # FIXME: ì•„ë˜ í‚¤ë“¤ì€ ì‹¤ì œ API ì‘ë‹µì— ë§ê²Œ ìˆ˜ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
                post_id = str(p_item.get("id"))  #
                title = p_item.get("title")  #
                raw_url = p_item.get("url")  #
                date_str = p_item.get("date")  #

                if not all([post_id, title, raw_url, date_str]):
                    logger.warning(f"JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½ (id, title, url, date): {p_item}")
                    continue

                # ì¦ë¶„ ìˆ˜ì§‘ ë¡œì§: DBì˜ ë§ˆì§€ë§‰ IDë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìœ¼ë©´ ì¤‘ë‹¨
                # ì£¼ì˜: ê²Œì‹œíŒì— ë”°ë¼ IDê°€ ë¬¸ìì—´ì´ê±°ë‚˜, ìˆ«ìê°€ ì•„ë‹ˆê±°ë‚˜, ìˆœì„œê°€ ë’¤ì£½ë°•ì£½ì¼ ìˆ˜ ìˆìŒ.
                # ì´ ê²½ìš°, ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œ ì¦ë¶„ ìˆ˜ì§‘í•˜ê±°ë‚˜, ë” ë³µì¡í•œ ë¹„êµ ë¡œì§ í•„ìš”.
                try:
                    if post_id.isdigit() and last_post_id_db.isdigit():
                        if int(post_id) <= int(last_post_id_db):
                            stop_crawling_current_board = True
                            break
                    elif post_id <= last_post_id_db:  # ë¬¸ìì—´ ë¹„êµ (ì¼ë¶€ ê²½ìš°ì—ë§Œ ìœ íš¨)
                        stop_crawling_current_board = True
                        break
                except ValueError:  # IDê°€ ìˆ«ìë¡œ ë³€í™˜ ì•ˆë  ë•Œ (ì˜ˆ: 'ê³µì§€', 'ì¤‘ìš”' ë“±)
                    logger.trace(f"Post ID '{post_id}'ëŠ” ìˆ«ìí˜•ì´ ì•„ë‹˜. ì¦ë¶„ ë¹„êµì—ì„œ ê±´ë„ˆëœ€.")
                    pass  # ì¼ë‹¨ ê³„ì† ì§„í–‰ (ìµœì‹  ê¸€ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŒ)

                posted_at_dt = parse_date_flexible(date_str)
                if not posted_at_dt:
                    logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê²Œì‹œê¸€ ê±´ë„ˆëœ€: ID={post_id}, Date='{date_str}'")
                    continue

                # URL ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                # full_url = raw_url if raw_url.startswith("http") else urljoin(base_url_dept, raw_url) # # base_url_dept ë˜ëŠ” list_url ì‚¬ìš©
                full_url = urljoin(list_url, raw_url)  # API/HTML ëª©ë¡ í˜ì´ì§€ URL ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ê²½ë¡œ í•´ì„

                posts_data.append({  #
                    "dept_id": dept.id,  #
                    "board": board_key,  #
                    "post_id": post_id,  #
                    "title": clean_text(title),  #
                    "url": full_url,  #
                    "posted_at": posted_at_dt  #
                })
            if stop_crawling_current_board:
                logger.info(f"[{dept.name} ({board_key})] í˜ì´ì§€ {page}ì—ì„œ ì´ì „ì— ìˆ˜ì§‘í•œ ê²Œì‹œê¸€ ID({last_post_id_db})ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¤‘ë‹¨.")
                break

        except (ClientError, json.JSONDecodeError, ValueError, Exception) as e:
            logger.warning(f"JSON API í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨ ({list_url}): {e}. HTML Fallback ì‹œë„.")

            try:  # HTML Fallback
                html = await fetch_text(list_url)  #

                # FIXME: ì•„ë˜ CSS ì„ íƒìë“¤ì€ ì›¹ì‚¬ì´íŠ¸ HTML êµ¬ì¡° ë³€ê²½ ì‹œ ë°˜ë“œì‹œ ìˆ˜ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
                # ê²Œì‹œê¸€ ID, ì œëª©, ë§í¬, ë‚ ì§œ ë“±ì„ í¬í•¨í•˜ëŠ” ê°€ì¥ ë°”ê¹¥ìª½ ë°˜ë³µ ìš”ì†Œë¥¼ ë¨¼ì € ì„ íƒí•˜ëŠ” ê²ƒì´ ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                # ì˜ˆ: notice_item_selector = "table.board_list > tbody > tr"
                # items = soup.select(notice_item_selector)
                # for item_html in items:
                #    post_id = html_first(item_html, "td.no_column_selector")
                #    ...

                # í˜„ì¬ ì½”ë“œ ê¸°ë°˜ ìˆ˜ì •:
                # ê²Œì‹œê¸€ ë²ˆí˜¸ ì„ íƒì (ì˜ˆ: <td class="no">...</td>)
                ids_selector = "td.no"  #
                # ê²Œì‹œê¸€ ì œëª© ì„ íƒì (ì˜ˆ: <td class="title"><a>...</a></td>)
                titles_selector = "td.title a"  #
                # ê²Œì‹œê¸€ ë§í¬ ì„ íƒì (ì œëª©ê³¼ ë™ì¼í•œ <a> íƒœê·¸ì˜ href ì†ì„±)
                links_selector = "td.title a"  #
                # ê²Œì‹œê¸€ ë‚ ì§œ ì„ íƒì (ì˜ˆ: <td class="date">...</td>)
                dates_selector = "td.date"  #

                post_ids_html = html_select(html, ids_selector)  #
                titles_html = html_select(html, titles_selector)  #
                links_html = html_select(html, links_selector, attr="href")  #
                dates_html = html_select(html, dates_selector)  #

                if not all([post_ids_html, titles_html, links_html, dates_html]):
                    logger.warning(f"HTMLì—ì„œ ì¼ë¶€ í•„ìˆ˜ ì •ë³´(ID, ì œëª©, ë§í¬, ë‚ ì§œ)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€: {page}, URL: {list_url}")

                min_len = min(len(post_ids_html), len(titles_html), len(links_html), len(dates_html))
                if len(post_ids_html) != min_len or len(titles_html) != min_len or \
                        len(links_html) != min_len or len(dates_html) != min_len:
                    logger.warning(
                        f"HTMLì—ì„œ ì¶”ì¶œëœ ê²Œì‹œê¸€ ì •ë³´ì˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (IDs: {len(post_ids_html)}, Titles: {len(titles_html)}, Links: {len(links_html)}, Dates: {len(dates_html)}). ìµœì†Œ ê°œìˆ˜({min_len})ë§Œí¼ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

                for i in range(min_len):
                    post_id = str(post_ids_html[i]).strip()
                    # 'ê³µì§€', 'ì¤‘ìš”' ë“±ì˜ í…ìŠ¤íŠ¸ ID ì²˜ë¦¬
                    if not post_id.isdigit():
                        # ì‹¤ì œ IDê°€ ë‹¤ë¥¸ ê³³ì— ìˆê±°ë‚˜, ë§í¬ì—ì„œ ì¶”ì¶œí•´ì•¼ í•  ìˆ˜ ìˆìŒ.
                        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ê³ ìœ ì„±ì„ ìœ„í•´ í•´ì‹œê°’ ë“±ìœ¼ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜, ê±´ë„ˆë›¸ ìˆ˜ ìˆìŒ.
                        # ì˜ˆ: import hashlib; post_id = hashlib.md5(links_html[i].encode()).hexdigest()[:8]
                        logger.trace(f"HTMLì—ì„œ ìˆ«ì ì•„ë‹Œ ID ë°œê²¬: '{post_id}'. ë§í¬ ê¸°ë°˜ìœ¼ë¡œ ID ëŒ€ì²´ ì‹œë„ ë˜ëŠ” ê±´ë„ˆëœ€.")
                        # ë˜ëŠ” ë§í¬ì—ì„œ IDë¥¼ ì¶”ì¶œí•˜ëŠ” ë¡œì§ ì¶”ê°€
                        match_id_from_url = re.search(r'postId=(\d+)|articleNo=(\d+)|bbsSn=(\d+)', links_html[i])
                        if match_id_from_url:
                            post_id = next(g for g in match_id_from_url.groups() if g is not None)
                        else:  # ì • IDë¥¼ ëª»ì°¾ê² ìœ¼ë©´ ì œëª©+ë‚ ì§œë¡œ ì„ì‹œ ID (ë§¤ìš° ë¶ˆì•ˆì •)
                            # post_id = f"html_{hashlib.md5((titles_html[i] + dates_html[i]).encode()).hexdigest()[:8]}"
                            # ë˜ëŠ” ê·¸ëƒ¥ ê±´ë„ˆë›°ê¸°
                            logger.warning(f"HTMLì—ì„œ '{post_ids_html[i]}' ID ì²˜ë¦¬ ë¶ˆê°€. ê²Œì‹œê¸€ ê±´ë„ˆëœ€: {titles_html[i]}")
                            continue

                    # ì¦ë¶„ ìˆ˜ì§‘ ë¡œì§ (HTML fallbackì—ì„œë„ ë™ì¼í•˜ê²Œ ì ìš©)
                    try:
                        if post_id.isdigit() and last_post_id_db.isdigit():
                            if int(post_id) <= int(last_post_id_db):
                                stop_crawling_current_board = True
                                break
                        elif post_id <= last_post_id_db:
                            stop_crawling_current_board = True
                            break
                    except ValueError:
                        logger.trace(f"HTML Post ID '{post_id}'ëŠ” ìˆ«ìí˜•ì´ ì•„ë‹˜. ì¦ë¶„ ë¹„êµì—ì„œ ê±´ë„ˆëœ€.")
                        pass

                    title = titles_html[i]
                    raw_url = links_html[i]
                    date_str = dates_html[i]

                    posted_at_dt = parse_date_flexible(date_str)
                    if not posted_at_dt:
                        logger.warning(f"HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê²Œì‹œê¸€ ê±´ë„ˆëœ€: ID={post_id}, Date='{date_str}'")
                        continue

                    # full_url = raw_url if raw_url.startswith("http") else urljoin(base_url_dept, raw_url) #
                    full_url = urljoin(list_url, raw_url)  # HTML ëª©ë¡ í˜ì´ì§€ URL ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ê²½ë¡œ í•´ì„

                    posts_data.append({  #
                        "dept_id": dept.id,  #
                        "board": board_key,  #
                        "post_id": post_id,  #
                        "title": clean_text(title),  #
                        "url": full_url,  #
                        "posted_at": posted_at_dt  #
                    })
                if stop_crawling_current_board:
                    logger.info(
                        f"[{dept.name} ({board_key})] HTML Fallback í˜ì´ì§€ {page}ì—ì„œ ì´ì „ì— ìˆ˜ì§‘í•œ ê²Œì‹œê¸€ ID({last_post_id_db})ì— ë„ë‹¬í•˜ì—¬ ìˆ˜ì§‘ ì¤‘ë‹¨.")
                    break

            except (ClientError, Exception) as e_html:
                logger.error(f"HTML Fallback ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ ({list_url}): {e_html}")
                # ì´ ê²½ìš° í•´ë‹¹ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê³  ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆìŒ
                page += 1
                continue

        if not posts_data and not stop_crawling_current_board:  # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì•„ë¬´ê²ƒë„ ëª»ê°€ì ¸ì™”ê³ , ì¦ë¶„ ì¤‘ë‹¨ë„ ì•„ë‹ˆë©´
            logger.info(f"[{dept.name} ({board_key})] í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê°€ì ¸ì˜¬ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            break

        if posts_data:
            try:
                with get_session() as sess:
                    sess.bulk_insert_mappings(Notice, posts_data)  #
                    sess.commit()  #
                inserted_count += len(posts_data)
                logger.debug(f"[{dept.name} ({board_key})] í˜ì´ì§€ {page}ì—ì„œ {len(posts_data)}ê±´ DB ì €ì¥ ì™„ë£Œ.")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")
                # ì¼ë¶€ ì €ì¥ ì‹¤íŒ¨ì‹œ rollback ê³ ë ¤

        if stop_crawling_current_board:  # ì´ë¯¸ ìœ„ì—ì„œ break í–ˆì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ í•œ ë²ˆ ë”
            break

        page += 1

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_key})] ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:
        logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")


async def crawl_department_notices(dept: Department):
    for board_key in BOARD_CODES:  #
        try:
            await crawl_board(dept, board_key)  #
        except Exception as e:
            logger.opt(exception=True).error(f"[{dept.name} ({board_key})] ê²Œì‹œíŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")