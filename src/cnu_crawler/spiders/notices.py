# src/cnu_crawler/spiders/notices.py
import asyncio
import json
from datetime import datetime
from typing import Dict, List
from urllib.parse import urljoin

from loguru import logger
from aiohttp import ClientError

from cnu_crawler.core.fetcher import fetch_json, fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import Department, Notice, get_session
from cnu_crawler.utils import clean_text, parse_date_flexible
from cnu_crawler.config import (
    REQUEST_DELAY_NOTICE_PAGE_SECONDS,
    REQUEST_DELAY_DEPARTMENT_SECONDS
)

BOARD_CODES = {
    "undergrad": "board?code=undergrad_notice",
    "grad": "board?code=grad_notice"
}


def get_notice_list_url(dept: Department, board_key: str, page: int) -> str:
    department_base_url = dept.url.rstrip("/")

    # --- !! ì¤‘ìš” !! ---
    # ì•„ë˜ëŠ” í•™ê³¼ë³„ URL ê·œì¹™ì„ ì ìš©í•˜ëŠ” ì˜ˆì‹œ ë¶€ë¶„ì…ë‹ˆë‹¤.
    # ì‹¤ì œ ê° í•™ê³¼ì˜ ì •í™•í•œ ê³µì§€ì‚¬í•­ URL êµ¬ì¡°ì— ë§ê²Œ ì´ ë¶€ë¶„ì„ ìƒì„¸íˆ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆë¥¼ ë“¤ì–´, dept.code (í•™ê³¼ ê³ ìœ  ì½”ë“œ) ë˜ëŠ” dept.nameì„ ì‚¬ìš©í•˜ì—¬ ë¶„ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    # ì˜ˆì‹œ: ê³µê³¼ëŒ€í•™ ëŒ€í•™ì›, ì˜ˆìˆ ëŒ€í•™ í•™ë¶€ ë“± íŠ¹ì • í•™ê³¼ì— ëŒ€í•œ ê·œì¹™
    # if dept.name == "ê³µê³¼ëŒ€í•™" and board_key == "grad":
    #     # FIXME: ê³µê³¼ëŒ€í•™ ëŒ€í•™ì› ê³µì§€ì‚¬í•­ì˜ ì‹¤ì œ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
    #     # ì˜ˆ: actual_board_url = f"https://eng.cnu.ac.kr/eng/real/grad_notice_path.do?pageNo={page}"
    #     # return actual_board_url
    #     pass # íŠ¹ë³„ ê·œì¹™ì´ ì—†ë‹¤ë©´ ì•„ë˜ ê¸°ë³¸ ê·œì¹™ìœ¼ë¡œ
    # elif dept.name == "ì˜ˆìˆ ëŒ€í•™" and board_key == "undergrad":
    #     # FIXME: ì˜ˆìˆ ëŒ€í•™ í•™ë¶€ ê³µì§€ì‚¬í•­ì˜ ì‹¤ì œ URL í…œí”Œë¦¿ìœ¼ë¡œ ìˆ˜ì •
    #     # ì˜ˆ: actual_board_url = f"https://art.cnu.ac.kr/art/real/undergrad_path.do?page_num={page}"
    #     # return actual_board_url
    #     pass

    # ê¸°ë³¸ URL ìƒì„± ê·œì¹™
    board_path_segment = BOARD_CODES.get(board_key)
    if not board_path_segment:
        logger.error(f"[{dept.name}] ìœ íš¨í•˜ì§€ ì•Šì€ board_key: {board_key}ì— ëŒ€í•œ BOARD_CODE ì—†ìŒ")
        return f"invalid_board_key_for_{dept.name}_{board_key}"

    final_url_base = f"{department_base_url}/{board_path_segment}"
    if '?' in final_url_base:
        return f"{final_url_base}&page={page}"
    else:
        return f"{final_url_base}?page={page}"


async def crawl_board(dept: Department, board_key: str):
    page = 1  # í•­ìƒ ì²« í˜ì´ì§€ë§Œ ëŒ€ìƒìœ¼ë¡œ í•¨
    inserted_count = 0
    # max_pages_to_crawl ë³€ìˆ˜ë¥¼ 1ë¡œ ì„¤ì •í•˜ì—¬ ì²« í˜ì´ì§€ë§Œ í¬ë¡¤ë§í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    max_pages_to_crawl = 1

    delay_per_page = REQUEST_DELAY_NOTICE_PAGE_SECONDS

    logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œì‘")

    # ì¦ë¶„ ìˆ˜ì§‘ì„ ìœ„í•œ last_post_id_db ë¡œì§ì€ ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¬ ê²½ìš°,
    # ê¸°ì¡´ DB ë‚´ìš©ê³¼ ë¹„êµí•˜ëŠ” ìš©ë„ë¡œëŠ” ê³„ì† ìœ íš¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ë§Œì•½ ì²« í˜ì´ì§€ì˜ ëª¨ë“  ê¸€ì„ í•­ìƒ ìƒˆë¡œ ê°€ì ¸ì˜¤ê³  ì‹¶ë‹¤ë©´ ì´ ë¶€ë¶„ì€ ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    with get_session() as sess:
        last_notice = (sess.query(Notice)
                       .filter_by(dept_id=dept.id, board=board_key)
                       .order_by(Notice.post_id.desc())
                       .first())
        last_post_id_db = last_notice.post_id if last_notice else "0"
    logger.debug(f"[{dept.name} ({board_key})] DBì˜ ë§ˆì§€ë§‰ ê²Œì‹œê¸€ ID: {last_post_id_db} (ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘ ì‹œ ì°¸ê³ ìš©)")

    consecutive_404_errors = 0

    # while ë£¨í”„ëŠ” ì´ì œ ìµœëŒ€ í•œ ë²ˆë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤ (max_pages_to_crawl = 1 ì´ë¯€ë¡œ).
    # ë˜ëŠ” ë£¨í”„ í›„ ë°”ë¡œ break í•˜ëŠ” ë°©ì‹ìœ¼ë¡œë„ êµ¬í˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    while page <= max_pages_to_crawl:
        # ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë¯€ë¡œ í˜ì´ì§€ ê°„ delayëŠ” í•„ìš” ì—†ì–´ì§ (page > 1 ì¡°ê±´ì´ í•­ìƒ false)
        # if page > 1 and delay_per_page > 0:
        #     logger.trace(f"[{dept.name} ({board_key})] ë‹¤ìŒ í˜ì´ì§€ ìš”ì²­ ì „ {delay_per_page:.1f}ì´ˆ ëŒ€ê¸°...")
        #     await asyncio.sleep(delay_per_page)

        list_url = get_notice_list_url(dept, board_key, page)
        if "invalid_board_key" in list_url:
            logger.error(f"[{dept.name} ({board_key})] ìœ íš¨í•œ ê³µì§€ì‚¬í•­ ëª©ë¡ URLì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì¤‘ë‹¨.")
            break  # URL ìƒì„± ì‹¤íŒ¨ ì‹œ ë£¨í”„ ì¢…ë£Œ

        logger.debug(f"í˜ì´ì§€ {page} ê³µì§€ì‚¬í•­ ëª©ë¡ ìš”ì²­: {list_url}")
        posts_data: List[Dict] = []
        stop_crawling_current_board = False  # ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë¯€ë¡œ, ì¦ë¶„ ë¹„êµ ê²°ê³¼ì— ë”°ë¼ ì¤‘ë‹¨ë  ìˆ˜ ìˆìŒ
        current_page_fetch_successful = False

        try:  # JSON API ì‹œë„
            data = await fetch_json(list_url)
            current_page_posts = data.get("posts") if isinstance(data, dict) else data

            if not isinstance(current_page_posts, list):
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API ì‘ë‹µì˜ 'posts'ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ ({list_url}). ë°ì´í„°: {str(data)[:200]}. HTML Fallback ì‹œë„.")
                raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

            logger.trace(f"[{dept.name} ({board_key})] JSON API ì„±ê³µ. {len(current_page_posts)}ê°œ í•­ëª© ìˆ˜ì‹ .")
            for p_item in current_page_posts:
                post_id_str = str(p_item.get("id", ""))
                title = clean_text(str(p_item.get("title", "")))
                raw_url = p_item.get("url", "")
                date_str = p_item.get("date", "")

                if not all([post_id_str, title, raw_url, date_str]):
                    logger.warning(f"[{dept.name} ({board_key})] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´ ëˆ„ë½: {p_item}")
                    continue

                # ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë”ë¼ë„, ì´ë¯¸ DBì— ìˆëŠ” ê¸€ì€ ê±´ë„ˆë›°ê¸° ìœ„í•œ ì¦ë¶„ ë¹„êµ
                if post_id_str.isdigit() and last_post_id_db.isdigit():
                    if int(post_id_str) <= int(last_post_id_db):
                        stop_crawling_current_board = True;
                        break
                elif post_id_str <= last_post_id_db and post_id_str != "":
                    stop_crawling_current_board = True;
                    break

                parsed_date = parse_date_flexible(date_str)
                if not parsed_date:
                    logger.warning(
                        f"[{dept.name} ({board_key})] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                full_url = urljoin(list_url, raw_url)
                posts_data.append({
                    "dept_id": dept.id, "board": board_key, "post_id": post_id_str,
                    "title": title, "url": full_url, "posted_at": parsed_date
                })

            if posts_data: current_page_fetch_successful = True
            consecutive_404_errors = 0
            if stop_crawling_current_board: break

        except (ClientError, json.JSONDecodeError, ValueError, Exception) as e_json:
            if isinstance(e_json, ClientError) and e_json.status == 404:  # type: ignore
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì‹¤íŒ¨ - 404 Not Found ({list_url}). HTML Fallback ì‹œë„.")
                consecutive_404_errors += 1
            elif isinstance(e_json, asyncio.TimeoutError):
                logger.warning(f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ ì‹œê°„ ì´ˆê³¼ ({list_url}). HTML Fallback ì‹œë„.")
            else:
                logger.warning(
                    f"[{dept.name} ({board_key})] JSON API í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨ ({list_url}): {type(e_json).__name__} - {e_json}. HTML Fallback ì‹œë„.")

            try:  # HTML Fallback
                html_content = await fetch_text(list_url)
                ids_html = html_select(html_content, "td.no")
                titles_html = html_select(html_content, "td.title a")
                links_html = html_select(html_content, "td.title a", "href")
                dates_html = html_select(html_content, "td.date")

                min_len = min(len(ids_html), len(titles_html), len(links_html), len(dates_html))
                if min_len == 0 and (len(ids_html) + len(titles_html) + len(links_html) + len(dates_html) > 0):
                    logger.warning(f"[{dept.name} ({board_key})] HTMLì—ì„œ ì¼ë¶€ ì •ë³´ë§Œ ì¶”ì¶œë¨. íŒŒì‹± ê±´ë„ˆëœ€.")
                elif min_len > 0:
                    logger.trace(f"[{dept.name} ({board_key})] HTML Fallback ì„±ê³µ. {min_len}ê°œ í•­ëª© í›„ë³´ ë°œê²¬.")

                for i in range(min_len):
                    post_id_str = clean_text(ids_html[i])
                    if not post_id_str.isdigit():
                        id_match_from_url = re.search(r'(?:idx|id|no|seq)=(\d+)', links_html[i], re.I)
                        if id_match_from_url:
                            post_id_str = id_match_from_url.group(1)
                        else:
                            logger.warning(
                                f"[{dept.name} ({board_key})] HTML í•­ëª© IDê°€ ìˆ«ìê°€ ì•„ë‹ˆê³  URLì—ì„œ ì¶”ì¶œ ë¶ˆê°€ ('{ids_html[i]}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue

                    if post_id_str.isdigit() and last_post_id_db.isdigit():
                        if int(post_id_str) <= int(last_post_id_db):
                            stop_crawling_current_board = True;
                            break
                    elif post_id_str <= last_post_id_db and post_id_str != "":
                        stop_crawling_current_board = True;
                        break

                    title = clean_text(titles_html[i])
                    raw_url = links_html[i]
                    date_str = dates_html[i]

                    parsed_date = parse_date_flexible(date_str)
                    if not parsed_date:
                        logger.warning(
                            f"[{dept.name} ({board_key})] HTML ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (ID: {post_id_str}, ë‚ ì§œ: '{date_str}'). ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    full_url = urljoin(list_url, raw_url)
                    posts_data.append({
                        "dept_id": dept.id, "board": board_key, "post_id": post_id_str,
                        "title": title, "url": full_url, "posted_at": parsed_date
                    })
                if posts_data: current_page_fetch_successful = True
                consecutive_404_errors = 0
                if stop_crawling_current_board: break

            except ClientError as e_html_fetch:
                logger.error(
                    f"[{dept.name} ({board_key})] HTML Fallback ì²˜ë¦¬ ì¤‘ HTTP ì˜¤ë¥˜ ({list_url}): {e_html_fetch.status}, {e_html_fetch.message}")
                if e_html_fetch.status == 404:  # type: ignore
                    logger.warning(f"[{dept.name} ({board_key})] í˜ì´ì§€ {page} (URL: {list_url})ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ (404).")
                    consecutive_404_errors += 1
            except Exception as e_html_parse:
                logger.error(f"[{dept.name} ({board_key})] HTML Fallback íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({list_url}): {e_html_parse}")

        # ë£¨í”„ ì¢…ë£Œ ì¡°ê±´ (ì²« í˜ì´ì§€ë§Œ ê°€ì ¸ì˜¤ë¯€ë¡œ, ì—¬ê¸°ì„œ í•­ìƒ break ë©ë‹ˆë‹¤)
        if stop_crawling_current_board:
            logger.info(f"[{dept.name} ({board_key})] ì¦ë¶„ ìˆ˜ì§‘ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ì²« í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ë‹¨.")
        elif not current_page_fetch_successful and consecutive_404_errors >= 1:  # ì²« í˜ì´ì§€ê°€ 404ì¸ ê²½ìš°
            logger.warning(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ë¶€í„° 404 ì˜¤ë¥˜ ë°œìƒ. í•´ë‹¹ ê²Œì‹œíŒ ìˆ˜ì§‘ ì¤‘ë‹¨.")
        elif not current_page_fetch_successful:  # 404ëŠ” ì•„ë‹ˆì§€ë§Œ ë°ì´í„° ëª» ì–»ì€ ê²½ìš°
            logger.info(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        if posts_data:
            try:
                with get_session() as sess:
                    sess.bulk_insert_mappings(Notice, posts_data)
                    sess.commit()
                inserted_count += len(posts_data)
                logger.debug(f"[{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ {len(posts_data)}ê±´ DB ì €ì¥ ì™„ë£Œ.")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{dept.name} ({board_key})] ê³µì§€ì‚¬í•­ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")

        # ì²« í˜ì´ì§€ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë£¨í”„ë¥¼ ë¹ ì ¸ë‚˜ê°‘ë‹ˆë‹¤.
        break  # while page <= max_pages_to_crawl ë£¨í”„ë¥¼ ì—¬ê¸°ì„œ ì¢…ë£Œì‹œí‚µë‹ˆë‹¤.
        # page += 1 # ì´ ì¤„ì€ ë” ì´ìƒ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

    if inserted_count > 0:
        logger.success(f"ğŸ“„ [{dept.name} ({board_key})] ì²« í˜ì´ì§€ ìƒˆ ê³µì§€ ì´ {inserted_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
    else:  # inserted_countê°€ 0ì¸ ë‹¤ì–‘í•œ ê²½ìš° (ì¦ë¶„ìœ¼ë¡œ ì¤‘ë‹¨, ë°ì´í„° ì—†ìŒ, 404 ë“±)
        logger.info(f"ğŸ“„ [{dept.name} ({board_key})] ì²« í˜ì´ì§€ì—ì„œ ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ì´ ì—†ê±°ë‚˜ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


async def crawl_department_notices(dept: Department):
    delay_before_dept_crawl = REQUEST_DELAY_DEPARTMENT_SECONDS
    if delay_before_dept_crawl > 0:
        logger.trace(f"'{dept.name}' í•™ê³¼ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘ ì‹œì‘ ì „ {delay_before_dept_crawl:.1f}ì´ˆ ëŒ€ê¸°...")
        await asyncio.sleep(delay_before_dept_crawl)

    for board_key_val in BOARD_CODES:
        try:
            await crawl_board(dept, board_key_val)
        except Exception as e:
            logger.opt(exception=True).error(f"[{dept.name} ({board_key_val})] ê²Œì‹œíŒ í¬ë¡¤ë§ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")