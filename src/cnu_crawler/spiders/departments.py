# src/cnu_crawler/spiders/departments.py
import re
import json
from urllib.parse import urljoin, urlparse, urlunparse
from typing import List, Dict, Optional

from loguru import logger
from aiohttp import ClientError
# BeautifulSoupì„ ì‚¬ìš©í•œ ìƒì„¸ íŒŒì‹±ì´ í•„ìš”í•  ê²½ìš° (í˜„ì¬ëŠ” html_select ì‚¬ìš©)
# from bs4 import BeautifulSoup

from cnu_crawler.core.fetcher import fetch_text, fetch_json  # fetch_jsonì€ ê±°ì˜ ì‚¬ìš© ì•ˆ í•¨
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import College, Department, get_session  #
from cnu_crawler.utils import clean_text  #
from cnu_crawler.config import ROOT_URL  # í•„ìš”ì‹œ ì‚¬ìš©

# í•™ê³¼ ì´ë¦„ì— í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” í‚¤ì›Œë“œ (HTML íŒŒì‹± ì‹œ ê²€ì¦ìš©)
DEPT_KEYWORDS = ["í•™ê³¼", "í•™ë¶€", "ì „ê³µ", "department", "school of", "major", "division", "ê³¼ì •", "ìœµí•©"]
# í•™ê³¼ ì´ë¦„ì—ì„œ ì œì™¸í•  í‚¤ì›Œë“œ (ì˜ˆ: ì¼ë°˜ ë§í¬ ë°©ì§€)
EXCLUDE_KEYWORDS_IN_NAME = ["ëŒ€í•™ì•ˆë‚´", "ì…í•™ì•ˆë‚´", "ëŒ€í•™ìƒí™œ", "ì»¤ë®¤ë‹ˆí‹°", "ì˜¤ì‹œëŠ”ê¸¸", "ì‚¬ì´íŠ¸ë§µ", "ì†Œê°œ", "ë”ë³´ê¸°", "ë°”ë¡œê°€ê¸°"]


def _generate_department_code(college_code: str, dept_name: str, url: str) -> str:
    """í•™ê³¼ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (college_code ë‚´ì—ì„œ ê³ ìœ í•˜ë„ë¡)."""
    cleaned_name = re.sub(r'\s+', '', dept_name.lower())
    alnum_name = re.sub(r'[^a-z0-9]', '', cleaned_name)[:15]  # ì´ë¦„ ì¼ë¶€ ì‚¬ìš©

    # URLì—ì„œ ì˜ë¯¸ ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œ ì‹œë„ (ë” ì •êµí•œ ë¡œì§ í•„ìš” ê°€ëŠ¥)
    path_parts = [part for part in urlparse(url).path.split('/') if part and not part.endswith((".do", ".jsp"))]
    url_suffix = path_parts[-1][:10] if path_parts else ""

    base_code = f"{college_code[:10]}_{alnum_name}_{url_suffix}"
    # í•´ì‹œë¥¼ ì¶”ê°€í•˜ì—¬ ê³ ìœ ì„± ë³´ì¥ ì‹œë„
    return f"dept_{base_code}_{hash(url + dept_name)[:6]}"[:50]  # ìµœëŒ€ ê¸¸ì´ ì œí•œ


def _extract_notice_url_template_from_page(html_content: str, base_url: str, keywords: List[str]) -> Optional[str]:
    """
    ì£¼ì–´ì§„ HTML ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë§í¬ë¥¼ ì°¾ì•„ ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ë§¤ìš° íœ´ë¦¬ìŠ¤í‹±í•˜ë©°, ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.
    """
    all_links = html_select(html_content, "a", attr="href")  #
    all_texts = html_select(html_content, "a")  #

    for text, href in zip(all_texts, all_links):
        cleaned_text = clean_text(text)
        if any(kw.lower() in cleaned_text.lower() for kw in keywords):
            # ë§í¬ê°€ ìœ íš¨í•œì§€, í˜ì´ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì–´ë–»ê²Œ ë¶™ì¼ì§€ ë“± ì¶”ê°€ ë¶„ì„ í•„ìš”
            full_url = urljoin(base_url, href)
            # ê°„ë‹¨íˆ ?page={} ë˜ëŠ” &page={} ë¥¼ ë¶™ì´ëŠ” í˜•íƒœë¡œ ê°€ì •
            # ì‹¤ì œë¡œëŠ” í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì´ë¦„ê³¼ í˜•ì‹ì„ ì•Œì•„ë‚´ì•¼ í•¨
            parsed_url = urlparse(full_url)
            # ì´ë¯¸ ì¿¼ë¦¬ê°€ ìˆë‹¤ë©´ &page={}, ì—†ë‹¤ë©´ ?page={}
            if parsed_url.query:
                return full_url + "&page={}"
            else:
                return full_url + "?page={}"
    return None


async def _parse_departments_from_grad_page(college: College, html_content: str) -> List[Dict]:
    """
    `grad.cnu.ac.kr` í˜ì´ì§€ì—ì„œ íŠ¹ì • 'ëŒ€í•™ëª…' (college.name) í•˜ìœ„ì˜ í•™ê³¼ ëª©ë¡ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
    ìš”êµ¬ì‚¬í•­: /html/body/div[1]/div[3]/div[2]/div[3]/div/div/div[1]/h4 ëŒ€í•™ëª…
              /html/body/div[1]/div[3]/div[2]/div[3]/div/div/div[1]/ul/li[1]/a == í•™ê³¼ëª…, href == link
    """
    depts_found: List[Dict] = []
    logger.debug(f"[{college.name}] ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ HTML ë‚´ìš©ìœ¼ë¡œ í•™ê³¼ íŒŒì‹± ì‹œì‘.")

    # FIXME: ì•„ë˜ ì„ íƒìë“¤ì€ `grad.cnu.ac.kr` í˜ì´ì§€ì˜ ì‹¤ì œ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë§¤ìš° ì •êµí•˜ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ì œê³µëœ XPathë¥¼ ê¸°ë°˜ìœ¼ë¡œ CSS ì„ íƒìë¡œ ë³€í™˜í•˜ë ¤ëŠ” ì‹œë„ì´ë©°, ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # 1. í˜ì´ì§€ë¥¼ "ëŒ€í•™ëª…" (h4)ì„ ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜í™”í•©ë‹ˆë‹¤.
    # 2. í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ `college.name`ê³¼ ì¼ì¹˜í•˜ê±°ë‚˜ ìœ ì‚¬í•œ `h4` í…ìŠ¤íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    # 3. í•´ë‹¹ `h4`ì™€ ê´€ë ¨ëœ `ul > li > a` êµ¬ì¡°ì—ì„œ í•™ê³¼ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    # ì´ ì‘ì—…ì€ BeautifulSoupë§Œìœ¼ë¡œ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. lxmlê³¼ XPath ì‚¬ìš©ì´ ë” ì í•©í•  ìˆ˜ ìˆìœ¼ë‚˜,
    # í˜„ì¬ `html_select`ëŠ” BeautifulSoupì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì•„ë˜ëŠ” ë§¤ìš° ë‹¨ìˆœí™”ëœ ì ‘ê·¼ì´ë©°, ì‹¤ì œë¡œëŠ” `h4`ì™€ `ul`ì˜ ê´€ê³„ë¥¼ ëª…í™•íˆ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.

    # ì˜ˆì‹œ: ëª¨ë“  <div class="department_list02"> (ëŒ€í•™ êµ¬ë¶„ ì»¨í…Œì´ë„ˆë¡œ ê°€ì •)ë¥¼ ì°¾ê³ ,
    # ê·¸ ì•ˆì—ì„œ <h4> (ëŒ€í•™ëª…)ì™€ <ul><li><a> (í•™ê³¼)ë¥¼ ì—°ê²°.
    # college_section_selector = "div.department_list02" # ì‹¤ì œ ì»¨í…Œì´ë„ˆ ì„ íƒìë¡œ ë³€ê²½
    # sections_html = html_select_elements_as_html(html_content, college_section_selector) # ì´ëŸ° í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •

    # í˜„ì¬ college.nameì€ "ì¸ë¬¸ëŒ€í•™(ì¼ë°˜ëŒ€í•™ì›ì†Œì†)" ê³¼ ê°™ì€ í˜•íƒœì¼ ìˆ˜ ìˆìŒ
    # ì—¬ê¸°ì„œ (ì¼ë°˜ëŒ€í•™ì›ì†Œì†) ë¶€ë¶„ì„ ì œê±°í•˜ê³  ìˆœìˆ˜ ëŒ€í•™ëª…ìœ¼ë¡œ ë¹„êµí•´ì•¼ í•  ìˆ˜ ìˆìŒ
    target_college_name_pure = college.name.split('(')[0]  # ì˜ˆ: "ì¸ë¬¸ëŒ€í•™"

    # ë§¤ìš° ë‹¨ìˆœí•œ ì ‘ê·¼: í˜ì´ì§€ ì „ì²´ì—ì„œ ul > li > a ë¥¼ ì°¾ê³ , ë¶€ëª¨ êµ¬ì¡°ë¥¼ í†µí•´ ëŒ€í•™ëª…ê³¼ ì—°ê²° ì‹œë„ (í•œê³„ ëª…í™•)
    # ë” ë‚˜ì€ ë°©ë²•: ê° ëŒ€í•™ëª…ì„ ê°ì‹¸ëŠ” divë¥¼ ì°¾ê³ , ê·¸ div ë‚´ì˜ ul > li > a ë¥¼ ì°¾ì•„ì•¼ í•¨.
    # ì˜ˆì‹œ: í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ì—¬ (h4_text, ul_of_depts_html) ìŒì„ ë§Œë“¦.
    #       ê·¸ í›„ h4_textê°€ target_college_name_pureì™€ ì¼ì¹˜í•˜ëŠ” ul_of_depts_htmlì—ì„œ í•™ê³¼ ì¶”ì¶œ.
    # ì´ ë¶€ë¶„ì€ ìƒì„¸í•œ HTML êµ¬ì¡° ë¶„ì„ ì—†ì´ëŠ” ì •í™•í•œ êµ¬í˜„ì´ ì–´ë µìŠµë‹ˆë‹¤.

    # ì—¬ê¸°ì„œëŠ” "ëª¨ë“  í•™ê³¼ ë§í¬"ë¥¼ ê°€ì ¸ì˜¨ í›„, ì´ë“¤ì´ í˜„ì¬ College ê°ì²´ì— ì†í•œë‹¤ê³  ê°€ì •í•˜ëŠ”
    # ë§¤ìš° ë‹¨ìˆœí™”ëœ ì ‘ê·¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (spiders/colleges.pyì˜ discover_grad_page_colleges_and_depts ì—ì„œë„ ìœ ì‚¬í•œ ë¬¸ì œ)
    # ì´ ë°©ì‹ì€ college.nameê³¼ ì‹¤ì œ HTMLì˜ h4 ëŒ€í•™ëª…ì„ ë§¤ì¹­í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì •í™•ë„ê°€ ë–¨ì–´ì§‘ë‹ˆë‹¤.

    # FIXME: ì•„ë˜ ì„ íƒìëŠ” /html/body/div[1]/div[3]/div[2]/div[3]/div/div/div[*]/ul/li/a ì™€ ìœ ì‚¬í•œ ëª¨ë“  í•™ê³¼ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ í•´ì•¼ í•¨
    #        ê·¸ë¦¬ê³  ê°€ì ¸ì˜¨ í•™ê³¼ê°€ í˜„ì¬ `college.name`ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ í•„ìš”.
    #        ê°€ì¥ ì¢‹ì€ ê²ƒì€, `college.name`ê³¼ ì¼ì¹˜í•˜ëŠ” `h4`ë¥¼ ë¨¼ì € ì°¾ê³ , ê·¸ `h4`ì˜ í˜•ì œ ë˜ëŠ” ìì‹ ìš”ì†Œì¸ `ul`ì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.
    department_links_selector = "//div[@class='department_box']//ul/li/a"  # ì´ì „ ë‹µë³€ì˜ ì˜ˆì‹œ ì„ íƒì

    hrefs = html_select(html_content, department_links_selector, attr="href")
    names = html_select(html_content, department_links_selector)

    if hrefs and names and len(hrefs) == len(names):
        logger.info(f"[{college.name}] ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ì—ì„œ ì„ íƒì '{department_links_selector}'ë¡œ {len(hrefs)}ê°œ í•™ê³¼ í›„ë³´ ë°œê²¬.")
        for nm_raw, href_val in zip(names, hrefs):
            nm_cleaned = clean_text(nm_raw)
            if not nm_cleaned or not href_val: continue

            full_url = urljoin(college.url, href_val)  # college.urlì€ grad.cnu.ac.kr/...

            dept_code = _generate_department_code(college.code, nm_cleaned, full_url)

            # ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ì€ ì´ í•™ê³¼ í˜ì´ì§€(full_url)ë¥¼ ë°©ë¬¸í•˜ì—¬ ì°¾ì•„ì•¼ í•¨
            # ì„ì‹œë¡œ ë¹„ì›Œë‘ê±°ë‚˜, ì¼ë°˜ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì¶”ë¡  ì‹œë„
            # ì˜ˆ: undergrad_tpl = _extract_notice_url_template_from_page(await fetch_text(full_url), full_url, ["ê³µì§€", "í•™ë¶€"])
            #     grad_tpl = _extract_notice_url_template_from_page(await fetch_text(full_url), full_url, ["ê³µì§€", "ëŒ€í•™ì›"])

            depts_found.append({
                "code": dept_code, "name": nm_cleaned, "url": full_url,
                "dept_type": "grad_school_dept",  # Department ëª¨ë¸ì— ì •ì˜ëœ íƒ€ì…
                # "undergrad_notice_url_template": undergrad_tpl, # ì‹¤ì œë¡œëŠ” ëŒ€í•™ì› ê³µì§€ê°€ ë©”ì¸ì¼ ê²ƒ
                # "grad_notice_url_template": grad_tpl,
            })
    else:
        logger.warning(f"[{college.name}] ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ì—ì„œ í•™ê³¼ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ì„ íƒì: '{department_links_selector}').")

    return depts_found


async def _parse_departments_from_normal_college(college: College, html_content: str) -> List[Dict]:
    """ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ í˜ì´ì§€ì—ì„œ í•™ê³¼ ëª©ë¡ì„ íŒŒì‹±í•©ë‹ˆë‹¤. (ì´ì „ ë‹µë³€ì˜ HTML Fallback ë¡œì§ ê°œì„ )"""
    depts_found: List[Dict] = []
    logger.debug(f"[{college.name}] ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ HTML ë‚´ìš©ìœ¼ë¡œ í•™ê³¼ íŒŒì‹± ì‹œì‘.")

    selectors_to_try = [
        "a[href*='department']", "a[href*='dept']", "a[href*='major']",
        "a[href*='í•™ë¶€']", "a[href*='í•™ê³¼']",
        "nav a", "div[class*='nav'] a", "div[id*='nav'] a",
        "ul[class*='menu'] li a", "ul[id*='menu'] li a",
        "div.menu_area ul li a", "div.snb_wrap ul li a",
        "div[class*='dept_list'] a", "ul[class*='dept_list'] li a",
        # ë‹¤ìŒì€ ë§¤ìš° ì¼ë°˜ì ì´ë¯€ë¡œ ì£¼ì˜í•´ì„œ ì‚¬ìš©í•˜ê³ , ê²€ì¦ ë¡œì§ì´ ì¤‘ìš”
        # "ul li a",
    ]

    # íœ´ë¦¬ìŠ¤í‹±: í•œ ì„ íƒìë¡œ ì°¾ì€ í•™ê³¼ ìˆ˜ê°€ ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ë¶€ì ì ˆí•˜ë‹¤ê³  íŒë‹¨
    MIN_EXPECTED_DEPTS = 1  # ìµœì†Œ 1ê°œëŠ” ìˆì–´ì•¼ í•¨ (ë‹¨ì¼ í•™ë¶€ ëŒ€í•™ë„ ìˆìœ¼ë¯€ë¡œ)
    MAX_EXPECTED_DEPTS = 40  # ë§¤ìš° í° ë‹¨ê³¼ëŒ€í•™ ê³ ë ¤

    temp_html_depts_candidates = {}  # URLì„ í‚¤ë¡œ í•˜ì—¬ ì¤‘ë³µ ë°©ì§€

    for selector_idx, current_selector in enumerate(selectors_to_try):
        logger.debug(f"[{college.name}] HTML ì„ íƒì ì‹œë„ ({selector_idx + 1}/{len(selectors_to_try)}): '{current_selector}'")
        hrefs = html_select(html_content, current_selector, attr="href")
        names = html_select(html_content, current_selector)

        if hrefs and names and len(hrefs) == len(names):
            current_selector_valid_depts_count = 0
            for i in range(len(hrefs)):
                nm_cleaned = clean_text(names[i])
                href_val = hrefs[i]

                if not nm_cleaned or not href_val or len(nm_cleaned) < 2 or len(nm_cleaned) > 50:
                    continue
                if any(ex_kw in nm_cleaned for ex_kw in EXCLUDE_KEYWORDS_IN_NAME):
                    continue
                # ì´ë¦„ì— í•™ê³¼ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ì•¼ í•¨
                if not any(kw.lower() in nm_cleaned.lower() for kw in DEPT_KEYWORDS):
                    continue

                full_url = urljoin(college.url, href_val)
                # ì™¸ë¶€ ë§í¬ë‚˜ í˜„ì¬ ëŒ€í•™ URLê³¼ ë„ˆë¬´ ë‹¤ë¥¸ ìƒìœ„ ê²½ë¡œ ì œì™¸ (ê°„ë‹¨í•œ ì²´í¬)
                if not full_url.startswith(college.url.rsplit('/', 1)[0]):  # í˜„ì¬ college URLì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì‹œì‘í•˜ëŠ”ì§€
                    if not full_url.startswith("http") or urlparse(full_url).netloc != urlparse(college.url).netloc:
                        continue  # ì™„ì „ ë‹¤ë¥¸ ë„ë©”ì¸ì´ê±°ë‚˜ í”„ë¡œí† ì½œ ì—†ëŠ” ì´ìƒí•œ ë§í¬

                dept_code = _generate_department_code(college.code, nm_cleaned, full_url)

                if full_url not in temp_html_depts_candidates:  # ì¤‘ë³µ URL ë°©ì§€
                    temp_html_depts_candidates[full_url] = {
                        "code": dept_code, "name": nm_cleaned, "url": full_url,
                        "dept_type": "normal_dept",  #
                        "selector_used": current_selector  # ì–´ë–¤ ì„ íƒìë¡œ ì°¾ì•˜ëŠ”ì§€ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
                    }
                    current_selector_valid_depts_count += 1

            # í˜„ì¬ ì„ íƒìë¡œ ì°¾ì€ ìœ íš¨ í•™ê³¼ ìˆ˜ê°€ ë²”ìœ„ ë‚´ì— ìˆê³ , ì´ì „ë³´ë‹¤ ë§ì´ ì°¾ì•˜ë‹¤ë©´ ì´ ê²°ê³¼ë¥¼ ìš°ì„  ì‚¬ìš©
            if MIN_EXPECTED_DEPTS <= current_selector_valid_depts_count <= MAX_EXPECTED_DEPTS:
                if current_selector_valid_depts_count > len(depts_found):  # ë” ë§ì€ (ì ì ˆí•œ ìˆ˜ì˜) í•™ê³¼ë¥¼ ì°¾ì€ ì„ íƒì ì‚¬ìš©
                    logger.info(
                        f"[{college.name}] ì„ íƒì '{current_selector}'ë¡œ {current_selector_valid_depts_count}ê°œì˜ ìœ íš¨ í•™ê³¼ ì •ë³´ ë°œê²¬. ì´ ê²°ê³¼ ì‚¬ìš©.")
                    depts_found = [v for k, v in temp_html_depts_candidates.items() if
                                   v["selector_used"] == current_selector]
                    # ì´ ì„ íƒìê°€ ì„±ê³µì ì´ë¼ê³  íŒë‹¨ë˜ë©´ ë” ì´ìƒ ë‹¤ë¥¸ ì„ íƒì ì‹œë„ ì•ˆ í•¨ (ì„ íƒì‚¬í•­)
                    # break

    # ìµœì¢…ì ìœ¼ë¡œ depts_foundê°€ ë¹„ì–´ìˆë‹¤ë©´, temp_html_depts_candidates ì¤‘ ê°€ì¥ ë§ì€ ê²ƒì„ ì‚¬ìš©í•˜ê±°ë‚˜, ëª¨ë‘ ì‚¬ìš©
    if not depts_found and temp_html_depts_candidates:
        logger.warning(f"[{college.name}] ì£¼ìš” ì„ íƒìì—ì„œ ê¸°ëŒ€ ë²”ìœ„ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í•¨. ëª¨ë“  í›„ë³´({len(temp_html_depts_candidates)})ë¥¼ ì„ì‹œ ì‚¬ìš©.")
        depts_found = list(temp_html_depts_candidates.values())
        # ì—¬ê¸°ì„œë„ ë„ˆë¬´ ë§ê±°ë‚˜ ì ìœ¼ë©´ í•„í„°ë§ í•„ìš”
        if len(depts_found) > MAX_EXPECTED_DEPTS:
            logger.warning(f"[{college.name}] í›„ë³´ê°€ ë„ˆë¬´ ë§ì•„ ({len(depts_found)}) ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ì¶”ê°€ í•„í„°ë§ í•„ìš”. (í˜„ì¬ëŠ” ëª¨ë‘ ì‚¬ìš©)")
            # depts_found = depts_found[:MAX_EXPECTED_DEPTS] # ì˜ˆì‹œ: ìµœëŒ€ ê°œìˆ˜ ì œí•œ

    # `selector_used` í•„ë“œëŠ” DB ì €ì¥ ì „ì— ì œê±°
    return [{k: v for k, v in dept.items() if k != "selector_used"} for dept in depts_found]


async def _create_dept_for_plus_grad_school(college: College) -> List[Dict]:
    """plus.cnu.ac.krì—ì„œ ê°€ì ¸ì˜¨ íŠ¹ìˆ˜/ì „ë¬¸ëŒ€í•™ì›ì˜ ê²½ìš°, í•´ë‹¹ College ìì²´ë¥¼ Departmentë¡œ ì·¨ê¸‰í•˜ê±°ë‚˜,
       í˜ì´ì§€ ë‚´ì—ì„œ ê³µì§€ì‚¬í•­ ë§í¬ë¥¼ ì°¾ì•„ Departmentì˜ URL í…œí”Œë¦¿ì— ì„¤ì •í•©ë‹ˆë‹¤."""
    depts_found: List[Dict] = []
    logger.info(f"[{college.name}] Plus íŠ¹ìˆ˜/ì „ë¬¸ëŒ€í•™ì› ìì²´ë¥¼ í•™ê³¼ë¡œ ì²˜ë¦¬ ë˜ëŠ” ê³µì§€ ë§í¬ íƒìƒ‰.")

    # í•´ë‹¹ college.url í˜ì´ì§€ì—ì„œ "ê³µì§€ì‚¬í•­", "notice" ë“±ì˜ ë§í¬ë¥¼ ì°¾ì•„ URL í…œí”Œë¦¿ìœ¼ë¡œ ì„¤ì • ì‹œë„
    undergrad_tpl = None
    grad_tpl = None  # íŠ¹ìˆ˜/ì „ë¬¸ëŒ€í•™ì›ì€ ë³´í†µ í•™ë¶€/ëŒ€í•™ì› êµ¬ë¶„ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë‹¨ì¼
    try:
        html_content = await fetch_text(college.url)
        undergrad_tpl = _extract_notice_url_template_from_page(html_content, college.url, ["ê³µì§€ì‚¬í•­", "notice", "ê³µì§€"])
        # ëŒ€í•™ì› ê³µì§€ê°€ ë³„ë„ë¡œ ìˆë‹¤ë©´ í‚¤ì›Œë“œ ì¶”ê°€
        # grad_tpl = _extract_notice_url_template_from_page(html_content, college.url, ["ëŒ€í•™ì›ê³µì§€", "gradnotice"])
        if undergrad_tpl:  # ì°¾ì•˜ë‹¤ë©´, ë³´í†µ ëŒ€í•™ì›ë„ ê°™ì€ íŒ¨í„´ì¼ ìˆ˜ ìˆìŒ
            grad_tpl = undergrad_tpl
            logger.info(f"[{college.name}] ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ ì¶”ë¡ : {undergrad_tpl}")

    except Exception as e:
        logger.error(f"[{college.name}] Plus ëŒ€í•™ì› í˜ì´ì§€({college.url})ì—ì„œ ê³µì§€ì‚¬í•­ ë§í¬ íƒìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

    depts_found.append({
        "code": college.code + "_main_dept",  # College ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Department ì½”ë“œ ìƒì„±
        "name": college.name,  # College ì´ë¦„ì„ Department ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        "url": college.url,  # Department URLë„ College URLê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
        "dept_type": "plus_special_grad_dept",  #
        "undergrad_notice_url_template": undergrad_tpl,
        "grad_notice_url_template": grad_tpl,
        "academic_notice_url_template": undergrad_tpl,  # í•™ì‚¬ê³µì§€ë„ ì¼ë‹¨ ë™ì¼í•˜ê²Œ ì„¤ì • (ì¶”í›„ í™•ì¸)
    })
    return depts_found


async def crawl_departments(college: College):
    logger.info(f"ğŸ« [{college.name} (Type: {college.college_type})] í•™ê³¼/í•™ë¶€ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
    depts_to_save: List[Dict] = []

    # --- ì¸ê³µì§€ëŠ¥í•™ê³¼ëŠ” í•˜ë“œì½”ë”©ëœ ì •ë³´ë¡œ ì²˜ë¦¬ ---
    # ì¸ê³µì§€ëŠ¥í•™ê³¼ëŠ” ì–´ë–¤ Collegeì— ì†í•˜ëŠ”ì§€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
    # ì—¬ê¸°ì„œëŠ” íŠ¹ì • College(ì˜ˆ: 'normal_college' íƒ€ì…ì˜ 'ìì—°ê³¼í•™ëŒ€í•™' ë“±)ì¼ ë•Œë§Œ ì¶”ê°€í•˜ê±°ë‚˜,
    # ì•„ë‹ˆë©´ Collegeì™€ ë¬´ê´€í•˜ê²Œ í•œ ë²ˆë§Œ ì¶”ê°€ë˜ë„ë¡ ë³„ë„ ê´€ë¦¬ í•„ìš”.
    # ì—¬ê¸°ì„œëŠ” College ë£¨í”„ ë‚´ì— ìˆìœ¼ë¯€ë¡œ, íŠ¹ì • Collegeì™€ ì—°ê²°í•˜ê±°ë‚˜, í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ í”Œë˜ê·¸ ê´€ë¦¬ í•„ìš”.
    # ì§€ê¸ˆì€ AI í•™ê³¼ê°€ íŠ¹ì • College ì†Œì†ìœ¼ë¡œ DBì— ë“¤ì–´ê°€ì§€ ì•ŠëŠ”ë‹¤ê³  ê°€ì •í•˜ê³ ,
    # ë§Œì•½ AIí•™ê³¼ë¥¼ ìœ„í•œ College ê°ì²´ê°€ ìˆë‹¤ë©´ ê·¸ ë•Œ ì²˜ë¦¬í•˜ë„ë¡ í•¨.
    # ë˜ëŠ”, `spiders/colleges.py`ì—ì„œ AIí•™ê³¼ë¥¼ ìœ„í•œ ê°€ìƒì˜ Collegeë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆìŒ.
    # ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `scheduler.py`ì—ì„œ `crawl_departments` ë£¨í”„ ì „ì— ë³„ë„ë¡œ AI í•™ê³¼ë¥¼ DBì— ì¶”ê°€í•˜ëŠ” ê²ƒ.
    # ì—¬ê¸°ì„œëŠ” `college.code == "ai_college_placeholder"` ì™€ ê°™ì€ ê°€ìƒì˜ ì¡°ê±´ìœ¼ë¡œ ì¶”ê°€.
    if college.code == "AI_COLLEGE_CODE":  # ì´ ì½”ë“œëŠ” colleges.pyì—ì„œ AI ëŒ€í•™ì„ ìœ„í•´ ìƒì„±í•œ ì½”ë“œì—¬ì•¼ í•¨
        logger.info("ì¸ê³µì§€ëŠ¥í•™ê³¼(í•˜ë“œì½”ë”©) ì •ë³´ë¥¼ Departmentë¡œ ì¶”ê°€ ì‹œë„...")
        # ì‹¤ì œ ê³µì§€ì‚¬í•­ URL í…œí”Œë¦¿ í™•ì¸ í•„ìš”
        ai_undergrad_tpl = "https://ai.cnu.ac.kr/ai/community/notice.do?mode=list&page={}"  # ì˜ˆì‹œ
        ai_academic_tpl = "https://ai.cnu.ac.kr/ai/community/undergraduate_course_notice.do?mode=list&page={}"  # ì˜ˆì‹œ

        depts_to_save.append({
            "college_id": college.id,  # ì´ Collegeê°€ AI ëŒ€í•™ì„ ë‚˜íƒ€ë‚´ì•¼ í•¨
            "code": "cnu_ai_dept",
            "name": "ì¸ê³µì§€ëŠ¥í•™ê³¼",
            "url": "https://ai.cnu.ac.kr/ai/index.do",
            "dept_type": "ai_hardcoded",  #
            "undergrad_notice_url_template": ai_undergrad_tpl,
            "academic_notice_url_template": ai_academic_tpl,
            "grad_notice_url_template": None  # ëŒ€í•™ì› ê³µì§€ ë³„ë„ í™•ì¸
        })

    # --- College íƒ€ì…ì— ë”°ë¥¸ ë¶„ê¸° ---
    if college.college_type == "grad_page_college" or \
            (college.college_type == "plus_general_grad" and "grad.cnu.ac.kr" in college.url):
        # ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ (grad.cnu.ac.kr) ì—ì„œ í•™ê³¼ íŒŒì‹±
        try:
            html_content = await fetch_text(college.url)
            parsed_depts = await _parse_departments_from_grad_page(college, html_content)
            depts_to_save.extend(parsed_depts)
        except Exception as e:
            logger.error(f"[{college.name}] ì¼ë°˜ëŒ€í•™ì› í˜ì´ì§€ í•™ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")

    elif college.college_type == "normal_college":
        # ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ í˜ì´ì§€ì—ì„œ í•™ê³¼ íŒŒì‹± (ê¸°ì¡´ HTML Fallback ë°©ì‹ ê°œì„ )
        try:
            # ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ì€ JSON API ì‹œë„ë¥¼ ë¨¼ì € í•  ìˆ˜ë„ ìˆìŒ (í˜„ì¬ëŠ” HTMLë§Œ ê°€ì •)
            # api_url = f"{college.url.rstrip('/')}/department/list.json"
            # try: json_data = await fetch_json(api_url) ...
            # except: html_fallback ...
            html_content = await fetch_text(college.url)
            parsed_depts = await _parse_departments_from_normal_college(college, html_content)
            depts_to_save.extend(parsed_depts)
        except Exception as e:
            logger.error(f"[{college.name}] ì¼ë°˜ ë‹¨ê³¼ëŒ€í•™ í•™ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")

    elif college.college_type == "plus_special_grad" or college.college_type == "plus_general_grad":
        # plus.cnu.ac.kr ì—ì„œ ê°€ì ¸ì˜¨ íŠ¹ìˆ˜/ì „ë¬¸ëŒ€í•™ì› ë˜ëŠ” ì¼ë°˜ëŒ€í•™ì› ë§í¬ì˜ ê²½ìš°
        # í•´ë‹¹ College ìì²´ë¥¼ í•˜ë‚˜ì˜ Departmentë¡œ ì·¨ê¸‰í•˜ê±°ë‚˜,
        # í•´ë‹¹ í˜ì´ì§€ ë‚´ì—ì„œ ê³µì§€ì‚¬í•­ ë§í¬ë¥¼ ì°¾ì•„ URL í…œí”Œë¦¿ìœ¼ë¡œ ì„¤ì •
        try:
            parsed_depts = await _create_dept_for_plus_grad_school(college)
            depts_to_save.extend(parsed_depts)
        except Exception as e:
            logger.error(f"[{college.name}] Plus ëŒ€í•™ì› ê¸°ë°˜ í•™ê³¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")

    else:
        logger.warning(f"[{college.name}] ì•Œ ìˆ˜ ì—†ëŠ” college_type ('{college.college_type}')ìœ¼ë¡œ í•™ê³¼ ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # --- ìµœì¢… DB ì €ì¥ ---
    if not depts_to_save:
        logger.warning(f"[{college.name}] ìµœì¢…ì ìœ¼ë¡œ DBì— ì €ì¥í•  í•™ê³¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # DB ì €ì¥ ì „ college_id ì„¤ì • ë° ì¤‘ë³µ ì œê±°
    final_unique_depts_for_db = []
    seen_codes_in_college_for_db = set()
    for dept_info_dict in depts_to_save:
        dept_info_dict["college_id"] = college.id  # í˜„ì¬ Collegeì˜ IDë¥¼ í• ë‹¹
        if dept_info_dict["code"] not in seen_codes_in_college_for_db:
            final_unique_depts_for_db.append(dept_info_dict)
            seen_codes_in_college_for_db.add(dept_info_dict["code"])

    if len(final_unique_depts_for_db) != len(depts_to_save):
        logger.info(f"[{college.name}] DB ì €ì¥ ì „ ì¤‘ë³µ í•™ê³¼ ì½”ë“œ ì œê±°ë¨: {len(depts_to_save)} -> {len(final_unique_depts_for_db)}ê°œ")

    with get_session() as sess:
        added_count = 0
        updated_count = 0
        for d_item_db in final_unique_depts_for_db:
            # DB ì €ì¥ì„ ìœ„í•´ ëª¨ë¸ í•„ë“œì— ë§ê²Œ ë°ì´í„° ì¤€ë¹„
            db_ready_dept_data = {
                "college_id": d_item_db["college_id"],
                "code": d_item_db["code"],
                "name": d_item_db["name"],
                "url": d_item_db["url"],
                "dept_type": d_item_db.get("dept_type", "unknown"),
                "academic_notice_url_template": d_item_db.get("academic_notice_url_template"),
                "undergrad_notice_url_template": d_item_db.get("undergrad_notice_url_template"),
                "grad_notice_url_template": d_item_db.get("grad_notice_url_template"),
                "specific_grad_keyword_notice_url": d_item_db.get("specific_grad_keyword_notice_url")
            }
            obj = (sess.query(Department)
                   .filter_by(college_id=db_ready_dept_data["college_id"],
                              code=db_ready_dept_data["code"]).one_or_none())
            if obj:
                changed = False
                for key, value in db_ready_dept_data.items():
                    if hasattr(obj, key) and getattr(obj, key) != value:
                        setattr(obj, key, value)
                        changed = True
                if changed:
                    updated_count += 1
                    logger.trace(
                        f"[{college.name}] ê¸°ì¡´ í•™ê³¼ ì •ë³´ ì—…ë°ì´íŠ¸: code='{db_ready_dept_data['code']}', name='{db_ready_dept_data['name']}'")

            else:
                obj = Department(**db_ready_dept_data)
                sess.add(obj)
                added_count += 1
                logger.trace(
                    f"[{college.name}] ìƒˆ í•™ê³¼ ì •ë³´ ì¶”ê°€: code='{db_ready_dept_data['code']}', name='{db_ready_dept_data['name']}'")

        if added_count > 0 or updated_count > 0:
            try:
                sess.commit()
                logger.success(f"[{college.name}] í•™ê³¼ ì •ë³´ DB ìµœì¢… ì—…ë°ì´íŠ¸: {added_count}ê°œ ì¶”ê°€, {updated_count}ê°œ ìˆ˜ì •.")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{college.name}] í•™ê³¼ ì •ë³´ DB ìµœì¢… ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")
                sess.rollback()
        else:
            logger.info(f"[{college.name}] DBì— ë³€ê²½ëœ í•™ê³¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ (ì²˜ë¦¬ëœ ê³ ìœ  í•™ê³¼ ìˆ˜: {len(final_unique_depts_for_db)}).")