# cnu_crawler/spiders/departments.py
from loguru import logger
import json  # aiohttpëŠ” ì´ë¯¸ JSONDecodeErrorë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ëª…ì‹œì  import
from aiohttp import ClientError  # fetcherì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜ˆì™¸

from cnu_crawler.core.fetcher import fetch_json, fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import College, Department, get_session


async def crawl_departments(college: College):
    logger.info(f"ğŸ« [{college.name}] í•™ë¶€/í•™ê³¼ í¬ë¡¤ë§ ì‹œì‘")
    dept_list_data = []

    # â‘  JSON API ì‹œë„
    # FIXME: ì‹¤ì œ API ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”. '/departmentList.do', '/getDeptList.json' ë“± ë‹¤ì–‘í•  ìˆ˜ ìˆìŒ.
    api_url = f"{college.url.rstrip('/')}/department/list.json"  #

    try:
        logger.debug(f"JSON API ì‹œë„: {api_url}")
        data = await fetch_json(api_url)  #

        # FIXME: ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ì•„ë˜ í‚¤ë“¤ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì˜ˆ: dataê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼ dict ì•ˆì— ìˆë‹¤ë©´ data = data.get('departments', [])
        if not isinstance(data, list):
            logger.warning(f"[{college.name}] JSON API ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. Fallback ì‹œë„. ë°ì´í„°: {str(data)[:200]}")
            raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤.")  # Fallback ë¡œì§ìœ¼ë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•¨

        for d_item in data:
            # FIXME: 'deptCd', 'deptNm', 'url' í‚¤ê°€ ì‹¤ì œ API ì‘ë‹µê³¼ ë‹¤ë¥¼ ê²½ìš° ìˆ˜ì • í•„ìš”.
            code = d_item.get("deptCd")  #
            name = d_item.get("deptNm")  #
            url = d_item.get("url")  #

            if not all([code, name, url]):
                logger.warning(f"[{college.name}] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´(code, name, url)ê°€ ëˆ„ë½: {d_item}")
                continue
            dept_list_data.append({"code": str(code), "name": str(name), "url": str(url)})
        logger.info(f"[{college.name}] JSON APIë¥¼ í†µí•´ {len(dept_list_data)}ê°œ í•™ê³¼ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ.")

    except (
    ClientError, json.JSONDecodeError, ValueError, TypeError, Exception) as e:  # TypeError ì¶”ê°€ (dataê°€ Noneì¼ ê²½ìš° ë“±)
        logger.warning(f"[{college.name}] JSON API í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨ ({api_url}): {e}. HTML Fallback ì‹œë„.")

        # â‘¡ ì •ì  HTML fallback
        try:
            # college.urlì´ ì‹¤ì œ í•™ê³¼ ëª©ë¡ì´ ìˆëŠ” í˜ì´ì§€ì¸ì§€ í™•ì¸ í•„ìš”
            html_page_url = college.url  # ì´ URLì´ í•™ê³¼ ëª©ë¡ì„ í¬í•¨í•´ì•¼ í•¨
            logger.debug(f"HTML Fallback ì‹œë„: {html_page_url}")
            html = await fetch_text(html_page_url)  #

            # FIXME: ì•„ë˜ CSS ì„ íƒìë“¤ì€ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ ì‹œ ë°˜ë“œì‹œ í•¨ê»˜ ìˆ˜ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            # ì„ íƒìëŠ” ìµœëŒ€í•œ êµ¬ì²´ì ì´ë©´ì„œë„ ê¹¨ì§€ê¸° ì‰½ì§€ ì•Šê²Œ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
            # ì˜ˆ: í•™ê³¼ ë§í¬ ì„ íƒì: 'div.department_list > ul > li > a'
            # ì˜ˆ: í•™ê³¼ëª… ì„ íƒì: 'div.department_list > ul > li > a > span.name' (ë§Œì•½ ì´ë¦„ì´ span ì•ˆì— ìˆë‹¤ë©´)

            # í˜„ì¬ ë¡œì§ì€ ë§í¬ì™€ ì´ë¦„ì„ ë³„ë„ë¡œ ê°€ì ¸ì˜¤ëŠ”ë°, ì´ëŠ” ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # í•˜ë‚˜ì˜ ë°˜ë³µ ë‹¨ìœ„(ì˜ˆ: ê° í•™ê³¼ë¥¼ ê°ì‹¸ëŠ” div)ë¥¼ ë¨¼ì € ì°¾ê³ , ê·¸ ì•ˆì—ì„œ ë§í¬ì™€ ì´ë¦„ì„ ì°¾ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.
            # department_container_selector = "div.dept_item_selector" # ì˜ˆì‹œ
            # containers = some_new_html_select_containers(html, department_container_selector)
            # for container_html in containers:
            #    href = html_first(container_html, "a.dept_link_selector", attr="href")
            #    name = html_first(container_html, "span.dept_name_selector")

            # í˜„ì¬ ì½”ë“œ ê¸°ë°˜ ìˆ˜ì •:
            # í•™ê³¼ ë§í¬ ì„ íƒì (hrefì— 'department' ë˜ëŠ” ìœ ì‚¬í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ <a> íƒœê·¸)
            dept_link_selector = "a[href*='department']"  #
            # í•™ê³¼ ì´ë¦„ ì„ íƒì (ìœ„ì™€ ë™ì¼í•œ <a> íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸)
            dept_name_selector = "a[href*='department']"  #

            hrefs = html_select(html, dept_link_selector, attr="href")  #
            names = html_select(html, dept_name_selector)  #

            if not hrefs or not names:
                logger.warning(f"[{college.name}] HTMLì—ì„œ í•™ê³¼ ë§í¬ë‚˜ ì´ë¦„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì„ íƒì: '{dept_link_selector}')")
            elif len(hrefs) != len(names):
                logger.warning(f"[{college.name}] ì¶”ì¶œëœ í•™ê³¼ ë§í¬({len(hrefs)}ê°œ)ì™€ ì´ë¦„({len(names)}ê°œ)ì˜ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤. HTML êµ¬ì¡° í™•ì¸ í•„ìš”.")
            else:
                for nm, href_val in zip(names, hrefs):
                    # FIXME: URLë¡œë¶€í„° í•™ê³¼ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ì‹. URL êµ¬ì¡° ë³€ê²½ ì‹œ ìˆ˜ì • í•„ìš”.
                    # href_val.split("/")[-2]ëŠ” URLì´ /path/to/dept_code/ í˜•íƒœì¼ ë•Œ ìœ íš¨.
                    #  ë” ì•ˆì •ì ì¸ ë°©ë²•ì€ ì •ê·œí‘œí˜„ì‹ì´ë‚˜ URL query parameter ì‚¬ìš©ì¼ ìˆ˜ ìˆìŒ.
                    try:
                        # URL ì •ê·œí™” (ìƒëŒ€ ê²½ë¡œ -> ì ˆëŒ€ ê²½ë¡œ)
                        if not href_val.startswith(('http://', 'https://')):
                            from urllib.parse import urljoin
                            href_val = urljoin(html_page_url, href_val)

                        # ì½”ë“œ ì¶”ì¶œ ë¡œì§ ê°œì„  (ë” ë§ì€ ì¼€ì´ìŠ¤ ê³ ë ¤)
                        code_match = re.search(r'/department/(\w+)/', href_val) or \
                                     re.search(r'deptCd=(\w+)', href_val) or \
                                     re.search(r'/(\w+)/?$', href_val.rstrip('/'))  # /abc ë˜ëŠ” /abc/

                        if code_match:
                            dept_code = code_match.group(1)
                        else:
                            # Fallback: ê¸°ì¡´ ë°©ì‹ ë˜ëŠ” ê³ ìœ  ID ìƒì„± (ì˜ˆ: í•´ì‹œê°’)
                            dept_code = href_val.split("/")[-2] if len(href_val.split("/")) > 2 else href_val  #
                            logger.trace(f"[{college.name}] '{href_val}' ì—ì„œ ì½”ë“œ ì¶”ì¶œì— ì •ê·œì‹ ì‹¤íŒ¨. ê¸°ë³¸ ë¶„í•  ì‚¬ìš©: {dept_code}")

                        dept_list_data.append({"code": dept_code, "name": nm.strip(), "url": href_val})
                    except Exception as ex_parse:
                        logger.error(f"[{college.name}] í•™ê³¼ ì •ë³´ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ì´ë¦„: {nm}, ë§í¬: {href_val}): {ex_parse}")
                logger.info(f"[{college.name}] HTML Fallbackì„ í†µí•´ {len(dept_list_data)}ê°œ í•™ê³¼ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ.")

        except (ClientError, Exception) as e_html:
            logger.error(f"[{college.name}] HTML Fallback ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e_html}")
            # ì—¬ê¸°ì„œ ë¹„ì–´ìˆëŠ” dept_list_dataë¡œ DB ì—…ë°ì´íŠ¸ ë¡œì§ì´ ì‹¤í–‰ë  ìˆ˜ ìˆìŒ (ì˜ë„ëœ ë™ì‘ì¸ì§€ í™•ì¸)

    if not dept_list_data:
        logger.warning(f"[{college.name}] ìµœì¢…ì ìœ¼ë¡œ í•™ê³¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    with get_session() as sess:
        for d in dept_list_data:
            # college_idëŠ” í˜„ì¬ college ê°ì²´ì˜ idë¥¼ ì‚¬ìš©
            obj = (sess.query(Department)
                   .filter_by(college_id=college.id, code=d["code"]).one_or_none())  #
            if obj:
                obj.name, obj.url = d["name"], d["url"]  #
            else:
                obj = Department(college_id=college.id, **d)  #
                sess.add(obj)  #
        try:
            sess.commit()  #
            logger.success(f"[{college.name}] ì´ {len(dept_list_data)}ê°œ í•™ê³¼ ì •ë³´ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        except Exception as e_db:
            logger.opt(exception=True).error(f"[{college.name}] í•™ê³¼ ì •ë³´ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")
            sess.rollback()