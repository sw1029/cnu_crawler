# src/cnu_crawler/spiders/departments.py
import re  # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ ì„í¬íŠ¸
import json
from urllib.parse import urljoin  # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜í•˜ê¸° ìœ„í•¨
from loguru import logger
from aiohttp import ClientError  # aiohttp ê´€ë ¨ ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ìœ„í•´

from cnu_crawler.core.fetcher import fetch_json, fetch_text
from cnu_crawler.core.parser import html_select
from cnu_crawler.storage import College, Department, get_session
from cnu_crawler.utils import clean_text

# í•™ê³¼ ì´ë¦„ì— í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” í‚¤ì›Œë“œ (ê²€ì¦ìš©)
DEPT_KEYWORDS = ["í•™ê³¼", "í•™ë¶€", "ì „ê³µ", "department", "school of", "major", "division"]
# í•™ê³¼ ì´ë¦„ì—ì„œ ì œì™¸í•  í‚¤ì›Œë“œ (ì˜ˆ: ëŒ€í•™ ìì²´ ë§í¬ ë°©ì§€)
EXCLUDE_KEYWORDS_IN_NAME = ["ëŒ€í•™ì•ˆë‚´", "ì…í•™ì•ˆë‚´", "ëŒ€í•™ìƒí™œ", "ì»¤ë®¤ë‹ˆí‹°", "ì˜¤ì‹œëŠ”ê¸¸", "ì‚¬ì´íŠ¸ë§µ"]


async def crawl_departments(college: College):
    logger.info(f"ğŸ« [{college.name}] í•™ë¶€/í•™ê³¼ í¬ë¡¤ë§ ì‹œì‘")
    dept_list_data_final = []  # ìµœì¢…ì ìœ¼ë¡œ í™•ì •ëœ í•™ê³¼ ì •ë³´

    # 1. JSON API ì‹œë„ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    api_url = f"{college.url.rstrip('/')}/department/list.json"
    json_api_succeeded = False
    try:
        logger.debug(f"JSON API ì‹œë„: {api_url}")
        data = await fetch_json(api_url)

        actual_data_list = []
        if isinstance(data, list):
            actual_data_list = data
        elif isinstance(data, dict) and "departments" in data and isinstance(data["departments"], list):  # ì˜ˆì‹œ í‚¤
            actual_data_list = data["departments"]
        # ë‹¤ë¥¸ ê°€ëŠ¥í•œ JSON êµ¬ì¡°ì— ëŒ€í•œ ì²˜ë¦¬ ì¶”ê°€ ê°€ëŠ¥
        # elif ...

        if not actual_data_list and isinstance(data, dict):  # ë§Œì•½ ë‹¤ë¥¸ í‚¤ì— ë°ì´í„°ê°€ ìˆì„ ê²½ìš°
            logger.warning(f"[{college.name}] JSON API ì‘ë‹µì´ ì§ì ‘ì ì¸ ë¦¬ìŠ¤íŠ¸ëŠ” ì•„ë‹ˆì§€ë§Œ dict í˜•íƒœì„. ë‹¤ë¥¸ í‚¤ í™•ì¸ ì‹œë„. ë°ì´í„°: {str(data)[:200]}")
            # ì—¬ê¸°ì„œ data dictë¥¼ íƒìƒ‰í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆì‹œë¡œ ë‚¨ê²¨ë‘ )

        if not actual_data_list and not isinstance(data, list):  # ìµœì¢…ì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ëª» ì°¾ìœ¼ë©´
            logger.warning(f"[{college.name}] JSON API ì‘ë‹µì´ ì˜ˆìƒí•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤ (URL: {api_url}). ë°ì´í„°: {str(data)[:200]}")
            raise ValueError("JSON API ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")

        temp_json_depts = []
        for d_item in actual_data_list:
            code = d_item.get("deptCd")
            name = d_item.get("deptNm")
            url = d_item.get("url")

            if not all([code, name, url]):
                logger.warning(f"[{college.name}] JSON í•­ëª©ì— í•„ìˆ˜ ì •ë³´(code, name, url)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {d_item}")
                continue

            full_url = urljoin(college.url, url)
            temp_json_depts.append({"code": str(code), "name": clean_text(str(name)), "url": full_url})

        if temp_json_depts:
            logger.info(f"[{college.name}] JSON APIë¥¼ í†µí•´ {len(temp_json_depts)}ê°œ í•™ê³¼ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ.")
            dept_list_data_final.extend(temp_json_depts)  # JSON ê²°ê³¼ë¥¼ ìµœì¢… ëª©ë¡ì— ì¶”ê°€
            json_api_succeeded = True  # JSON API ì„±ê³µ í”Œë˜ê·¸

    except (ClientError, json.JSONDecodeError, ValueError, TypeError, Exception) as e:
        logger.warning(
            f"[{college.name}] JSON API í˜¸ì¶œ/íŒŒì‹± ì‹¤íŒ¨ (URL: {api_url}): {type(e).__name__} - {e}. HTML Fallbackì„ ì‹œë„í•©ë‹ˆë‹¤.")
        # JSON API ì‹¤íŒ¨ ì‹œ dept_list_data_finalì€ ë¹„ì–´ìˆìŒ

    # 2. HTML Fallback ì‹œë„ (JSON API ì‹¤íŒ¨ ì‹œ ë˜ëŠ” JSON API ê²°ê³¼ê°€ ì—†ì—ˆì„ ê²½ìš°)
    if not json_api_succeeded:  # JSON APIê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜, ì„±ê³µí–ˆì–´ë„ ê²°ê³¼ê°€ ì—†ì—ˆì„ ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„  ì‹¤íŒ¨ ì‹œì—ë§Œìœ¼ë¡œ í•œì •)
        try:
            html_page_url_to_crawl = college.url
            logger.debug(f"HTML Fallback ì‹œë„: {html_page_url_to_crawl}")
            html_content = await fetch_text(html_page_url_to_crawl)

            # --- ë‹¤ì–‘í•œ ì„ íƒì ëª©ë¡ ---
            # ì¼ë°˜ì ì¸ ë„¤ë¹„ê²Œì´ì…˜, ë¦¬ìŠ¤íŠ¸, ì½˜í…ì¸  ì˜ì—­ ë“±ì„ íƒ€ê²Ÿìœ¼ë¡œ í•˜ëŠ” ì„ íƒìë“¤
            # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ (ë” êµ¬ì²´ì ì´ê±°ë‚˜ ê°€ëŠ¥ì„± ë†’ì€) ì„ íƒìë¥¼ ì•ì— ë°°ì¹˜
            selectors_to_try = [
                "a[href*='department']",  # ê¸°ë³¸ ì„ íƒì (ë¡œê·¸ì—ì„œ ì¼ë¶€ ëŒ€í•™ ì‹¤íŒ¨)
                "a[href*='dept']",  # 'dept' í¬í•¨ ë§í¬
                "a[href*='major']",  # 'major' í¬í•¨ ë§í¬
                "a[href*='í•™ë¶€']",  # 'í•™ë¶€' í¬í•¨ ë§í¬ (URLì— í•œê¸€ì´ ìˆëŠ” ê²½ìš°)
                "a[href*='í•™ê³¼']",  # 'í•™ê³¼' í¬í•¨ ë§í¬ (URLì— í•œê¸€ì´ ìˆëŠ” ê²½ìš°)
                # ë©”ë‰´/ë„¤ë¹„ê²Œì´ì…˜ êµ¬ì¡°ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì„ íƒì
                "nav a",  # <nav> íƒœê·¸ ì•ˆì˜ ëª¨ë“  ë§í¬
                "div[class*='nav'] a",  # classì— 'nav'ë¥¼ í¬í•¨í•˜ëŠ” div ì•ˆì˜ ë§í¬
                "div[id*='nav'] a",  # idì— 'nav'ë¥¼ í¬í•¨í•˜ëŠ” div ì•ˆì˜ ë§í¬
                "ul[class*='menu'] li a",  # classì— 'menu'ë¥¼ í¬í•¨í•˜ëŠ” ul ì•ˆì˜ li ì•ˆì˜ ë§í¬
                "ul[id*='menu'] li a",  # idì— 'menu'ë¥¼ í¬í•¨í•˜ëŠ” ul ì•ˆì˜ li ì•ˆì˜ ë§í¬
                "div.menu_wrap ul li a",  # êµ¬ì²´ì ì¸ ë©”ë‰´ êµ¬ì¡° ì˜ˆì‹œ
                "div.snb_wrap ul li a",  # ì‚¬ì´ë“œ ë„¤ë¹„ê²Œì´ì…˜ ë°” êµ¬ì¡° ì˜ˆì‹œ
                # í•™ê³¼ ëª©ë¡ì„ ì§ì ‘ ë‹´ê³  ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” êµ¬ì¡°
                "div[class*='dept_list'] a",  # classì— 'dept_list' í¬í•¨í•˜ëŠ” div ì•ˆì˜ ë§í¬
                "ul[class*='dept_list'] li a",  # classì— 'dept_list' í¬í•¨í•˜ëŠ” ul ì•ˆì˜ ë§í¬
                # ì¢€ ë” ì¼ë°˜ì ì¸ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ë‚´ì˜ ë§í¬
                "ul li a",
                # ì½˜í…ì¸  ì˜ì—­ ë‚´ì˜ ë§í¬ ì¤‘ íŠ¹ì • íŒ¨í„´ (ë§¤ìš° ì¼ë°˜ì ì´ë¯€ë¡œ ì£¼ì˜)
                # "article a", "div.content a"
            ]

            # íœ´ë¦¬ìŠ¤í‹±: í•œ ì„ íƒìë¡œ ì°¾ì€ í•™ê³¼ ìˆ˜ê°€ ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ ë¶€ì ì ˆí•˜ë‹¤ê³  íŒë‹¨ (ì¡°ì • ê°€ëŠ¥)
            MIN_EXPECTED_DEPTS = 2
            MAX_EXPECTED_DEPTS = 30  # ë§¤ìš° í° ë‹¨ê³¼ëŒ€í•™ë„ ê³ ë ¤

            temp_html_depts_candidates = {}  # ì„ íƒìë³„ í›„ë³´êµ° ì €ì¥ (ì¤‘ë³µ ë°©ì§€ìš© URL í‚¤)

            for selector_idx, current_selector in enumerate(selectors_to_try):
                logger.debug(
                    f"[{college.name}] HTML ì„ íƒì ì‹œë„ ({selector_idx + 1}/{len(selectors_to_try)}): '{current_selector}'")
                hrefs = html_select(html_content, current_selector, attr="href")
                names = html_select(html_content, current_selector)  # ë§í¬ì˜ í…ìŠ¤íŠ¸

                if hrefs and names and len(hrefs) == len(names):
                    logger.trace(f"[{college.name}] ì„ íƒì '{current_selector}'ë¡œ {len(hrefs)}ê°œ ë§í¬/ì´ë¦„ ìŒ ë°œê²¬.")

                    # í˜„ì¬ ì„ íƒìë¡œ ì°¾ì€ ìœ íš¨í•œ í•™ê³¼ í›„ë³´
                    current_selector_valid_depts = []

                    for i in range(len(hrefs)):
                        nm_cleaned = clean_text(names[i])
                        href_val = hrefs[i]

                        if not nm_cleaned or not href_val:
                            continue

                        # 1. ì´ë¦„ ê²€ì¦ (ë„ˆë¬´ ì§§ê±°ë‚˜, ì¼ë°˜ì ì´ì§€ ì•Šê±°ë‚˜, ì œì™¸ í‚¤ì›Œë“œ í¬í•¨)
                        if len(nm_cleaned) < 2 or len(nm_cleaned) > 50:  # ë§¤ìš° ì§§ê±°ë‚˜ ê¸´ ì´ë¦„ ì œì™¸
                            # logger.trace(f"[{college.name}] ì´ë¦„ ê¸¸ì´ ë¶€ì ì ˆ ({nm_cleaned}). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        if any(ex_kw in nm_cleaned for ex_kw in EXCLUDE_KEYWORDS_IN_NAME):
                            # logger.trace(f"[{college.name}] ì´ë¦„ì— ì œì™¸ í‚¤ì›Œë“œ í¬í•¨ ({nm_cleaned}). ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue
                        if not any(kw in nm_cleaned.lower() for kw in DEPT_KEYWORDS):
                            # ì´ë¦„ì— í•™ê³¼ ê´€ë ¨ í‚¤ì›Œë“œê°€ ì „í˜€ ì—†ìœ¼ë©´ ì˜ì‹¬ (í•˜ì§€ë§Œ ëª¨ë“  ê²½ìš°ì— ë§ì§€ëŠ” ì•ŠìŒ)
                            # logger.trace(f"[{college.name}] ì´ë¦„ì— í•™ê³¼ í‚¤ì›Œë“œ ë¶€ì¬ ({nm_cleaned}). ì¼ë‹¨ í¬í•¨í•˜ë‚˜ ì£¼ì˜.")
                            pass  # ì¼ë‹¨ì€ í¬í•¨ì‹œí‚¤ê³  ì•„ë˜ URL ë“±ìœ¼ë¡œ ì¶”ê°€ íŒë‹¨

                        # 2. URL ê²€ì¦
                        full_url = urljoin(html_page_url_to_crawl, href_val)
                        if not full_url.startswith(college.url.rstrip('/')):  # í˜„ì¬ ëŒ€í•™ ì‚¬ì´íŠ¸ ì™¸ë¶€ ë§í¬ ì œì™¸
                            if not full_url.startswith("http"):  # ìƒëŒ€ê²½ë¡œì˜€ë‹¤ë©´ í˜„ì¬ ë„ë©”ì¸ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²ƒì„
                                pass
                            elif full_url.split('/')[2] != college.url.split('/')[2]:  # ë‹¤ë¥¸ ë„ë©”ì¸ì´ë©´ ì œì™¸
                                # logger.trace(f"[{college.name}] ì™¸ë¶€ ë„ë©”ì¸ ë§í¬ ({full_url}). ê±´ë„ˆëœë‹ˆë‹¤.")
                                continue

                        # í•™ê³¼ ì½”ë“œ ì¶”ì¶œ ë¡œì§ (ì´ì „ ë‹µë³€ì˜ ë¡œì§ í™œìš©)
                        code_match = re.search(r'/department[s]?/([\w-]+)', full_url, re.I) or \
                                     re.search(r'dept[C|c]d(?:=|/)(\w+)', full_url, re.I) or \
                                     re.search(r'major(?:=|/)(\w+)', full_url, re.I) or \
                                     re.search(r'/(\w{3,})/?$', full_url.rstrip('/').split('?')[0])  # ë§ˆì§€ë§‰ ê²½ë¡œ (3ê¸€ì ì´ìƒ)

                        dept_code_extracted = ""
                        if code_match:
                            dept_code_extracted = next((g for g in code_match.groups() if g is not None), None)

                        if not dept_code_extracted:
                            path_parts = [part for part in full_url.split('?')[0].split('/') if part]
                            if path_parts and len(path_parts[-1]) > 2 and not path_parts[-1].endswith(
                                    (".do", ".jsp", ".html", ".htm")):
                                dept_code_extracted = path_parts[-1]
                            else:  # ìµœí›„ì˜ ìˆ˜ë‹¨ (ì´ë¦„ ê¸°ë°˜ - ê³ ìœ ì„± ë‚®ìŒ)
                                dept_code_extracted = re.sub(r'[^a-z0-9]', '', nm_cleaned.lower().replace(" ", ""))[:15]

                        dept_code_final = clean_text(dept_code_extracted)[:50] if dept_code_extracted else ""

                        if not dept_code_final:
                            # logger.warning(f"[{college.name}] í•™ê³¼ ì½”ë“œ ìƒì„± ìµœì¢… ì‹¤íŒ¨ (ì´ë¦„: {nm_cleaned}, URL: {full_url}).")
                            continue

                        # í›„ë³´êµ°ì— ì¶”ê°€ (URLì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
                        if full_url not in temp_html_depts_candidates:
                            candidate_dept_info = {"code": dept_code_final, "name": nm_cleaned, "url": full_url,
                                                   "selector": current_selector}
                            temp_html_depts_candidates[full_url] = candidate_dept_info
                            current_selector_valid_depts.append(candidate_dept_info)

                    # í˜„ì¬ ì„ íƒìë¡œ ì°¾ì€ í•™ê³¼ ìˆ˜ê°€ ì ì ˆí•œ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                    if MIN_EXPECTED_DEPTS <= len(current_selector_valid_depts) <= MAX_EXPECTED_DEPTS:
                        logger.info(
                            f"[{college.name}] ì„ íƒì '{current_selector}'ë¡œ {len(current_selector_valid_depts)}ê°œì˜ ìœ íš¨í•œ í•™ê³¼ ì •ë³´ í›„ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        dept_list_data_final.extend(current_selector_valid_depts)  # ì²« ì„±ê³µ ê²°ê³¼ë¥¼ ìµœì¢… ëª©ë¡ì— ì¶”ê°€
                        break  # ì„±ê³µì ì¸ ì„ íƒìë¥¼ ì°¾ì•˜ìœ¼ë¯€ë¡œ ë” ì´ìƒ ë‹¤ë¥¸ ì„ íƒì ì‹œë„ ì•ˆ í•¨
                    elif current_selector_valid_depts:  # ë²”ìœ„ëŠ” ë²—ì–´ë‚¬ì§€ë§Œ ì¼ë‹¨ ì°¾ê¸´ ì°¾ì€ ê²½ìš°
                        logger.debug(
                            f"[{college.name}] ì„ íƒì '{current_selector}'ë¡œ {len(current_selector_valid_depts)}ê°œ í›„ë³´ ë°œê²¬ (ê¸°ëŒ€ ë²”ìœ„: {MIN_EXPECTED_DEPTS}~{MAX_EXPECTED_DEPTS}). ë‹¤ìŒ ì„ íƒì ê³„ì† ì‹œë„.")
                        # ì´ í›„ë³´ë“¤ì„ ì„ì‹œë¡œ ì €ì¥í•´ë‘ê³ , ë‹¤ë¥¸ ì„ íƒìê°€ ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ

            # ëª¨ë“  ì„ íƒìë¥¼ ì‹œë„í•œ í›„ì—ë„ dept_list_data_finalì´ ë¹„ì–´ìˆê³ ,
            # temp_html_depts_candidatesì— ë­”ê°€ ìˆë‹¤ë©´, ê·¸ ì¤‘ ê°€ì¥ ë§ì€ ê²ƒì„ ì„ íƒ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            if not dept_list_data_final and temp_html_depts_candidates:
                logger.warning(f"[{college.name}] ëª¨ë“  ì£¼ìš” ì„ íƒìì—ì„œ ê¸°ëŒ€ ë²”ìœ„ ë‚´ì˜ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ëœ ëª¨ë“  í›„ë³´ ì¤‘ ê°€ì¥ ê°€ëŠ¥ì„± ìˆëŠ” ê²°ê³¼ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.")
                # ê°€ì¥ ë§ì€ í›„ë³´ë¥¼ ìƒì„±í•œ ì„ íƒìì˜ ê²°ê³¼ ë˜ëŠ” ë‹¤ë¥¸ íœ´ë¦¬ìŠ¤í‹± ì ìš© ê°€ëŠ¥
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ëª¨ë“  í›„ë³´ë¥¼ ë‹¤ ë„£ì–´ë´„ (ì¤‘ë³µì€ URL í‚¤ë¡œ ì œê±°ë¨)
                dept_list_data_final.extend(list(temp_html_depts_candidates.values()))
                if dept_list_data_final:
                    logger.info(f"[{college.name}] ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ {len(dept_list_data_final)}ê°œ í•™ê³¼ ì •ë³´ í›„ë³´ë¥¼ ìµœì¢… ëª©ë¡ì— í¬í•¨.")

            if not dept_list_data_final:  # HTML Fallback ìµœì¢… ì‹¤íŒ¨
                logger.warning(f"[{college.name}] HTMLì—ì„œ í•™ê³¼ ë§í¬ë‚˜ ì´ë¦„ì„ ìµœì¢…ì ìœ¼ë¡œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                logger.debug(f"[{college.name}] ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹œë„ëœ HTML ë‚´ìš© (ì²˜ìŒ 1000ì): {html_content[:1000]}")

        except (ClientError, Exception) as e_html:
            logger.error(f"[{college.name}] HTML Fallback ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {type(e_html).__name__} - {e_html}")

    # --- ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ ë° DB ì €ì¥ ---
    if not dept_list_data_final:
        logger.warning(f"[{college.name}] ìµœì¢…ì ìœ¼ë¡œ í•™ê³¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # DB ì €ì¥ ì „ ì¤‘ë³µ ì œê±° (ë™ì¼ code í•™ê³¼ê°€ ì—¬ëŸ¬ ì„ íƒìì—ì„œ ì¡í˜”ì„ ìˆ˜ ìˆìŒ)
    final_unique_depts_to_save = []
    seen_codes = set()
    for dept_info in dept_list_data_final:
        if dept_info["code"] not in seen_codes:
            final_unique_depts_to_save.append(dept_info)
            seen_codes.add(dept_info["code"])

    if len(final_unique_depts_to_save) != len(dept_list_data_final):
        logger.info(
            f"[{college.name}] DB ì €ì¥ ì „ ì¤‘ë³µëœ í•™ê³¼ ì½”ë“œ ì œê±°: {len(dept_list_data_final)} -> {len(final_unique_depts_to_save)}ê°œ")

    with get_session() as sess:
        added_count = 0
        updated_count = 0
        for d_item_to_save in final_unique_depts_to_save:
            # ì„ íƒì ì •ë³´ëŠ” DBì— ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
            d_item_for_db = {k: v for k, v in d_item_to_save.items() if k != "selector"}

            obj = (sess.query(Department)
                   .filter_by(college_id=college.id, code=d_item_for_db["code"]).one_or_none())
            if obj:
                if obj.name != d_item_for_db["name"] or obj.url != d_item_for_db["url"]:
                    obj.name, obj.url = d_item_for_db["name"], d_item_for_db["url"]
                    updated_count += 1
            else:
                obj = Department(college_id=college.id, **d_item_for_db)
                sess.add(obj)
                added_count += 1

        if added_count > 0 or updated_count > 0:
            try:
                sess.commit()
                logger.success(
                    f"[{college.name}] í•™ê³¼ ì •ë³´ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: {added_count}ê°œ ì¶”ê°€, {updated_count}ê°œ ìˆ˜ì • (ê³ ìœ  í•™ê³¼ ìˆ˜: {len(final_unique_depts_to_save)}).")
            except Exception as e_db:
                logger.opt(exception=True).error(f"[{college.name}] í•™ê³¼ ì •ë³´ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e_db}")
                sess.rollback()
        else:
            logger.info(f"[{college.name}] DBì— ë³€ê²½ëœ í•™ê³¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤ (ì²˜ë¦¬ëœ ê³ ìœ  í•™ê³¼ ìˆ˜: {len(final_unique_depts_to_save)}).")